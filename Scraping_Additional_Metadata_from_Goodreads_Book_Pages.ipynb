{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTpMI2eigj31Rkd+pXxI8X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/Scraping_Additional_Metadata_from_Goodreads_Book_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Install Dependencies and Imports"
      ],
      "metadata": {
        "id": "GSe-i6KnujRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (quiet to avoid verbose output)\n",
        "!pip install beautifulsoup4 requests pandas selenium tqdm lxml webdriver-manager geckodriver-autoinstaller credentials --quiet\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"Cell 1: Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsAML3uIvGCJ",
        "outputId": "292cad3e-6477-41a9-dcdf-bd854ee41344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cell 1: Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Cookie and Login Functions"
      ],
      "metadata": {
        "id": "aM0aa2q_vSVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cookies(driver, auto_login=False):\n",
        "    \"\"\"\n",
        "    Load cookies to simulate logged-in state or perform auto-login.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting cookie/login process ===\")\n",
        "\n",
        "    # Complete list of 13 cookies\n",
        "    cookies = [\n",
        "        {\n",
        "            'name': '_session_id2',\n",
        "            'value': '6c4ac9d7944645498873dcfaf76b294f',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365)).timestamp())  # 1 year from now\n",
        "        },\n",
        "        {\n",
        "            'name': 'at-main',\n",
        "            'value': 'Atza|IwEBIP_WVjU2wDt5XXIwnYO37c_HBxHHpvejaw457NkT7-DHU7flVrkMM02XEH0cYeYRpEYlSuc7aN4cr-0ME-ruM28LUkrC7ODW8WKTpaqeH-sCuNVv1z9YM7xBS-Z4T0jhtLlPe8xzqTMozRae_ZNoxsqvpLNQxyDu0Woeei-Ip3E_PYxuZeikLbIDzpe17BVcrNocfqj4fl5KszGyF7ExHAvfVTGVBH_UeX5rQbSY6ZOCTHiIoca2U5sZfChiCoUH3wk',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'ccsid',\n",
        "            'value': '724-3321647-0772806',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'csm-hit',\n",
        "            'value': 'tb:s-2R1WY81WFQ4QG5VKQVVG|1754352395507&t:1754352395508',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'lc-main',\n",
        "            'value': 'en_US',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'locale',\n",
        "            'value': 'en',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False\n",
        "        },\n",
        "        {\n",
        "            'name': 'logged_out_browsing_page_count',\n",
        "            'value': '1',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'sess-at-main',\n",
        "            'value': 'U8NDSc1MTsE8derMKmAGMZ+Uq9chMayYrfiC+B46wCE=',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-id',\n",
        "            'value': '140-3588248-4268607',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-id-time',\n",
        "            'value': '2385072442l',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-token',\n",
        "            'value': '0+8P54n7E6d+0GGShiBALFOEgtTQdUdna1ExcAeRw3Ul16HkHsMc2W2ZpSYd+dn2CmsRT4KTntQt8WF4Of+YP1EitZ4QW4VQMG/hg3NoH61WK00ztRFxR6GkLoQiygwTiqkHExpG3pEjipb2/x256UoQDqyJcqAdFLyXukWtVKwfCFNsWzxJZMP/gmSX/Ml1mSdPMmkb9yJ5gb+ugF6z5a1F2Hr01Tt1Ynz77AY8fV5BunhvuaXJYoMJjEKHvWoLmurVyyWT/YZ71mOlJJXAtphVwQmemG3C27hNxcCL3cC3x1N6iA36seE5LzPJmOkxNjbGZ5EnO3s+IVXsenDzWijTZZz3dAu9eDn03BYbLEF0fsf7cW4b3g==',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'ubid-main',\n",
        "            'value': '135-7477912-2392604',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'x-main',\n",
        "            'value': 'hf5q7qpx3x0HgBwzCGe?XOExwXUrkUVkjz1Z15Z2ptg0JUWQ9klSPvPcAXY9G41Z',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    driver.get('https://www.goodreads.com/')\n",
        "    print(f\"{datetime.now()}: Navigating to Goodreads homepage for cookie loading.\")\n",
        "    time.sleep(2)\n",
        "    driver.delete_all_cookies()\n",
        "    print(f\"{datetime.now()}: Cleared existing cookies.\")\n",
        "    for cookie in cookies:\n",
        "        if 'expiry' in cookie:\n",
        "            cookie['expiry'] = int(cookie['expiry'])\n",
        "        driver.add_cookie(cookie)\n",
        "    driver.refresh()\n",
        "    print(f\"{datetime.now()}: Loaded {len(cookies)} cookies and refreshed page.\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    if is_logged_in(driver):\n",
        "        print(f\"{datetime.now()}: Success: Cookie-based login successful!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"{datetime.now()}: Cookie login failed. Falling back to auto-login if enabled.\")\n",
        "        if auto_login:\n",
        "            return attempt_auto_login(driver)\n",
        "        return False\n",
        "\n",
        "def attempt_auto_login(driver):\n",
        "    email = os.environ.get('GOODREADS_EMAIL')\n",
        "    password = os.environ.get('GOODREADS_PASSWORD')\n",
        "    if not email or not password:\n",
        "        print(f\"{datetime.now()}: ERROR: GOODREADS_EMAIL and GOODREADS_PASSWORD not set. Skipping auto-login.\")\n",
        "        return False\n",
        "    print(f\"{datetime.now()}: Attempting auto-login with provided credentials.\")\n",
        "    driver.get('https://www.goodreads.com/user/sign_in')\n",
        "    time.sleep(3)\n",
        "    driver.find_element(By.ID, 'user_email').send_keys(email)\n",
        "    driver.find_element(By.ID, 'user_password').send_keys(password)\n",
        "    driver.find_element(By.NAME, 'commit').click()\n",
        "    time.sleep(5)\n",
        "    return is_logged_in(driver)\n",
        "\n",
        "def is_logged_in(driver):\n",
        "    indicators = [\n",
        "        (By.CLASS_NAME, 'siteHeader__personalMenu'),\n",
        "        (By.XPATH, \"//a[contains(@href, '/user/show/')]\")\n",
        "    ]\n",
        "    for by, value in indicators:\n",
        "        try:\n",
        "            driver.find_element(by, value)\n",
        "            return True\n",
        "        except NoSuchElementException:\n",
        "            continue\n",
        "    return False\n",
        "\n",
        "# Set up Selenium with anti-detection\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "load_cookies(driver, auto_login=True)  # Enable auto-login fallback if cookies fail\n",
        "\n",
        "print(\"Cell 2: Login functions ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HTBECkkvVv9",
        "outputId": "00836d03-5c53-44bd-d802-a180fb95778e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting cookie/login process ===\n",
            "2025-08-07 04:20:30.593550: Navigating to Goodreads homepage for cookie loading.\n",
            "2025-08-07 04:20:33.017921: Cleared existing cookies.\n",
            "2025-08-07 04:20:35.852029: Loaded 13 cookies and refreshed page.\n",
            "2025-08-07 04:20:38.988625: Success: Cookie-based login successful!\n",
            "Cell 2: Login functions ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Main Scraping Functions and Execution"
      ],
      "metadata": {
        "id": "XY2rJICwvaNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_abstract(soup):\n",
        "    desc_elem = soup.find('div', class_='BookPageMetadataSection__description')\n",
        "    if desc_elem:\n",
        "        formatted_span = desc_elem.find('span', class_='Formatted')\n",
        "        text = formatted_span.get_text(strip=True) if formatted_span else desc_elem.get_text(strip=True)\n",
        "        print(f\"{datetime.now()}: Scraped description: {text[:100]}...\")  # Print first 100 chars for brevity\n",
        "        return text\n",
        "    print(f\"{datetime.now()}: No description found.\")\n",
        "    return 'N/A'\n",
        "\n",
        "def extract_core_identifiers(soup, url):\n",
        "    print(f\"{datetime.now()}: Extracting core identifiers.\")\n",
        "    title_elem = soup.find('h1', {'data-testid': 'bookTitle'}) or soup.find('h1', class_='gr-h1')\n",
        "    title = title_elem.get_text(strip=True) if title_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped title: {title}\")\n",
        "\n",
        "    authors = [a.text.strip().replace('(Goodreads Author)', '').strip() for a in soup.find_all('span', {'data-testid': 'name'}) if 'Goodreads Author' not in a.text]\n",
        "    author = authors[0] if authors else 'N/A'\n",
        "    all_authors = '; '.join(authors) if len(authors) > 1 else author\n",
        "    print(f\"{datetime.now()}: Scraped author: {author}\")\n",
        "    print(f\"{datetime.now()}: Scraped all authors: {all_authors}\")\n",
        "\n",
        "    book_id_match = re.search(r'(\\d+)', url)\n",
        "    book_id = book_id_match.group(1) if book_id_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped book_id: {book_id}\")\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'all_authors': all_authors,\n",
        "        'book_id': book_id\n",
        "    }\n",
        "\n",
        "def extract_ratings_reviews(soup):\n",
        "    print(f\"{datetime.now()}: Extracting ratings and reviews metadata.\")\n",
        "    rating_elem = soup.find('div', class_='RatingStatistics__rating')\n",
        "    average_rating = float(rating_elem.get_text(strip=True)) if rating_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped average_rating: {average_rating}\")\n",
        "\n",
        "    ratings_count_elem = soup.find('span', {'data-testid': 'ratingsCount'})\n",
        "    num_ratings = int(re.search(r'(\\d+[\\d,]*)', ratings_count_elem.get_text(strip=True)).group(1).replace(',', '')) if ratings_count_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped num_ratings: {num_ratings}\")\n",
        "\n",
        "    reviews_count_elem = soup.find('span', {'data-testid': 'reviewsCount'})\n",
        "    num_reviews = int(re.search(r'(\\d+[\\d,]*)', reviews_count_elem.get_text(strip=True)).group(1).replace(',', '')) if reviews_count_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped num_reviews: {num_reviews}\")\n",
        "\n",
        "    distribution = {}\n",
        "    rating_bars = soup.find_all('div', class_='RatingStatistics__column')\n",
        "    for i, bar in enumerate(rating_bars):\n",
        "        aria_label = bar.get('aria-label', '')\n",
        "        percentage_match = re.search(r'(\\d+)%', aria_label)\n",
        "        if percentage_match:\n",
        "            distribution[f'{5-i}_star'] = int(percentage_match.group(1))\n",
        "    print(f\"{datetime.now()}: Scraped rating_distribution: {distribution}\")\n",
        "\n",
        "    return {\n",
        "        'average_rating': average_rating,\n",
        "        'num_ratings': num_ratings,\n",
        "        'num_reviews': num_reviews,\n",
        "        'rating_distribution': distribution\n",
        "    }\n",
        "\n",
        "def extract_publication_details(soup):\n",
        "    print(f\"{datetime.now()}: Extracting publication details.\")\n",
        "    pub_info_elem = soup.find('p', {'data-testid': 'publicationInfo'})\n",
        "    pub_text = pub_info_elem.get_text(strip=True) if pub_info_elem else ''\n",
        "    print(f\"{datetime.now()}: Scraped pub_info: {pub_text}\")\n",
        "\n",
        "    year_match = re.search(r'(\\d{4})', pub_text)\n",
        "    pub_date = int(year_match.group(1)) if year_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped pub_date: {pub_date}\")\n",
        "\n",
        "    publisher = pub_text.split('by ')[-1].strip() if 'by ' in pub_text else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped publisher: {publisher}\")\n",
        "\n",
        "    format_elem = soup.find('p', {'data-testid': 'pagesFormat'})\n",
        "    format_text = format_elem.get_text(strip=True) if format_elem else ''\n",
        "    print(f\"{datetime.now()}: Scraped format_info: {format_text}\")\n",
        "\n",
        "    pages_match = re.search(r'(\\d+) pages', format_text)\n",
        "    page_count = int(pages_match.group(1)) if pages_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped page_count: {page_count}\")\n",
        "\n",
        "    return {\n",
        "        'pub_info': pub_text,\n",
        "        'pub_date': pub_date,\n",
        "        'publisher': publisher,\n",
        "        'isbn': soup.find('span', itemprop='isbn').text if soup.find('span', itemprop='isbn') else 'N/A',\n",
        "        'format_info': format_text,\n",
        "        'page_count': page_count,\n",
        "        'language': 'English'  # Default or parse if available\n",
        "    }\n",
        "\n",
        "def extract_genres_shelves(soup):\n",
        "    print(f\"{datetime.now()}: Extracting genres and shelves.\")\n",
        "    genres = []\n",
        "    genre_section = soup.find('div', class_='BookPageMetadataSection__genres')\n",
        "    if genre_section:\n",
        "        genre_links = genre_section.find_all('a', class_='Button')\n",
        "        genres = [link.text.strip() for link in genre_links[:10]]  # Top 10\n",
        "    primary_genre = genres[0] if genres else 'N/A'\n",
        "    genres_str = '; '.join(genres) if genres else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped genres: {genres_str}\")\n",
        "    print(f\"{datetime.now()}: Scraped primary_genre: {primary_genre}\")\n",
        "\n",
        "    return {\n",
        "        'genres': genres_str,\n",
        "        'primary_genre': primary_genre\n",
        "    }\n",
        "\n",
        "def extract_characters_and_places(soup):\n",
        "    print(f\"{datetime.now()}: Extracting characters and places.\")\n",
        "    characters = [c.text.strip() for c in soup.select('div[data-testid=\"characters\"] a')]\n",
        "    places = [p.text.strip() for p in soup.select('div[data-testid=\"places\"] a')]\n",
        "    characters_str = '; '.join(characters) if characters else 'N/A'\n",
        "    places_str = '; '.join(places) if places else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped characters: {characters_str}\")\n",
        "    print(f\"{datetime.now()}: Scraped places: {places_str}\")\n",
        "\n",
        "    return {\n",
        "        'characters': characters_str,\n",
        "        'places': places_str\n",
        "    }\n",
        "\n",
        "def collect_reviews(driver, book_id, max_reviews=100):\n",
        "    reviews_url = f\"https://www.goodreads.com/book/show/{book_id}/reviews?review_filters=has_user_rating\"\n",
        "    print(f\"{datetime.now()}: Navigating to reviews URL: {reviews_url}\")\n",
        "    driver.get(reviews_url)\n",
        "    time.sleep(random.uniform(2, 4))\n",
        "\n",
        "    reviews = []\n",
        "    loaded = 0\n",
        "    pbar = tqdm(total=max_reviews, desc=\"Collecting reviews\")\n",
        "    while loaded < max_reviews:\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        review_elements = soup.find_all('section', class_='ReviewCard')[loaded:]\n",
        "        for rev in review_elements:\n",
        "            rating = rev.find('span', class_='RatingStars')['aria-label'] if rev.find('span', class_='RatingStars') else 'N/A'\n",
        "            text = rev.find('span', class_='ReviewText').text.strip() if rev.find('span', class_='ReviewText') else 'N/A'\n",
        "            reviews.append({'rating': rating, 'text': text})\n",
        "            loaded += 1\n",
        "            pbar.update(1)\n",
        "            if loaded >= max_reviews:\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            load_more = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Load More')]\")))\n",
        "            print(f\"{datetime.now()}: Clicking 'Load More' (loaded so far: {loaded})\")\n",
        "            driver.execute_script(\"arguments[0].click();\", load_more)\n",
        "            time.sleep(random.uniform(2, 4))\n",
        "        except TimeoutException:\n",
        "            print(f\"{datetime.now()}: No more 'Load More' button or timeout. Stopping review collection.\")\n",
        "            break\n",
        "    pbar.close()\n",
        "\n",
        "    # Print scraped reviews summary and sample\n",
        "    print(f\"{datetime.now()}: Collected {len(reviews)} reviews.\")\n",
        "    if reviews:\n",
        "        print(f\"{datetime.now()}: Sample of first 5 reviews:\")\n",
        "        for idx, rev in enumerate(reviews[:5]):\n",
        "            print(f\"Review {idx+1}: Rating = {rev['rating']}, Text = {rev['text'][:100]}...\")  # First 100 chars for brevity\n",
        "\n",
        "    return reviews\n",
        "\n",
        "def enrich_book_metadata(row):\n",
        "    start_time = datetime.now()\n",
        "    print(f\"\\n{start_time}: Starting metadata enrichment for book: {row['title']} (URL: {row['url']})\")\n",
        "\n",
        "    driver.get(row['url'])\n",
        "    time.sleep(random.uniform(2, 4))\n",
        "\n",
        "    # Expand sections\n",
        "    print(f\"{datetime.now()}: Expanding sections...\")\n",
        "    expand_buttons = driver.find_elements(By.XPATH, \"//button[contains(text(), 'more') or contains(text(), 'More') or @aria-label='Show all']\")\n",
        "    for btn in expand_buttons:\n",
        "        try:\n",
        "            driver.execute_script(\"arguments[0].click();\", btn)\n",
        "            time.sleep(1)\n",
        "        except:\n",
        "            print(f\"{datetime.now()}: Failed to click an expand button (continuing).\")\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    metadata = extract_core_identifiers(soup, row['url'])\n",
        "    metadata.update(extract_ratings_reviews(soup))\n",
        "    metadata.update(extract_publication_details(soup))\n",
        "    metadata.update(extract_genres_shelves(soup))\n",
        "    metadata.update(extract_characters_and_places(soup))\n",
        "    metadata['description'] = extract_abstract(soup)\n",
        "\n",
        "    duration = (datetime.now() - start_time).total_seconds()\n",
        "    print(f\"{datetime.now()}: Metadata enrichment complete for {row['title']} (took {duration:.2f} seconds).\")\n",
        "    return pd.Series(metadata)\n",
        "\n",
        "# Load CSV\n",
        "csv_path = '/content/drive/MyDrive/Romantic_Extened_Dataset_Scraping/romantic_books_final_scraped.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"{datetime.now()}: Loaded DataFrame with {len(df)} rows.\")\n",
        "\n",
        "# For testing: Process only first 3 rows (comment out for full dataset)\n",
        "df = df.head(3)\n",
        "print(f\"{datetime.now()}: Testing on sample of {len(df)} books (first 3 rows).\")\n",
        "\n",
        "# Enrich metadata with progress bar\n",
        "enriched_data = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Enriching metadata for books\"):\n",
        "    try:\n",
        "        enriched_data.append(enrich_book_metadata(row))\n",
        "    except Exception as e:\n",
        "        print(f\"{datetime.now()}: Error enriching metadata for {row['title']}: {e}. Skipping.\")\n",
        "df = pd.concat([df] + enriched_data, axis=1)\n",
        "\n",
        "# Collect reviews for each book with progress\n",
        "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting reviews for books\"):\n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        print(f\"\\n{start_time}: Starting review collection for book: {row['title']} (ID: {row['book_id']})\")\n",
        "        df.at[i, 'reviews'] = json.dumps(collect_reviews(driver, row['book_id']))\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"{datetime.now()}: Review collection complete for {row['title']} (took {duration:.2f} seconds).\")\n",
        "    except Exception as e:\n",
        "        print(f\"{datetime.now()}: Error collecting reviews for {row['title']}: {e}. Skipping.\")\n",
        "\n",
        "# Save enriched CSV\n",
        "enriched_path = '/content/drive/MyDrive/Romantic_Extened_Dataset_Scraping/romantic_books_enriched.csv'\n",
        "df.to_csv(enriched_path, index=False)\n",
        "print(f\"{datetime.now()}: Enriched CSV saved to {enriched_path}\")\n",
        "\n",
        "driver.quit()\n",
        "print(f\"{datetime.now()}: Cell 3: Enrichment complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS0JZ83evZu5",
        "outputId": "22d2b645-52d2-47bd-b1bd-fd19d0d3c7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 04:20:39.033697: Loaded DataFrame with 2860 rows.\n",
            "2025-08-07 04:20:39.034138: Testing on sample of 3 books (first 3 rows).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEnriching metadata for books:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2025-08-07 04:20:39.040829: Starting metadata enrichment for book: The Love Hypothesis (Paperback) (URL: https://www.goodreads.com/book/show/56732449-the-love-hypothesis)\n"
          ]
        }
      ]
    }
  ]
}