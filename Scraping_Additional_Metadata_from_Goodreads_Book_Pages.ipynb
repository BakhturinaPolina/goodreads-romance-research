{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI2+0wY7VwkY0zOm9+OHgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/Scraping_Additional_Metadata_from_Goodreads_Book_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Install Dependencies and Imports"
      ],
      "metadata": {
        "id": "GSe-i6KnujRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (quiet to avoid verbose output)\n",
        "!pip install beautifulsoup4 requests pandas selenium tqdm lxml webdriver-manager --quiet\n",
        "\n",
        "# Install Chrome browser using deb package\n",
        "!apt-get update --quiet > /dev/null\n",
        "!apt-get install -y libvulkan1 > /dev/null # Install missing dependency\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb --quiet\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb > /dev/null\n",
        "!apt-get install -f --yes > /dev/null # Install remaining dependencies\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException, InvalidSelectorException\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"Cell 1: Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kV6Wosnz8AB",
        "outputId": "23a87e5e-84ac-4251-b9d2-36dca190cff8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cell 1: Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Cookie and Login Functions"
      ],
      "metadata": {
        "id": "aM0aa2q_vSVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cookies(driver, auto_login=False):\n",
        "    \"\"\"\n",
        "    Load cookies to simulate logged-in state or perform auto-login.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting cookie/login process ===\")\n",
        "\n",
        "    # Complete list of 13 cookies (NOTE: These may expire; refresh from a logged-in browser session if needed)\n",
        "    cookies = [\n",
        "        {\n",
        "            'name': '_session_id2',\n",
        "            'value': '6c4ac9d7944645498873dcfaf76b294f',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365)).timestamp())  # 1 year from now\n",
        "        },\n",
        "        {\n",
        "            'name': 'at-main',\n",
        "            'value': 'Atza|IwEBIP_WVjU2wDt5XXIwnYO37c_HBxHHpvejaw457NkT7-DHU7flVrkMM02XEH0cYeYRpEYlSuc7aN4cr-0ME-ruM28LUkrC7ODW8WKTpaqeH-sCuNVv1z9YM7xBS-Z4T0jhtLlPe8xzqTMozRae_ZNoxsqvpLNQxyDu0Woeei-Ip3E_PYxuZeikLbIDzpe17BVcrNocfqj4fl5KszGyF7ExHAvfVTGVBH_UeX5rQbSY6ZOCTHiIoca2U5sZfChiCoUH3wk',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'ccsid',\n",
        "            'value': '724-3321647-0772806',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'csm-hit',\n",
        "            'value': 'tb:s-2R1WY81WFQ4QG5VKQVVG|1754352395507&t:1754352395508',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'lc-main',\n",
        "            'value': 'en_US',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'locale',\n",
        "            'value': 'en',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False\n",
        "        },\n",
        "        {\n",
        "            'name': 'logged_out_browsing_page_count',\n",
        "            'value': '1',\n",
        "            'domain': 'www.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'sess-at-main',\n",
        "            'value': 'U8NDSc1MTsE8derMKmAGMZ+Uq9chMayYrfiC+B46wCE=',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': True,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-id',\n",
        "            'value': '140-3588248-4268607',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-id-time',\n",
        "            'value': '2385072442l',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'session-token',\n",
        "            'value': '0+8P54n7E6d+0GGShiBALFOEgtTQdUdna1ExcAeRw3Ul16HkHsMc2W2ZpSYd+dn2CmsRT4KTntQt8WF4Of+YP1EitZ4QW4VQMG/hg3NoH61WK00ztRFxR6GkLoQiygwTiqkHExpG3pEjipb2/x256UoQDqyJcqAdFLyXukWtVKwfCFNsWzxJZMP/gmSX/Ml1mSdPMmkb9yJ5gb+ugF6z5a1F2Hr01Tt1Ynz77AY8fV5BunhvuaXJYoMJjEKHvWoLmurVyyWT/YZ71mOlJJXAtphVwQmemG3C27hNxcCL3cC3x1N6iA36seE5LzPJmOkxNjbGZ5EnO3s+IVXsenDzWijTZZz3dAu9eDn03BYbLEF0fsf7cW4b3g==',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'ubid-main',\n",
        "            'value': '135-7477912-2392604',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        },\n",
        "        {\n",
        "            'name': 'x-main',\n",
        "            'value': 'hf5q7qpx3x0HgBwzCGe?XOExwXUrkUVkjz1Z15Z2ptg0JUWQ9klSPvPcAXY9G41Z',\n",
        "            'domain': '.goodreads.com',\n",
        "            'path': '/',\n",
        "            'secure': False,\n",
        "            'httpOnly': True,\n",
        "            'expiry': int((datetime.now() + timedelta(days=365*2)).timestamp())\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for attempt in range(2):  # Retry cookie loading once if needed\n",
        "        driver.get('https://www.goodreads.com/')\n",
        "        print(f\"{datetime.now()}: Navigating to Goodreads homepage for cookie loading (attempt {attempt+1}).\")\n",
        "        time.sleep(3)\n",
        "        driver.delete_all_cookies()\n",
        "        print(f\"{datetime.now()}: Cleared existing cookies.\")\n",
        "        for cookie in cookies:\n",
        "            if 'expiry' in cookie:\n",
        "                cookie['expiry'] = int(cookie['expiry'])\n",
        "            driver.add_cookie(cookie)\n",
        "        driver.refresh()\n",
        "        print(f\"{datetime.now()}: Loaded {len(cookies)} cookies and refreshed page.\")\n",
        "        time.sleep(3)\n",
        "\n",
        "        if is_logged_in(driver):\n",
        "            print(f\"{datetime.now()}: Success: Cookie-based login successful!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"{datetime.now()}: Cookie login failed (attempt {attempt+1}).\")\n",
        "            # Insight: Print page_source for debugging login failures\n",
        "            print(f\"{datetime.now()}: Debugging - Current page source:\\n{driver.page_source[:500]}...\")  # Truncated for brevity\n",
        "\n",
        "    print(f\"{datetime.now()}: Cookie login failed after retries. Falling back to auto-login if enabled.\")\n",
        "    if auto_login:\n",
        "        return attempt_auto_login(driver)\n",
        "    return False\n",
        "\n",
        "def attempt_auto_login(driver):\n",
        "    email = os.environ.get('GOODREADS_EMAIL')\n",
        "    password = os.environ.get('GOODREADS_PASSWORD')\n",
        "    if not email or not password:\n",
        "        print(f\"{datetime.now()}: ERROR: GOODREADS_EMAIL and GOODREADS_PASSWORD not set. Skipping auto-login.\")\n",
        "        return False\n",
        "    print(f\"{datetime.now()}: Attempting auto-login with provided credentials.\")\n",
        "    driver.get('https://www.goodreads.com/user/sign_in')\n",
        "    time.sleep(3)\n",
        "    driver.find_element(By.ID, 'user_email').send_keys(email)\n",
        "    driver.find_element(By.ID, 'user_password').send_keys(password)\n",
        "    driver.find_element(By.NAME, 'commit').click()\n",
        "    time.sleep(5)\n",
        "    return is_logged_in(driver)\n",
        "\n",
        "def is_logged_in(driver):\n",
        "    indicators = [\n",
        "        (By.CLASS_NAME, 'siteHeader__personalMenu'),\n",
        "        (By.XPATH, \"//a[contains(@href, '/user/show/')]\")\n",
        "    ]\n",
        "    for by, value in indicators:\n",
        "        try:\n",
        "            driver.find_element(by, value)\n",
        "            return True\n",
        "        except NoSuchElementException:\n",
        "            continue\n",
        "    return False\n",
        "\n",
        "# Set up Selenium with anti-detection\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36')  # Updated to latest Chrome version\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "if not load_cookies(driver, auto_login=True):  # Enable auto-login fallback\n",
        "    raise ValueError(\"Login failed. Cannot proceed with scraping.\")\n",
        "\n",
        "print(\"Cell 2: Login functions ready and login successful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3khz9nj0BD-",
        "outputId": "3bf2216c-a0fb-4e12-f9bd-af1fb0f185fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting cookie/login process ===\n",
            "2025-08-07 18:26:43.173389: Navigating to Goodreads homepage for cookie loading (attempt 1).\n",
            "2025-08-07 18:26:46.441573: Cleared existing cookies.\n",
            "2025-08-07 18:26:56.059926: Loaded 13 cookies and refreshed page.\n",
            "2025-08-07 18:27:03.897012: Success: Cookie-based login successful!\n",
            "Cell 2: Login functions ready and login successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Main Scraping Functions and Execution"
      ],
      "metadata": {
        "id": "XY2rJICwvaNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_abstract(driver, soup):  # Insight: Pass driver for waits; use explicit wait from provided code\n",
        "    print(f\"{datetime.now()}: Extracting abstract.\")\n",
        "    try:\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \".BookPageMetadataSection__description span.Formatted\"))\n",
        "        )\n",
        "        text = element.text.strip()\n",
        "        print(f\"{datetime.now()}: Scraped description: {text[:100]}...\")\n",
        "        return text\n",
        "    except TimeoutException:\n",
        "        print(f\"{datetime.now()}: No description found. Debugging - Current page source:\\n{driver.page_source[:500]}...\")\n",
        "        return 'N/A'\n",
        "\n",
        "def extract_core_identifiers(soup, url):\n",
        "    print(f\"{datetime.now()}: Extracting core identifiers.\")\n",
        "    title_elem = soup.find('h1', {'data-testid': 'bookTitle'}) or soup.find('h1', class_='gr-h1')\n",
        "    title = title_elem.get_text(strip=True) if title_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped title: {title}\")\n",
        "\n",
        "    authors = [a.text.strip().replace('(Goodreads Author)', '').strip() for a in soup.find_all('span', {'data-testid': 'name'}) if 'Goodreads Author' not in a.text]\n",
        "    author = authors[0] if authors else 'N/A'\n",
        "    all_authors = '; '.join(authors) if len(authors) > 1 else author\n",
        "    print(f\"{datetime.now()}: Scraped author: {author}\")\n",
        "    print(f\"{datetime.now()}: Scraped all authors: {all_authors}\")\n",
        "\n",
        "    book_id_match = re.search(r'(\\d+)', url)\n",
        "    book_id = book_id_match.group(1) if book_id_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped book_id: {book_id}\")\n",
        "\n",
        "    return {\n",
        "        'title': title,  # Updated: Use lowercase 'title'\n",
        "        'author': author,\n",
        "        'all_authors': all_authors,\n",
        "        'book_id': book_id\n",
        "    }\n",
        "\n",
        "def extract_ratings_reviews(driver, soup):  # Updated: Parse histogram widths from provided HTML\n",
        "    print(f\"{datetime.now()}: Extracting ratings and reviews metadata.\")\n",
        "    rating_elem = soup.find('div', class_='RatingStatistics__rating')\n",
        "    average_rating = float(rating_elem.get_text(strip=True)) if rating_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped average_rating: {average_rating}\")\n",
        "\n",
        "    ratings_count_elem = soup.find('span', {'data-testid': 'ratingsCount'})\n",
        "    num_ratings = int(re.search(r'(\\d+[\\d,]*)', ratings_count_elem.get_text(strip=True)).group(1).replace(',', '')) if ratings_count_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped num_ratings: {num_ratings}\")\n",
        "\n",
        "    reviews_count_elem = soup.find('span', {'data-testid': 'reviewsCount'})\n",
        "    num_reviews = int(re.search(r'(\\d+[\\d,]*)', reviews_count_elem.get_text(strip=True)).group(1).replace(',', '')) if reviews_count_elem else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped num_reviews: {num_reviews}\")\n",
        "\n",
        "    distribution = {}\n",
        "    try:\n",
        "        # Wait for histogram\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'RatingsHistogram')))\n",
        "        histogram_bars = soup.find_all('div', class_='RatingsHistogram__bar')\n",
        "        stars = [5, 4, 3, 2, 1]\n",
        "        for i, bar in enumerate(histogram_bars):\n",
        "            if i >= len(stars):\n",
        "                break\n",
        "            fill_elem = bar.find('div', {'data-testid': f'fill-{stars[i]}'})\n",
        "            if fill_elem and 'style' in fill_elem.attrs:\n",
        "                width_match = re.search(r'width:([\\d.]+)%', fill_elem['style'])\n",
        "                if width_match:\n",
        "                    percentage = round(float(width_match.group(1)))  # Round to nearest int\n",
        "                    distribution[f'{stars[i]}_star'] = percentage\n",
        "        print(f\"{datetime.now()}: Scraped rating_distribution: {distribution}\")\n",
        "    except TimeoutException:\n",
        "        print(f\"{datetime.now()}: Failed to load histogram. Debugging - Relevant page source:\\n\" +\n",
        "              str(soup.find('div', class_='ReviewsSectionStatistics__histogram'))[:500] + \"...\")\n",
        "\n",
        "    return {\n",
        "        'average_rating': average_rating,\n",
        "        'num_ratings': num_ratings,\n",
        "        'num_reviews': num_reviews,\n",
        "        'rating_distribution': distribution\n",
        "    }\n",
        "\n",
        "def extract_publication_details(soup):\n",
        "    print(f\"{datetime.now()}: Extracting publication details.\")\n",
        "    pub_info_elem = soup.find('p', {'data-testid': 'publicationInfo'})\n",
        "    pub_text = pub_info_elem.get_text(strip=True) if pub_info_elem else ''\n",
        "    print(f\"{datetime.now()}: Scraped pub_info: {pub_text}\")\n",
        "\n",
        "    year_match = re.search(r'(\\d{4})', pub_text)\n",
        "    pub_date = int(year_match.group(1)) if year_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped pub_date: {pub_date}\")\n",
        "\n",
        "    publisher = pub_text.split('by ')[-1].strip() if 'by ' in pub_text else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped publisher: {publisher}\")\n",
        "\n",
        "    format_elem = soup.find('p', {'data-testid': 'pagesFormat'})\n",
        "    format_text = format_elem.get_text(strip=True) if format_elem else ''\n",
        "    print(f\"{datetime.now()}: Scraped format_info: {format_text}\")\n",
        "\n",
        "    pages_match = re.search(r'(\\d+) pages', format_text)\n",
        "    page_count = int(pages_match.group(1)) if pages_match else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped page_count: {page_count}\")\n",
        "\n",
        "    return {\n",
        "        'pub_info': pub_text,\n",
        "        'pub_date': pub_date,\n",
        "        'publisher': publisher,\n",
        "        'isbn': soup.find('span', itemprop='isbn').text if soup.find('span', itemprop='isbn') else 'N/A',\n",
        "        'format_info': format_text,\n",
        "        'page_count': page_count,\n",
        "        'language': 'English'  # Default or parse if available\n",
        "    }\n",
        "\n",
        "def extract_genres_shelves(driver, soup, shelves_url):  # Updated: Scrape full shelves from shelves page\n",
        "    print(f\"{datetime.now()}: Extracting genres and shelves.\")\n",
        "    # Initial genres from main page\n",
        "    genres = []\n",
        "    genre_section = soup.find('div', class_='BookPageMetadataSection__genres')\n",
        "    if genre_section:\n",
        "        genre_links = genre_section.find_all('a', class_='Button')\n",
        "        genres = [link.text.strip() for link in genre_links[:10]]  # Top 10\n",
        "    primary_genre = genres[0] if genres else 'N/A'\n",
        "    genres_str = '; '.join(genres) if genres else 'N/A'\n",
        "    print(f\"{datetime.now()}: Scraped initial genres (main page): {genres_str}\")\n",
        "\n",
        "    # Navigate to shelves page for full list\n",
        "    all_shelves = 'N/A'\n",
        "    if shelves_url:\n",
        "        print(f\"{datetime.now()}: Navigating to shelves URL: {shelves_url}\")\n",
        "        driver.get(shelves_url)\n",
        "        time.sleep(random.uniform(4, 7))\n",
        "        # Expand on shelves page\n",
        "        try:\n",
        "            shelves_expand = driver.find_elements(By.XPATH, \"//div[@class='TruncatedContent']//button[contains(@aria-label, 'Show all') or contains(text(), 'more')]\")\n",
        "            for btn in shelves_expand:\n",
        "                driver.execute_script(\"arguments[0].click();\", btn)\n",
        "                time.sleep(2)\n",
        "        except:\n",
        "            print(f\"{datetime.now()}: No expanders on shelves page.\")\n",
        "\n",
        "        shelves_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        # Scrape all shelves (assuming listed in <li> or <a> in a shelves container; adjust if needed)\n",
        "        shelves_elements = shelves_soup.select('div.shelfList a, li.shelfItem')  # Common selectors for shelves\n",
        "        all_shelves_list = [elem.text.strip() for elem in shelves_elements if elem.text.strip()]\n",
        "        all_shelves = '; '.join(all_shelves_list) if all_shelves_list else 'N/A'\n",
        "        print(f\"{datetime.now()}: Scraped all shelves (from shelves page): {all_shelves[:200]}...\")  # Truncated print\n",
        "\n",
        "    return {\n",
        "        'genres': genres_str,\n",
        "        'primary_genre': primary_genre,\n",
        "        'all_shelves': all_shelves  # New field\n",
        "    }\n",
        "\n",
        "def extract_characters_and_places(driver, soup, shelves_soup):  # Updated: Parse from shelves page if needed\n",
        "    print(f\"{datetime.now()}: Extracting characters and places from main and shelves page.\")\n",
        "    characters_str = 'N/A'\n",
        "    places_str = 'N/A'\n",
        "    literary_awards = 'N/A'  # New field\n",
        "\n",
        "    # Parse from main soup first\n",
        "    characters_section = soup.find('dt', string=re.compile('Characters', re.I))\n",
        "    if characters_section:\n",
        "        dd = characters_section.find_next_sibling('dd')\n",
        "        if dd:\n",
        "            character_links = dd.select('div.TruncatedContent__text.TruncatedContent__text--small[data-testid=\"contentContainer\"] a')\n",
        "            characters_str = '; '.join([link.text.strip() for link in character_links if link.text.strip()]) or 'N/A'\n",
        "\n",
        "    places_section = soup.find('dt', string=re.compile('Places|Setting', re.I))\n",
        "    if places_section:\n",
        "        dd = places_section.find_next_sibling('dd')\n",
        "        if dd:\n",
        "            place_links = dd.select('div.TruncatedContent__text.TruncatedContent__text--small[data-testid=\"contentContainer\"] a')\n",
        "            places_str = '; '.join([link.text.strip() for link in place_links if link.text.strip()]) or 'N/A'\n",
        "\n",
        "    # Fallback to shelves_soup (work details page)\n",
        "    if (characters_str == 'N/A' or places_str == 'N/A') and shelves_soup:\n",
        "        # Characters\n",
        "        characters_section = shelves_soup.find('dt', string=re.compile('Characters', re.I))\n",
        "        if characters_section:\n",
        "            dd = characters_section.find_next_sibling('dd')\n",
        "            if dd:\n",
        "                character_links = dd.select('div.TruncatedContent__text.TruncatedContent__text--small[data-testid=\"contentContainer\"] a')\n",
        "                characters_str = '; '.join([link.text.strip() for link in character_links if link.text.strip()]) or 'N/A'\n",
        "\n",
        "        # Places (from Setting)\n",
        "        places_section = shelves_soup.find('dt', string=re.compile('Setting|Places', re.I))\n",
        "        if places_section:\n",
        "            dd = places_section.find_next_sibling('dd')\n",
        "            if dd:\n",
        "                place_links = dd.select('div.TruncatedContent__text.TruncatedContent__text--small[data-testid=\"contentContainer\"] a')\n",
        "                places_str = '; '.join([link.text.strip() for link in place_links if link.text.strip()]) or 'N/A'\n",
        "\n",
        "        # Literary awards (new)\n",
        "        awards_section = shelves_soup.find('dt', string=re.compile('Literary awards', re.I))\n",
        "        if awards_section:\n",
        "            dd = awards_section.find_next_sibling('dd')\n",
        "            if dd:\n",
        "                awards_spans = dd.select('span[data-testid=\"award\"] a')  # Updated to select <a> inside spans\n",
        "                literary_awards = '; '.join([span.get_text(strip=True) for span in awards_spans if span.get_text(strip=True)]) or 'N/A'\n",
        "\n",
        "    print(f\"{datetime.now()}: Scraped characters: {characters_str}\")\n",
        "    print(f\"{datetime.now()}: Scraped places: {places_str}\")\n",
        "    print(f\"{datetime.now()}: Scraped literary awards: {literary_awards}\")\n",
        "\n",
        "    return {\n",
        "        'characters': characters_str,\n",
        "        'places': places_str,\n",
        "        'literary_awards': literary_awards  # New field\n",
        "    }\n",
        "\n",
        "\n",
        "def collect_reviews(driver, book_id, max_reviews=50):  # Reduced for testing; change to 100 for full\n",
        "    book_id = str(int(book_id))  # Fix: Ensure clean integer string (removes .0)\n",
        "    reviews_url = f\"https://www.goodreads.com/book/show/{book_id}/reviews?review_filters=has_user_rating\"\n",
        "    print(f\"{datetime.now()}: Navigating to reviews URL: {reviews_url}\")\n",
        "    driver.get(reviews_url)\n",
        "    time.sleep(random.uniform(5, 7))  # Longer initial wait for load\n",
        "\n",
        "    # Wait for reviews container\n",
        "    try:\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'ReviewsSection')))\n",
        "    except TimeoutException:\n",
        "        print(f\"{datetime.now()}: Reviews container not found. Refreshing page once.\")\n",
        "        driver.refresh()\n",
        "        time.sleep(random.uniform(5, 7))\n",
        "\n",
        "    reviews = []\n",
        "    loaded = 0\n",
        "    pbar = tqdm(total=max_reviews, desc=\"Collecting reviews\")\n",
        "    retries = 0\n",
        "    max_retries = 3\n",
        "    while loaded < max_reviews and retries < max_retries:\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        review_elements = soup.find_all('section', class_='ReviewCard')[loaded:]\n",
        "        for rev in review_elements:\n",
        "            rating_elem = rev.find('span', class_='RatingStars')\n",
        "            rating = rating_elem['aria-label'] if rating_elem else 'N/A'\n",
        "            text_elem = rev.find('span', class_='ReviewText')\n",
        "            text = text_elem.get_text(strip=True) if text_elem else 'N/A'\n",
        "            reviews.append({'rating': rating, 'text': text})\n",
        "            loaded += 1\n",
        "            pbar.update(1)\n",
        "            if loaded >= max_reviews:\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            load_more = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Load More') or @aria-label='Load more reviews']\")))\n",
        "            print(f\"{datetime.now()}: Clicking 'Load More' (loaded so far: {loaded})\")\n",
        "            driver.execute_script(\"arguments[0].click();\", load_more)\n",
        "            time.sleep(random.uniform(2, 4) * (retries + 1))  # Exponential backoff\n",
        "            retries = 0\n",
        "        except TimeoutException:\n",
        "            retries += 1\n",
        "            print(f\"{datetime.now()}: No more 'Load More' button or timeout (retry {retries}/{max_retries}).\")\n",
        "            time.sleep(2)\n",
        "            if retries >= max_retries:\n",
        "                print(f\"{datetime.now()}: Max retries reached. Stopping review collection.\")\n",
        "                break\n",
        "    pbar.close()\n",
        "\n",
        "    # Print scraped reviews summary and sample\n",
        "    print(f\"{datetime.now()}: Collected {len(reviews)} reviews.\")\n",
        "    if len(reviews) < max_reviews:\n",
        "        print(f\"{datetime.now()}: WARNING: Fewer reviews collected than max ({max_reviews}). Possible page issue.\")\n",
        "    if reviews:\n",
        "        print(f\"{datetime.now()}: Sample of first 5 reviews:\")\n",
        "        for idx, rev in enumerate(reviews[:5]):\n",
        "            print(f\"Review {idx+1}: Rating = {rev['rating']}, Text = {rev['text'][:100]}...\")  # First 100 chars for brevity\n",
        "\n",
        "    return reviews\n",
        "\n",
        "\n",
        "def enrich_book_metadata(row):\n",
        "    start_time = datetime.now()\n",
        "    print(f\"\\n{start_time}: Starting metadata enrichment for book: {row['title']} (URL: {row['url']})\") # Updated: Use lowercase 'title' and 'url'\n",
        "\n",
        "    for attempt in range(3):  # Increased to 3 retries for page load\n",
        "        try:\n",
        "            driver.get(row['url']) # Updated: Use lowercase 'url'\n",
        "            time.sleep(random.uniform(6, 10))  # Increased initial load time\n",
        "\n",
        "            # Global expand (for main page)\n",
        "            print(f\"{datetime.now()}: Expanding global sections...\")\n",
        "            for expand_attempt in range(3):  # 3 retries for expansion\n",
        "                try:\n",
        "                    # Separate XPaths to avoid invalid selector\n",
        "                    button_xpath = \"//div[contains(@class, 'BookPageMetadataSection')]//button[contains(text(), 'more') or contains(text(), 'More') or @aria-label='Show all' or contains(@class, 'ExpandableContent__button') or @aria-label='Show all items in the list']\"\n",
        "                    a_xpath = \"//a[contains(@href, '/work/shelves/') and contains(@aria-label, 'show all top genres')]\"\n",
        "                    expand_elements = driver.find_elements(By.XPATH, f\"{button_xpath} | {a_xpath}\")  # Use | for union\n",
        "                    if not expand_elements:\n",
        "                        print(f\"{datetime.now()}: No expand elements found (attempt {expand_attempt+1}). Refreshing page.\")\n",
        "                        driver.refresh()\n",
        "                        time.sleep(3)\n",
        "                        continue\n",
        "\n",
        "                    for elem in expand_elements:\n",
        "                        try:\n",
        "                            driver.execute_script(\"arguments[0].scrollIntoView();\", elem)\n",
        "                            WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//span[@tabindex='-1']\")))\n",
        "                            WebDriverWait(driver, 10).until(EC.element_to_be_clickable(elem))\n",
        "                            try:\n",
        "                                elem.click()\n",
        "                                print(f\"{datetime.now()}: Successfully clicked expand element.\")\n",
        "                            except ElementClickInterceptedException:\n",
        "                                print(f\"{datetime.now()}: Click intercepted (attempt {expand_attempt+1}). Using JS fallback.\")\n",
        "                                driver.execute_script(\"arguments[0].click();\", elem)\n",
        "                            except StaleElementReferenceException:\n",
        "                                print(f\"{datetime.now()}: Stale element during click (attempt {expand_attempt+1}). Skipping this element.\")\n",
        "                                continue\n",
        "                            except InvalidSelectorException as ise:\n",
        "                                print(f\"{datetime.now()}: Invalid selector during expand click: {ise} (attempt {expand_attempt+1}). Skipping this element.\")\n",
        "                                continue\n",
        "                            time.sleep(2)\n",
        "                        except TimeoutException:\n",
        "                            print(f\"{datetime.now()}: Timeout on expand element (attempt {expand_attempt+1}). Element type: {elem.tag_name}\")\n",
        "                            continue\n",
        "                        except StaleElementReferenceException:\n",
        "                            print(f\"{datetime.now()}: Stale element detected during expansion (attempt {expand_attempt+1}). Relocating...\")\n",
        "                            break  # Break to re-locate all elements\n",
        "                        except Exception as e:\n",
        "                            print(f\"{datetime.now()}: Unexpected error on expand: {e} (attempt {expand_attempt+1}).\")\n",
        "                    time.sleep(3)  # Delay between attempts\n",
        "                except InvalidSelectorException as e:\n",
        "                    print(f\"{datetime.now()}: Invalid XPath selector: {e}. Skipping expansion.\")\n",
        "                    break\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "            # Updated: Extract shelves URL with more flexible selector\n",
        "            shelves_link = soup.find('a', href=re.compile(r'/work/shelves/'), attrs={'aria-label': re.compile(r'(Tap to show all|show all top genres)', re.I)}) or \\\n",
        "                           soup.find('a', class_=re.compile(r'Button--tag'), string=re.compile(r'\\.\\.\\.show all')) or \\\n",
        "                           soup.find('a', href=re.compile(r'/work/shelves/'))  # Fallback to any matching href\n",
        "            shelves_url = 'https://www.goodreads.com' + shelves_link['href'] if shelves_link else None\n",
        "            if shelves_url:\n",
        "                print(f\"{datetime.now()}: Extracted shelves URL: {shelves_url}\")\n",
        "            else:\n",
        "                genres_section_html = str(soup.find('div', class_='BookPageMetadataSection__genres'))[:500] if soup.find('div', class_='BookPageMetadataSection__genres') else \"\"\n",
        "                print(f\"{datetime.now()}: Failed to extract shelves URL. Debugging - Genres section HTML:\\n{genres_section_html}...\")\n",
        "\n",
        "            # Navigate to shelves page and parse\n",
        "            shelves_soup = None  # Reset\n",
        "            if shelves_url:\n",
        "                driver.get(shelves_url)\n",
        "                time.sleep(random.uniform(4, 7))\n",
        "                # Expand on shelves page\n",
        "                try:\n",
        "                    shelves_expand = driver.find_elements(By.XPATH, \"//div[@class='TruncatedContent']//button[contains(@aria-label, 'Show all') or contains(text(), 'more')]\")\n",
        "                    for btn in shelves_expand:\n",
        "                        driver.execute_script(\"arguments[0].click();\", btn)\n",
        "                        time.sleep(2)\n",
        "                except:\n",
        "                    print(f\"{datetime.now()}: No expanders on shelves page.\")\n",
        "                shelves_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "            metadata = extract_core_identifiers(soup, row['url']) # Updated: Use lowercase 'url'\n",
        "            metadata.update(extract_ratings_reviews(driver, soup))\n",
        "            metadata.update(extract_publication_details(soup))\n",
        "            metadata.update(extract_genres_shelves(driver, soup, shelves_url))\n",
        "            metadata.update(extract_characters_and_places(driver, soup, shelves_soup))\n",
        "            metadata['description'] = extract_abstract(driver, soup)\n",
        "\n",
        "            duration = (datetime.now() - start_time).total_seconds()\n",
        "            print(f\"{datetime.now()}: Metadata enrichment complete for {row['title']} (took {duration:.2f} seconds).\") # Updated: Use lowercase 'title'\n",
        "            return pd.Series(metadata)\n",
        "        except Exception as e:\n",
        "            print(f\"{datetime.now()}: Error loading page for {row['title']} (attempt {attempt+1}): {e}. Retrying...\") # Updated: Use lowercase 'title'\n",
        "            time.sleep(5)\n",
        "    raise ValueError(f\"Failed to enrich metadata for {row['title']} after retries.\") # Updated: Use lowercase 'title'\n",
        "\n",
        "# --- Re-initialize driver at the start of the cell ---\n",
        "print(f\"{datetime.now()}: Re-initializing Selenium driver.\")\n",
        "# Set up Selenium with anti-detection\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36')  # Updated to latest Chrome version\n",
        "\n",
        "# Ensure driver is properly initialized using webdriver-manager\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# Reload cookies and attempt login\n",
        "if not load_cookies(driver, auto_login=True):  # Enable auto-login fallback\n",
        "    raise ValueError(\"Login failed. Cannot proceed with scraping.\")\n",
        "print(f\"{datetime.now()}: Driver re-initialized and login re-attempted.\")\n",
        "# --- End of driver re-initialization ---\n",
        "\n",
        "\n",
        "# Load CSV\n",
        "csv_path = '/content/drive/MyDrive/Romantic_Extened_Dataset_Scraping/romantic_books_final_scraped.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"{datetime.now()}: Loaded DataFrame with {len(df)} rows.\")\n",
        "\n",
        "# For testing: Process only first 3 rows (uncomment to enable)\n",
        "df = df.head(3)  # Enabled for testing\n",
        "print(f\"{datetime.now()}: Testing on sample of {len(df)} books (first 3 rows).\")\n",
        "\n",
        "# Insight: Add success/fail counters (inspired by provided code)\n",
        "success_count = {'abstract': 0, 'characters': 0, 'distribution': 0, 'total': 0}\n",
        "fail_count = {'abstract': 0, 'characters': 0, 'distribution': 0, 'total': 0}\n",
        "\n",
        "# Enrich metadata with progress bar\n",
        "enriched_data = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Enriching metadata for books\"):\n",
        "    try:\n",
        "        enriched = enrich_book_metadata(row)\n",
        "        enriched_data.append(enriched)\n",
        "        # Count successes (check if key fields were scraped successfully)\n",
        "        if enriched.get('description') != 'N/A':\n",
        "            success_count['abstract'] += 1\n",
        "        else:\n",
        "            fail_count['abstract'] += 1\n",
        "        if enriched.get('characters') != 'N/A':\n",
        "            success_count['characters'] += 1\n",
        "        else:\n",
        "            fail_count['characters'] += 1\n",
        "        if enriched.get('rating_distribution'):\n",
        "            success_count['distribution'] += 1\n",
        "        else:\n",
        "            fail_count['distribution'] += 1\n",
        "        success_count['total'] += 1\n",
        "    except Exception as e:\n",
        "        print(f\"{datetime.now()}: Error enriching metadata for {row['title']}: {e}. Skipping.\") # Updated: Use lowercase 'title'\n",
        "        fail_count['total'] += 1\n",
        "df = pd.concat([df] + enriched_data, axis=1)\n",
        "\n",
        "# Collect reviews for each book with progress\n",
        "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting reviews for books\"):\n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        print(f\"\\n{start_time}: Starting review collection for book: {row['title']} (ID: {row['book_id']})\") # Updated: Use lowercase 'title'\n",
        "        df.at[i, 'reviews'] = json.dumps(collect_reviews(driver, row['book_id']))\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"{datetime.now()}: Review collection complete for {row['title']} (took {duration:.2f} seconds).\") # Updated: Use lowercase 'title'\n",
        "        success_count['total'] += 1  # Count as overall success\n",
        "    except Exception as e:\n",
        "        print(f\"{datetime.now()}: Error collecting reviews for {row['title']}: {e}. Skipping.\") # Updated: Use lowercase 'title'\n",
        "        fail_count['total'] += 1\n",
        "\n",
        "# Insight: Print success/fail summary\n",
        "print(f\"{datetime.now()}: Scraping summary - Abstracts: {success_count['abstract']} success, {fail_count['abstract']} fails | \"\n",
        "      f\"Characters: {success_count['characters']} success, {fail_count['characters']} fails | \"\n",
        "      f\"Distributions: {success_count['distribution']} success, {fail_count['distribution']} fails | \"\n",
        "      f\"Total: {success_count['total']} success, {fail_count['total']} fails.\")\n",
        "\n",
        "# Save enriched CSV\n",
        "enriched_path = '/content/drive/MyDrive/Romantic_Extened_Dataset_Scraping/romantic_books_enriched.csv'\n",
        "df.to_csv(enriched_path, index=False)\n",
        "print(f\"{datetime.now()}: Enriched CSV saved to {enriched_path}\")\n",
        "\n",
        "driver.quit()\n",
        "print(f\"{datetime.now()}: Cell 3: Enrichment complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j31oFst1x6FP",
        "outputId": "b667fb17-abfc-4882-f55b-691b7105b2a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:28:04.138573: Re-initializing Selenium driver.\n",
            "\n",
            "=== Starting cookie/login process ===\n",
            "2025-08-07 18:28:07.511420: Navigating to Goodreads homepage for cookie loading (attempt 1).\n",
            "2025-08-07 18:28:10.736139: Cleared existing cookies.\n",
            "2025-08-07 18:28:12.006636: Loaded 13 cookies and refreshed page.\n",
            "2025-08-07 18:28:15.775396: Success: Cookie-based login successful!\n",
            "2025-08-07 18:28:15.775669: Driver re-initialized and login re-attempted.\n",
            "2025-08-07 18:28:19.327494: Loaded DataFrame with 2860 rows.\n",
            "2025-08-07 18:28:19.328821: Testing on sample of 3 books (first 3 rows).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEnriching metadata for books:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2025-08-07 18:28:19.348729: Starting metadata enrichment for book: The Love Hypothesis (Paperback) (URL: https://www.goodreads.com/book/show/56732449-the-love-hypothesis)\n",
            "2025-08-07 18:28:30.295430: Expanding global sections...\n",
            "2025-08-07 18:28:40.875392: Timeout on expand element (attempt 1). Element type: button\n",
            "2025-08-07 18:28:54.376013: Timeout on expand element (attempt 2). Element type: button\n",
            "2025-08-07 18:29:07.411567: Timeout on expand element (attempt 3). Element type: button\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEnriching metadata for books:  33%|███▎      | 1/3 [00:51<01:43, 51.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:29:11.003134: Failed to extract shelves URL. Debugging - Genres section HTML:\n",
            "<div class=\"BookPageMetadataSection__genres\" data-testid=\"genresList\"><ul aria-label=\"Top genres for this book\" class=\"CollapsableList\"><span tabindex=\"-1\"><span class=\"BookPageMetadataSection__genrePlainText\"><span class=\"Text Text__body3 Text__subdued\">Genres</span></span><span class=\"BookPageMetadataSection__genreButton\"><a class=\"Button Button--tag Button--medium\" href=\"https://www.goodreads.com/genres/romance\"><span class=\"Button__labelItem\">Romance</span></a></span><span class=\"BookPageMet...\n",
            "2025-08-07 18:29:11.003403: Extracting core identifiers.\n",
            "2025-08-07 18:29:11.004012: Scraped title: The Love Hypothesis\n",
            "2025-08-07 18:29:11.013335: Scraped author: Ali Hazelwood\n",
            "2025-08-07 18:29:11.013394: Scraped all authors: Ali Hazelwood; Ali Hazelwood\n",
            "2025-08-07 18:29:11.013583: Scraped book_id: 56732449\n",
            "2025-08-07 18:29:11.013604: Extracting ratings and reviews metadata.\n",
            "2025-08-07 18:29:11.014532: Scraped average_rating: 4.11\n",
            "2025-08-07 18:29:11.015392: Scraped num_ratings: 1714606\n",
            "2025-08-07 18:29:11.016040: Scraped num_reviews: 179471\n",
            "2025-08-07 18:29:11.053142: Scraped rating_distribution: {'5_star': 42, '4_star': 36, '3_star': 17, '2_star': 4, '1_star': 2}\n",
            "2025-08-07 18:29:11.053213: Extracting publication details.\n",
            "2025-08-07 18:29:11.053935: Scraped pub_info: First published September 14, 2021\n",
            "2025-08-07 18:29:11.054121: Scraped pub_date: 2021\n",
            "2025-08-07 18:29:11.054139: Scraped publisher: N/A\n",
            "2025-08-07 18:29:11.054794: Scraped format_info: 356 pages, Paperback\n",
            "2025-08-07 18:29:11.054931: Scraped page_count: 356\n",
            "2025-08-07 18:29:11.063224: Extracting genres and shelves.\n",
            "2025-08-07 18:29:11.064517: Scraped initial genres (main page): Romance; Contemporary; Fiction; Fake Dating; Contemporary Romance; Audiobook; Adult\n",
            "2025-08-07 18:29:11.064559: Extracting characters and places from main and shelves page.\n",
            "2025-08-07 18:29:11.072152: Scraped characters: N/A\n",
            "2025-08-07 18:29:11.072214: Scraped places: N/A\n",
            "2025-08-07 18:29:11.072224: Scraped literary awards: N/A\n",
            "2025-08-07 18:29:11.072241: Extracting abstract.\n",
            "2025-08-07 18:29:11.140356: Scraped description: When a fake relationship between scientists meets the irresistible force of attraction, it throws on...\n",
            "2025-08-07 18:29:11.140416: Metadata enrichment complete for The Love Hypothesis (Paperback) (took 51.79 seconds).\n",
            "\n",
            "2025-08-07 18:29:11.145595: Starting metadata enrichment for book: The Hating Game (Paperback) (URL: https://www.goodreads.com/book/show/27213238-the-hating-game)\n",
            "2025-08-07 18:29:26.953307: Expanding global sections...\n",
            "2025-08-07 18:29:37.327060: Timeout on expand element (attempt 1). Element type: button\n",
            "2025-08-07 18:29:50.357187: Timeout on expand element (attempt 2). Element type: button\n",
            "2025-08-07 18:30:03.847529: Timeout on expand element (attempt 3). Element type: button\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEnriching metadata for books:  67%|██████▋   | 2/3 [01:47<00:54, 54.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:30:07.106148: Failed to extract shelves URL. Debugging - Genres section HTML:\n",
            "<div class=\"BookPageMetadataSection__genres\" data-testid=\"genresList\"><ul aria-label=\"Top genres for this book\" class=\"CollapsableList\"><span tabindex=\"-1\"><span class=\"BookPageMetadataSection__genrePlainText\"><span class=\"Text Text__body3 Text__subdued\">Genres</span></span><span class=\"BookPageMetadataSection__genreButton\"><a class=\"Button Button--tag Button--medium\" href=\"https://www.goodreads.com/genres/romance\"><span class=\"Button__labelItem\">Romance</span></a></span><span class=\"BookPageMet...\n",
            "2025-08-07 18:30:07.106554: Extracting core identifiers.\n",
            "2025-08-07 18:30:07.107029: Scraped title: The Hating Game\n",
            "2025-08-07 18:30:07.112386: Scraped author: Sally  Thorne\n",
            "2025-08-07 18:30:07.112435: Scraped all authors: Sally  Thorne; Sally  Thorne\n",
            "2025-08-07 18:30:07.112466: Scraped book_id: 27213238\n",
            "2025-08-07 18:30:07.112484: Extracting ratings and reviews metadata.\n",
            "2025-08-07 18:30:07.113298: Scraped average_rating: 3.86\n",
            "2025-08-07 18:30:07.113885: Scraped num_ratings: 807116\n",
            "2025-08-07 18:30:07.114463: Scraped num_reviews: 75830\n",
            "2025-08-07 18:30:07.133628: Scraped rating_distribution: {'5_star': 34, '4_star': 33, '3_star': 22, '2_star': 8, '1_star': 4}\n",
            "2025-08-07 18:30:07.133697: Extracting publication details.\n",
            "2025-08-07 18:30:07.134427: Scraped pub_info: First published August 9, 2016\n",
            "2025-08-07 18:30:07.134459: Scraped pub_date: 2016\n",
            "2025-08-07 18:30:07.134470: Scraped publisher: N/A\n",
            "2025-08-07 18:30:07.135073: Scraped format_info: 365 pages, Paperback\n",
            "2025-08-07 18:30:07.135092: Scraped page_count: 365\n",
            "2025-08-07 18:30:07.140596: Extracting genres and shelves.\n",
            "2025-08-07 18:30:07.141791: Scraped initial genres (main page): Romance; Contemporary; Enemies To Lovers; Contemporary Romance; Fiction; Chick Lit; Adult\n",
            "2025-08-07 18:30:07.141824: Extracting characters and places from main and shelves page.\n",
            "2025-08-07 18:30:07.149177: Scraped characters: N/A\n",
            "2025-08-07 18:30:07.149230: Scraped places: N/A\n",
            "2025-08-07 18:30:07.149247: Scraped literary awards: N/A\n",
            "2025-08-07 18:30:07.149264: Extracting abstract.\n",
            "2025-08-07 18:30:07.175293: Scraped description: Nemesis (n.)\n",
            "1) An opponent or rival whom a person cannot best or overcome;\n",
            "2) A person’s undoing;\n",
            "3...\n",
            "2025-08-07 18:30:07.175352: Metadata enrichment complete for The Hating Game (Paperback) (took 56.03 seconds).\n",
            "\n",
            "2025-08-07 18:30:07.179510: Starting metadata enrichment for book: Beach Read (Paperback) (URL: https://www.goodreads.com/book/show/52867387-beach-read)\n",
            "2025-08-07 18:30:17.105443: Expanding global sections...\n",
            "2025-08-07 18:30:27.268953: Timeout on expand element (attempt 1). Element type: button\n",
            "2025-08-07 18:30:40.763868: Timeout on expand element (attempt 2). Element type: button\n",
            "2025-08-07 18:30:54.255731: Timeout on expand element (attempt 3). Element type: button\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enriching metadata for books: 100%|██████████| 3/3 [02:38<00:00, 52.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:30:57.529046: Failed to extract shelves URL. Debugging - Genres section HTML:\n",
            "<div class=\"BookPageMetadataSection__genres\" data-testid=\"genresList\"><ul aria-label=\"Top genres for this book\" class=\"CollapsableList\"><span tabindex=\"-1\"><span class=\"BookPageMetadataSection__genrePlainText\"><span class=\"Text Text__body3 Text__subdued\">Genres</span></span><span class=\"BookPageMetadataSection__genreButton\"><a class=\"Button Button--tag Button--medium\" href=\"https://www.goodreads.com/genres/fiction\"><span class=\"Button__labelItem\">Fiction</span></a></span><span class=\"BookPageMet...\n",
            "2025-08-07 18:30:57.529263: Extracting core identifiers.\n",
            "2025-08-07 18:30:57.529724: Scraped title: Beach Read\n",
            "2025-08-07 18:30:57.536156: Scraped author: Emily Henry\n",
            "2025-08-07 18:30:57.536204: Scraped all authors: Emily Henry; Emily Henry\n",
            "2025-08-07 18:30:57.536233: Scraped book_id: 52867387\n",
            "2025-08-07 18:30:57.536247: Extracting ratings and reviews metadata.\n",
            "2025-08-07 18:30:57.537065: Scraped average_rating: 3.98\n",
            "2025-08-07 18:30:57.537671: Scraped num_ratings: 1529938\n",
            "2025-08-07 18:30:57.538435: Scraped num_reviews: 158459\n",
            "2025-08-07 18:30:57.567854: Scraped rating_distribution: {'5_star': 32, '4_star': 41, '3_star': 21, '2_star': 4, '1_star': 1}\n",
            "2025-08-07 18:30:57.567921: Extracting publication details.\n",
            "2025-08-07 18:30:57.569357: Scraped pub_info: First published May 19, 2020\n",
            "2025-08-07 18:30:57.569411: Scraped pub_date: 2020\n",
            "2025-08-07 18:30:57.569423: Scraped publisher: N/A\n",
            "2025-08-07 18:30:57.570079: Scraped format_info: 358 pages, Paperback\n",
            "2025-08-07 18:30:57.570113: Scraped page_count: 358\n",
            "2025-08-07 18:30:57.577932: Extracting genres and shelves.\n",
            "2025-08-07 18:30:57.581813: Scraped initial genres (main page): Fiction; Contemporary; Audiobook; Contemporary Romance; Summer; Adult; Chick Lit\n",
            "2025-08-07 18:30:57.581870: Extracting characters and places from main and shelves page.\n",
            "2025-08-07 18:30:57.593946: Scraped characters: N/A\n",
            "2025-08-07 18:30:57.594013: Scraped places: N/A\n",
            "2025-08-07 18:30:57.594022: Scraped literary awards: N/A\n",
            "2025-08-07 18:30:57.594038: Extracting abstract.\n",
            "2025-08-07 18:30:57.634277: Scraped description: A romance writer who no longer believes in love and a literary writer stuck in a rut engage in a sum...\n",
            "2025-08-07 18:30:57.634333: Metadata enrichment complete for Beach Read (Paperback) (took 50.45 seconds).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCollecting reviews for books:   0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2025-08-07 18:30:57.656323: Starting review collection for book: The Love Hypothesis (Paperback) (ID: 56732449.0)\n",
            "2025-08-07 18:30:57.656481: Navigating to reviews URL: https://www.goodreads.com/book/show/56732449/reviews?review_filters=has_user_rating\n",
            "2025-08-07 18:31:16.137417: Reviews container not found. Refreshing page once.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Collecting reviews:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:31:35.147623: No more 'Load More' button or timeout (retry 1/3).\n",
            "2025-08-07 18:31:47.965931: No more 'Load More' button or timeout (retry 2/3).\n",
            "2025-08-07 18:32:00.558759: No more 'Load More' button or timeout (retry 3/3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting reviews:   0%|          | 0/50 [00:38<?, ?it/s]\n",
            "Collecting reviews for books:   4%|▍         | 1/25 [01:04<25:57, 64.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:32:02.559013: Max retries reached. Stopping review collection.\n",
            "2025-08-07 18:32:02.560191: Collected 0 reviews.\n",
            "2025-08-07 18:32:02.560217: WARNING: Fewer reviews collected than max (50). Possible page issue.\n",
            "2025-08-07 18:32:02.567595: Review collection complete for The Love Hypothesis (Paperback) (took 64.91 seconds).\n",
            "\n",
            "2025-08-07 18:32:02.568775: Starting review collection for book: The Hating Game (Paperback) (ID: 27213238.0)\n",
            "2025-08-07 18:32:02.568852: Navigating to reviews URL: https://www.goodreads.com/book/show/27213238/reviews?review_filters=has_user_rating\n",
            "2025-08-07 18:32:20.129830: Reviews container not found. Refreshing page once.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Collecting reviews:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 18:32:37.454524: No more 'Load More' button or timeout (retry 1/3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCollecting reviews for books:   4%|▍         | 1/25 [01:48<43:27, 108.64s/it]Exception ignored in: <generator object tqdm.__iter__ at 0x7a034706e180>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1302, in close\n",
            "    self.display(pos=0)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1495, in display\n",
            "    self.sp(self.__str__() if msg is None else msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 459, in print_status\n",
            "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 453, in fp_write\n",
            "    fp_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/utils.py\", line 196, in inner\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/iostream.py\", line 488, in flush\n",
            "    if not evt.wait(self.flush_timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-377556797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{start_time}: Starting review collection for book: {row['title']} (ID: {row['book_id']})\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Updated: Use lowercase 'title'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime.now()}: Review collection complete for {row['title']} (took {duration:.2f} seconds).\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Updated: Use lowercase 'title'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-377556797.py\u001b[0m in \u001b[0;36mcollect_reviews\u001b[0;34m(driver, book_id, max_reviews)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mload_more\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"//button[contains(text(), 'Load More') or @aria-label='Load more reviews']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime.now()}: Clicking 'Load More' (loaded so far: {loaded})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arguments[0].click();\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_more\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}