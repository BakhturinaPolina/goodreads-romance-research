{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/scraping_ratings_information_expanded_romantic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Dependencies and Imports**"
      ],
      "metadata": {
        "id": "MtR4aDooI0Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install dependencies and import libraries\n",
        "# Explanation in comments: We install packages if not already present in Colab.\n",
        "# This ensures everything runs smoothly. Selenium needs ChromeDriver setup for headless browsing.\n",
        "# Updated: Using system chromedriver (no webdriver_manager) for Colab compatibility.\n",
        "# Added try-except and more debug prints for troubleshooting.\n",
        "\n",
        "# Install required packages (run this once per Colab session)\n",
        "!pip install beautifulsoup4 requests pandas selenium tqdm\n",
        "# Note: Removed webdriver_manager as it's not needed with system chromedriver\n",
        "\n",
        "# For Selenium in Colab: Install Chrome and ChromeDriver\n",
        "!apt-get update -qq  # Quiet update to avoid verbose output\n",
        "!apt install -y -qq chromium-chromedriver  # Quiet install\n",
        "\n",
        "# Ensure chromedriver is in /usr/bin (Colab often has it here already)\n",
        "# We check if it exists to avoid the \"same file\" error\n",
        "import os\n",
        "chromedriver_path = '/usr/lib/chromium-browser/chromedriver'\n",
        "if os.path.exists(chromedriver_path) and not os.path.exists('/usr/bin/chromedriver'):\n",
        "    !cp {chromedriver_path} /usr/bin\n",
        "else:\n",
        "    print(\"Debug: chromedriver already exists in /usr/bin or source path. Skipping copy.\")\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import json  # For handling JSON-like data (e.g., reviews)\n",
        "import os  # For file operations\n",
        "import sys  # For system paths (debug)\n",
        "\n",
        "# Set up Selenium Chrome options for Colab (headless, no sandbox)\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Run without visible browser window\n",
        "chrome_options.add_argument('--no-sandbox')  # Required for Colab\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Avoid shared memory issues\n",
        "chrome_options.add_argument('--disable-gpu')  # Extra stability for headless mode\n",
        "chrome_options.add_argument('window-size=1920x1080')  # Set a reasonable window size\n",
        "chrome_options.binary_location = '/usr/bin/chromium-browser'  # Point to the installed Chromium\n",
        "\n",
        "# Debug: Print system paths for troubleshooting\n",
        "print(f\"Debug: Python version: {sys.version}\")\n",
        "print(f\"Debug: Chromedriver path: /usr/bin/chromedriver\")\n",
        "print(f\"Debug: Chromium binary: {chrome_options.binary_location}\")\n",
        "\n",
        "# Initialize the WebDriver with try-except for error handling\n",
        "try:\n",
        "    driver = webdriver.Chrome(options=chrome_options)  # Use system chromedriver (no service/manager)\n",
        "    print(\"Debug: WebDriver initialized successfully.\")\n",
        "except WebDriverException as e:\n",
        "    print(f\"Error: Failed to initialize WebDriver: {e}\")\n",
        "    raise  # Re-raise to stop if critical\n",
        "\n",
        "# Debug print: Confirm WebDriver is set up by loading a test page\n",
        "try:\n",
        "    driver.get('https://www.goodreads.com/')\n",
        "    print(f\"Debug: WebDriver test - Page title: {driver.title}\")  # Should print \"Goodreads | Meet your next favorite book\" or similar\n",
        "    print(\"Debug: All dependencies installed and imported successfully. Ready to proceed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Test page load failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qp16X1JxJvwx",
        "outputId": "b4cd9465-bae5-4163-ce8e-3d5788320eb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126484 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126713 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Debug: chromedriver already exists in /usr/bin or source path. Skipping copy.\n",
            "Debug: Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Debug: Chromedriver path: /usr/bin/chromedriver\n",
            "Debug: Chromium binary: /usr/bin/chromium-browser\n",
            "Debug: WebDriver initialized successfully.\n",
            "Debug: WebDriver test - Page title: \n",
            "Debug: All dependencies installed and imported successfully. Ready to proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Subgenres and Global Variables**"
      ],
      "metadata": {
        "id": "PnaSL-eAKNut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define subgenres, URLs, and global configurations\n",
        "# Explanation: Here we list the subgenres and their Goodreads URLs as provided.\n",
        "# We also set configurable variables for scraping limits, delays, etc.\n",
        "# This makes the code flexible—e.g., change MIN_BOOKS_PER_SUBGENRE for testing.\n",
        "\n",
        "# List of subgenres and their shelf/genre URLs\n",
        "subgenres = {\n",
        "    \"Contemporary Romance\": \"https://www.goodreads.com/shelf/show/contemporary-romance\",\n",
        "    \"Historical Romance\": \"https://www.goodreads.com/shelf/show/historical-romance\",\n",
        "    \"Paranormal Romance\": \"https://www.goodreads.com/shelf/show/paranormal-romance\",\n",
        "    \"Romantic Suspense\": \"https://www.goodreads.com/shelf/show/romantic-suspense\",\n",
        "    \"Romantic Fantasy\": \"https://www.goodreads.com/genres/fantasy-romance\",\n",
        "    \"Science Fiction Romance\": \"https://www.goodreads.com/genres/science-fiction-romance\"\n",
        "}\n",
        "\n",
        "# Configurable scraping limits\n",
        "MIN_BOOKS_PER_SUBGENRE = 200  # Minimum to collect (we'll stop if we reach this and can't get more)\n",
        "MAX_BOOKS_PER_SUBGENRE = 300  # Maximum to aim for (if available on pages)\n",
        "MAX_REVIEWS_PER_BOOK = 200  # Cap for reviews if >200; set to None for no cap\n",
        "ALL_REVIEWS = False  # Set to True to scrape ALL reviews regardless of count (warning: can be slow!)\n",
        "DELAY_MIN = 2  # Minimum delay between requests (seconds)\n",
        "DELAY_MAX = 5  # Maximum delay (for randomness to mimic human behavior)\n",
        "\n",
        "# User-agent for requests (to avoid blocks; rotate if needed)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Empty list to store all scraped book data (will convert to DataFrame later)\n",
        "all_books = []\n",
        "\n",
        "# Debug prints: Show configurations\n",
        "print(\"Debug: Subgenres defined:\")\n",
        "for genre, url in subgenres.items():\n",
        "    print(f\"  - {genre}: {url}\")\n",
        "print(f\"Debug: Config - Books per subgenre: {MIN_BOOKS_PER_SUBGENRE} to {MAX_BOOKS_PER_SUBGENRE}\")\n",
        "print(f\"Debug: Config - Max reviews per book: {MAX_REVIEWS_PER_BOOK} (all_reviews={ALL_REVIEWS})\")\n",
        "print(f\"Debug: Config - Delays between requests: {DELAY_MIN}-{DELAY_MAX} seconds\")\n",
        "print(\"Debug: Ready to scrape book lists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zJvaoWY4KSp3",
        "outputId": "530c7429-51a6-41a9-87c4-bc33220e9c8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Subgenres defined:\n",
            "  - Contemporary Romance: https://www.goodreads.com/shelf/show/contemporary-romance\n",
            "  - Historical Romance: https://www.goodreads.com/shelf/show/historical-romance\n",
            "  - Paranormal Romance: https://www.goodreads.com/shelf/show/paranormal-romance\n",
            "  - Romantic Suspense: https://www.goodreads.com/shelf/show/romantic-suspense\n",
            "  - Romantic Fantasy: https://www.goodreads.com/genres/fantasy-romance\n",
            "  - Science Fiction Romance: https://www.goodreads.com/genres/science-fiction-romance\n",
            "Debug: Config - Books per subgenre: 200 to 300\n",
            "Debug: Config - Max reviews per book: 200 (all_reviews=False)\n",
            "Debug: Config - Delays between requests: 2-5 seconds\n",
            "Debug: Ready to scrape book lists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Function to Scrape Book Lists from Subgenre Pages**"
      ],
      "metadata": {
        "id": "z0nhH2DnKjnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Function to scrape book lists from a subgenre URL (Using Selenium for pagination)\n",
        "# Explanation: This function uses Selenium to load the subgenre page, extract books, and click \"next\" for pagination.\n",
        "# It collects unique books by tracking book_ids in a set.\n",
        "# Selenium handles dynamic content and proper pagination better than requests (e.g., avoids repeats by simulating clicks).\n",
        "# Note: This is slower than BS4, so we use waits and delays. Run with care to avoid bans.\n",
        "# Assumes global 'driver' from Cell 1.\n",
        "\n",
        "def scrape_subgenre_books(genre, base_url):\n",
        "    \"\"\"\n",
        "    Scrape book details from a subgenre's paginated list using Selenium.\n",
        "    Args:\n",
        "        genre (str): Subgenre name (e.g., \"Contemporary Romance\")\n",
        "        base_url (str): The starting URL for the subgenre\n",
        "    Returns:\n",
        "        list: List of dicts with book info (title, author, url, book_id, subgenre)\n",
        "    \"\"\"\n",
        "    collected_books = []  # List to hold books for this subgenre\n",
        "    seen_ids = set()  # Set to track unique book_ids and skip duplicates\n",
        "    current_url = base_url  # Start with the base URL\n",
        "    page = 1  # Track page for debugging\n",
        "    print(f\"Debug: Starting Selenium scrape for {genre} at {base_url}\")\n",
        "\n",
        "    try:\n",
        "        # Load the initial page\n",
        "        driver.get(current_url)\n",
        "        time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))  # Initial delay for page load\n",
        "        print(f\"Debug: Loaded initial page: {driver.current_url}\")\n",
        "\n",
        "        while len(collected_books) < MAX_BOOKS_PER_SUBGENRE:\n",
        "            # Wait for book items to load (use WebDriverWait for reliability)\n",
        "            try:\n",
        "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'elementList')))\n",
        "                print(f\"Debug: Page {page} loaded successfully.\")\n",
        "            except TimeoutException:\n",
        "                print(f\"Debug: Timeout waiting for book items on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # Parse the page source with BeautifulSoup for easier extraction\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "            # Find all book items on the page\n",
        "            if \"shelf/show\" in base_url:\n",
        "                book_items = soup.find_all('div', class_='elementList')  # For shelf pages\n",
        "            else:\n",
        "                book_items = soup.find_all('div', class_='grid-item')  # For genre pages (adjust if needed)\n",
        "\n",
        "            if not book_items:\n",
        "                print(f\"Debug: No book items found on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Found {len(book_items)} book items on page {page}\")\n",
        "\n",
        "            added_this_page = 0\n",
        "            for item in book_items:\n",
        "                # Extract title\n",
        "                title_tag = item.find('a', class_='bookTitle')\n",
        "                title = title_tag.text.strip() if title_tag else None\n",
        "\n",
        "                # Extract author\n",
        "                author_tag = item.find('a', class_='authorName')\n",
        "                author = author_tag.text.strip() if author_tag else None\n",
        "\n",
        "                # Extract partial URL and book_id\n",
        "                if title_tag and title_tag['href']:\n",
        "                    url_partial = title_tag['href']\n",
        "                    full_url = f\"https://www.goodreads.com{url_partial.split('?')[0]}\"  # Clean URL\n",
        "                    book_id_match = re.search(r'/show/(\\d+)', url_partial)\n",
        "                    book_id = book_id_match.group(1) if book_id_match else None\n",
        "                else:\n",
        "                    full_url = None\n",
        "                    book_id = None\n",
        "\n",
        "                # Add if valid and unique\n",
        "                if book_id and title and author and book_id not in seen_ids:\n",
        "                    seen_ids.add(book_id)\n",
        "                    collected_books.append({\n",
        "                        'book_id': book_id,\n",
        "                        'title': title,\n",
        "                        'author': author,\n",
        "                        'url': full_url,\n",
        "                        'subgenre': genre\n",
        "                    })\n",
        "                    print(f\"Debug: Added book - ID: {book_id}, Title: {title[:50]}..., Author: {author}, URL: {full_url}\")\n",
        "                    added_this_page += 1\n",
        "                elif book_id in seen_ids:\n",
        "                    print(f\"Debug: Skipped duplicate book ID: {book_id}\")\n",
        "                else:\n",
        "                    print(\"Debug: Skipped invalid book (missing title/author/ID)\")\n",
        "\n",
        "                # Stop if max reached\n",
        "                if len(collected_books) >= MAX_BOOKS_PER_SUBGENRE:\n",
        "                    print(f\"Debug: Reached max books ({MAX_BOOKS_PER_SUBGENRE}) for {genre}.\")\n",
        "                    break\n",
        "\n",
        "            if added_this_page == 0:\n",
        "                print(f\"Debug: No new unique books on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # Find and click \"next\" button for pagination\n",
        "            try:\n",
        "                next_button = WebDriverWait(driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.XPATH, \"//a[@class='next_page' and not(contains(@class, 'disabled'))]\"))\n",
        "                )\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)  # Scroll to button\n",
        "                next_button.click()\n",
        "                print(f\"Debug: Clicked 'next' button. Moving to page {page + 1}\")\n",
        "                time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))  # Delay after click\n",
        "                page += 1\n",
        "            except TimeoutException:\n",
        "                print(f\"Debug: No clickable 'next' button found on page {page}. End of list.\")\n",
        "                break\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error: Failed to click next: {e}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Unique books collected so far for {genre}: {len(collected_books)}\")\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error: Selenium error for {genre}: {e}\")\n",
        "\n",
        "    # Check minimum\n",
        "    if len(collected_books) < MIN_BOOKS_PER_SUBGENRE:\n",
        "        print(f\"Warning: Only collected {len(collected_books)} unique books for {genre} (less than min {MIN_BOOKS_PER_SUBGENRE})\")\n",
        "\n",
        "    print(f\"Debug: Finished scraping {genre}. Total unique books: {len(collected_books)}\")\n",
        "    return collected_books\n",
        "\n",
        "# Example usage (for testing): Scrape one subgenre and add to all_books\n",
        "all_books = []  # Reset for testing\n",
        "test_genre = \"Contemporary Romance\"\n",
        "all_books.extend(scrape_subgenre_books(test_genre, subgenres[test_genre]))\n",
        "print(f\"Debug: Total books after test scrape: {len(all_books)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgRzPiaNQbh7",
        "outputId": "30aa1a48-4e12-40dd-a182-68d8118b1b21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Starting Selenium scrape for Contemporary Romance at https://www.goodreads.com/shelf/show/contemporary-romance\n",
            "Debug: Loaded initial page: https://www.goodreads.com/shelf/show/contemporary-romance\n",
            "Debug: Timeout waiting for book items on page 1. Stopping.\n",
            "Warning: Only collected 0 unique books for Contemporary Romance (less than min 200)\n",
            "Debug: Finished scraping Contemporary Romance. Total unique books: 0\n",
            "Debug: Total books after test scrape: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}