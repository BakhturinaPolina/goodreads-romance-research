{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/scraping_ratings_information_expanded_romantic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Dependencies and Imports**"
      ],
      "metadata": {
        "id": "MtR4aDooI0Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install dependencies and import libraries (Updated with anti-detection for Selenium)\n",
        "# Explanation in comments: We install packages if not already present in Colab.\n",
        "# This ensures everything runs smoothly. Selenium needs ChromeDriver setup for headless browsing.\n",
        "# Updated: Added options to hide bot detection (e.g., disable automation flags, custom user-agent) to avoid empty pages on Goodreads.\n",
        "\n",
        "# Install required packages (run this once per Colab session)\n",
        "!pip install beautifulsoup4 requests pandas selenium tqdm\n",
        "# Note: Removed webdriver_manager as it's not needed with system chromedriver\n",
        "\n",
        "# For Selenium in Colab: Install Chrome and ChromeDriver\n",
        "!apt-get update -qq  # Quiet update to avoid verbose output\n",
        "!apt install -y -qq chromium-chromedriver  # Quiet install\n",
        "\n",
        "# Ensure chromedriver is in /usr/bin (Colab often has it here already)\n",
        "import os\n",
        "chromedriver_path = '/usr/lib/chromium-browser/chromedriver'\n",
        "if os.path.exists(chromedriver_path) and not os.path.exists('/usr/bin/chromedriver'):\n",
        "    !cp {chromedriver_path} /usr/bin\n",
        "else:\n",
        "    print(\"Debug: chromedriver already exists in /usr/bin or source path. Skipping copy.\")\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import json  # For handling JSON-like data (e.g., reviews)\n",
        "import os  # For file operations\n",
        "import sys  # For system paths (debug)\n",
        "\n",
        "# Set up Selenium Chrome options for Colab (headless, no sandbox) with anti-detection\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Run without visible browser window\n",
        "chrome_options.add_argument('--no-sandbox')  # Required for Colab\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Avoid shared memory issues\n",
        "chrome_options.add_argument('--disable-gpu')  # Extra stability for headless mode\n",
        "chrome_options.add_argument('window-size=1920x1080')  # Set a reasonable window size\n",
        "chrome_options.binary_location = '/usr/bin/chromium-browser'  # Point to the installed Chromium\n",
        "\n",
        "# Anti-detection options (to avoid bot blocks and empty pages)\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # Hide Selenium flag\n",
        "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')  # Mimic real browser user-agent\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Exclude automation switches\n",
        "chrome_options.add_experimental_option('useAutomationExtension', False)  # Disable automation extension\n",
        "\n",
        "# Debug: Print system paths for troubleshooting\n",
        "print(f\"Debug: Python version: {sys.version}\")\n",
        "print(f\"Debug: Chromedriver path: /usr/bin/chromedriver\")\n",
        "print(f\"Debug: Chromium binary: {chrome_options.binary_location}\")\n",
        "\n",
        "# Initialize the WebDriver with try-except for error handling\n",
        "try:\n",
        "    driver = webdriver.Chrome(options=chrome_options)  # Use system chromedriver (no service/manager)\n",
        "    print(\"Debug: WebDriver initialized successfully.\")\n",
        "except WebDriverException as e:\n",
        "    print(f\"Error: Failed to initialize WebDriver: {e}\")\n",
        "    raise  # Re-raise to stop if critical\n",
        "\n",
        "# Debug print: Confirm WebDriver is set up by loading a test page\n",
        "try:\n",
        "    driver.get('https://www.goodreads.com/')  # Test with Goodreads home (as in your code)\n",
        "    print(f\"Debug: WebDriver test - Page title: {driver.title}\")  # Should print \"Goodreads | Meet your next favorite book\"\n",
        "    print(f\"Debug: Test page source sample: {driver.page_source[:500]}\")  # Print sample to verify content\n",
        "    print(\"Debug: All dependencies installed and imported successfully. Ready to proceed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Test page load failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGBUHJLwVr31",
        "outputId": "557d694b-a479-4c63-f710-fa8450a42a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126484 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126713 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Subgenres and Global Variables**"
      ],
      "metadata": {
        "id": "PnaSL-eAKNut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define subgenres, URLs, and global configurations\n",
        "# Explanation: Here we list the subgenres and their Goodreads URLs as provided.\n",
        "# We also set configurable variables for scraping limits, delays, etc.\n",
        "# This makes the code flexible—e.g., change MIN_BOOKS_PER_SUBGENRE for testing.\n",
        "\n",
        "# List of subgenres and their shelf/genre URLs\n",
        "subgenres = {\n",
        "    \"Contemporary Romance\": \"https://www.goodreads.com/shelf/show/contemporary-romance\",\n",
        "    \"Historical Romance\": \"https://www.goodreads.com/shelf/show/historical-romance\",\n",
        "    \"Paranormal Romance\": \"https://www.goodreads.com/shelf/show/paranormal-romance\",\n",
        "    \"Romantic Suspense\": \"https://www.goodreads.com/shelf/show/romantic-suspense\",\n",
        "    \"Romantic Fantasy\": \"https://www.goodreads.com/genres/fantasy-romance\",\n",
        "    \"Science Fiction Romance\": \"https://www.goodreads.com/genres/science-fiction-romance\"\n",
        "}\n",
        "\n",
        "# Configurable scraping limits\n",
        "MIN_BOOKS_PER_SUBGENRE = 200  # Minimum to collect (we'll stop if we reach this and can't get more)\n",
        "MAX_BOOKS_PER_SUBGENRE = 300  # Maximum to aim for (if available on pages)\n",
        "MAX_REVIEWS_PER_BOOK = 200  # Cap for reviews if >200; set to None for no cap\n",
        "ALL_REVIEWS = False  # Set to True to scrape ALL reviews regardless of count (warning: can be slow!)\n",
        "DELAY_MIN = 2  # Minimum delay between requests (seconds)\n",
        "DELAY_MAX = 5  # Maximum delay (for randomness to mimic human behavior)\n",
        "\n",
        "# User-agent for requests (to avoid blocks; rotate if needed)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Empty list to store all scraped book data (will convert to DataFrame later)\n",
        "all_books = []\n",
        "\n",
        "# Debug prints: Show configurations\n",
        "print(\"Debug: Subgenres defined:\")\n",
        "for genre, url in subgenres.items():\n",
        "    print(f\"  - {genre}: {url}\")\n",
        "print(f\"Debug: Config - Books per subgenre: {MIN_BOOKS_PER_SUBGENRE} to {MAX_BOOKS_PER_SUBGENRE}\")\n",
        "print(f\"Debug: Config - Max reviews per book: {MAX_REVIEWS_PER_BOOK} (all_reviews={ALL_REVIEWS})\")\n",
        "print(f\"Debug: Config - Delays between requests: {DELAY_MIN}-{DELAY_MAX} seconds\")\n",
        "print(\"Debug: Ready to scrape book lists.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zJvaoWY4KSp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Function to Scrape Book Lists from Subgenre Pages**"
      ],
      "metadata": {
        "id": "z0nhH2DnKjnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Function to scrape book lists from a subgenre URL (Using Selenium for pagination)\n",
        "# Explanation: This function uses Selenium to load the subgenre page, extract books, and click \"next\" for pagination.\n",
        "# It collects unique books by tracking book_ids in a set.\n",
        "# Selenium handles dynamic content and proper pagination better than requests (e.g., avoids repeats by simulating clicks).\n",
        "# Note: This is slower than BS4, so we use waits and delays. Run with care to avoid bans.\n",
        "# Assumes global 'driver' from Cell 1. We parse with BeautifulSoup after loading for easier tag finding.\n",
        "# We scroll to the next button and use execute_script for reliable clicks in headless mode.\n",
        "# Adapted from your provided code: Added handling for potential overlays (wait for invisibility), JS click fallback,\n",
        "# longer timeouts, anti-detection options (user-agent), and page source print on timeout for debugging.\n",
        "# New: Added a retry on timeout (up to 2 times), an initial test load of Goodreads home for verification, robust button XPath, and debug for button presence.\n",
        "# Integrated Login: Defines and calls login_to_goodreads at the start with prompts for credentials (secure via getpass).\n",
        "\n",
        "from getpass import getpass  # For secure password input (import here if not in Cell 1)\n",
        "\n",
        "def scrape_subgenre_books(genre, base_url):\n",
        "    \"\"\"\n",
        "    Scrape book details from a subgenre's paginated list using Selenium.\n",
        "    Args:\n",
        "        genre (str): Subgenre name (e.g., \"Contemporary Romance\")\n",
        "        base_url (str): The starting URL for the subgenre\n",
        "    Returns:\n",
        "        list: List of dicts with book info (title, author, url, book_id, subgenre)\n",
        "    \"\"\"\n",
        "    def login_to_goodreads():\n",
        "        \"\"\"\n",
        "        Log in to Goodreads using Selenium (nested function for integration).\n",
        "        Prompts for credentials securely.\n",
        "        \"\"\"\n",
        "        print(\"Debug: Starting login to Goodreads.\")\n",
        "\n",
        "        # Check if already logged in (look for personal menu or profile name)\n",
        "        try:\n",
        "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'siteHeader__personalMenu')))  # Example: personal menu if logged in\n",
        "            print(\"Debug: Already logged in (profile menu found). Skipping login.\")\n",
        "            return True  # Success\n",
        "        except TimeoutException:\n",
        "            print(\"Debug: Not logged in. Proceeding with login.\")\n",
        "\n",
        "        # Prompt for credentials securely\n",
        "        username = input(\"Enter your Goodreads username/email: \")  # Prompt for username/email\n",
        "        password = getpass(\"Enter your Goodreads password: \")  # Hidden password input\n",
        "\n",
        "        # Navigate to login page\n",
        "        driver.get('https://www.goodreads.com/user/sign_in')\n",
        "        time.sleep(random.uniform(4, 6))  # Longer delay for load (increased for reliability)\n",
        "\n",
        "        # Debug: Print title and source sample after load\n",
        "        print(f\"Debug: Login page title: {driver.title}\")  # Should be \"Sign In\" or similar\n",
        "        print(f\"Debug: Login page source sample: {driver.page_source[:1000]}\")  # Inspect for form elements\n",
        "\n",
        "        max_retries = 3  # Increased retries if form doesn't load\n",
        "        retry_count = 0\n",
        "        form_loaded = False\n",
        "\n",
        "        while retry_count < max_retries and not form_loaded:\n",
        "            # Wait for and fill email field (try fallback selectors: name > ID > XPath)\n",
        "            try:\n",
        "                # Try by name (robust)\n",
        "                email_field = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.NAME, 'user[email]')))  # Primary: name-based\n",
        "                email_field.send_keys(username)  # Enter username/email\n",
        "                print(f\"Debug: Entered username using name selector on attempt {retry_count + 1}.\")\n",
        "                form_loaded = True  # Flag success\n",
        "            except TimeoutException:\n",
        "                try:\n",
        "                    # Fallback to ID\n",
        "                    email_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'user_email')))\n",
        "                    email_field.send_keys(username)\n",
        "                    print(f\"Debug: Entered username using ID fallback on attempt {retry_count + 1}.\")\n",
        "                    form_loaded = True\n",
        "                except TimeoutException:\n",
        "                    try:\n",
        "                        # Final fallback to XPath (for type='email')\n",
        "                        email_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//input[@type='email']\")))  # Generic email input\n",
        "                        email_field.send_keys(username)\n",
        "                        print(f\"Debug: Entered username using XPath fallback on attempt {retry_count + 1}.\")\n",
        "                        form_loaded = True\n",
        "                    except TimeoutException:\n",
        "                        retry_count += 1\n",
        "                        print(f\"Debug: Timeout waiting for email field (attempt {retry_count}). Refreshing and retrying...\")\n",
        "                        print(f\"Debug: Page source on failure: {driver.page_source[:1000]}\")  # Print for diagnosis\n",
        "                        driver.refresh()  # Refresh page\n",
        "                        time.sleep(6)  # Longer delay before retry\n",
        "                        if retry_count >= max_retries:\n",
        "                            print(\"Error: Max retries exceeded for email field. Check if form is in source (search for 'email' or 'user[email]').\")\n",
        "                            if 'email' in driver.page_source.lower():\n",
        "                                print(\"Debug: Form keyword found in source—possible JS load issue.\")\n",
        "                            else:\n",
        "                                print(\"Debug: No form keyword in source—page may be blocked.\")\n",
        "                            return False  # Failure\n",
        "\n",
        "        if not form_loaded:\n",
        "            return False  # Exit if form still not loaded\n",
        "\n",
        "        # Fill password field (similar fallbacks)\n",
        "        try:\n",
        "            # Primary: name\n",
        "            password_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'user[password]')))  # Name-based\n",
        "            password_field.send_keys(password)  # Enter password\n",
        "            print(\"Debug: Entered password using name selector.\")\n",
        "        except TimeoutException:\n",
        "            try:\n",
        "                # Fallback to ID\n",
        "                password_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'user_password')))\n",
        "                password_field.send_keys(password)\n",
        "                print(\"Debug: Entered password using ID fallback.\")\n",
        "            except TimeoutException:\n",
        "                try:\n",
        "                    # Fallback to XPath\n",
        "                    password_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//input[@type='password']\")))\n",
        "                    password_field.send_keys(password)\n",
        "                    print(\"Debug: Entered password using XPath fallback.\")\n",
        "                except TimeoutException:\n",
        "                    print(\"Error: Timeout waiting for password field across all fallbacks.\")\n",
        "                    print(f\"Debug: Page source on failure: {driver.page_source[:1000]}\")\n",
        "                    return False\n",
        "\n",
        "        # Click submit button (with JS fallback, as in your code)\n",
        "        try:\n",
        "            submit_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, 'next')))  # Button name (or update to class/value if changed)\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", submit_button)  # Scroll to button\n",
        "            try:\n",
        "                submit_button.click()  # Direct click\n",
        "            except:\n",
        "                print(\"Debug: Direct submit click failed; using JS click.\") # Corrected line\n",
        "                driver.execute_script(\"arguments[0].click();\", submit_button)  # Fallback JS click\n",
        "            print(\"Debug: Submitted login form.\")\n",
        "        except TimeoutException:\n",
        "            print(\"Error: Timeout waiting for submit button.\")\n",
        "            print(f\"Debug: Page source on failure: {driver.page_source[:1000]}\")\n",
        "            return False\n",
        "\n",
        "        # Wait for login to complete (e.g., redirect to home or profile)\n",
        "        time.sleep(random.uniform(5, 8))  # Delay for redirect\n",
        "        if \"Goodreads\" in driver.title:\n",
        "            print(\"Debug: Login successful! Current title: \", driver.title)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Warning: Login may have failed. Current title: \", driver.title)\n",
        "            print(f\"Debug: Post-login source sample: {driver.page_source[:1000]}\")  # Check for errors\n",
        "            return False\n",
        "\n",
        "    collected_books = []  # List to hold books for this subgenre (each a dict with details)\n",
        "    seen_ids = set()  # Set to track unique book_ids and skip duplicates (prevents adding the same book multiple times)\n",
        "    current_url = base_url  # Start with the base URL; we'll update it as we paginate\n",
        "    page = 1  # Track page number for debugging and logging\n",
        "    max_retries = 3  # Define max_retries here to fix NameError\n",
        "    print(f\"Debug: Starting Selenium scrape for {genre} at {base_url}\")  # Debug print to show start\n",
        "\n",
        "    try:\n",
        "        # Call login before scraping\n",
        "        if not login_to_goodreads():\n",
        "            print(\"Error: Login failed. Scraping without login (limited to 1 page).\")\n",
        "\n",
        "        # Test load: Try loading Goodreads home first to verify driver (as in your code)\n",
        "        driver.get('https://www.goodreads.com/')  # Load home page\n",
        "        time.sleep(3)  # Short delay\n",
        "        home_title = driver.title  # Get title\n",
        "        print(f\"Debug: Test load - Goodreads home title: {home_title}\")  # Should be \"Goodreads | Meet your next favorite book\"\n",
        "        if \"Goodreads\" not in home_title:\n",
        "            print(\"Warning: Test load failed - Driver may not be loading content properly.\")  # Alert if test fails\n",
        "\n",
        "        # Now load the actual subgenre page\n",
        "        driver.get(current_url)\n",
        "        time.sleep(random.uniform(3, 6))  # Slightly longer initial delay for full load (adjusted from your code's time.sleep(2))\n",
        "        print(f\"Debug: Loaded initial page: {driver.current_url}\")  # Confirm the page loaded\n",
        "        print(f\"Debug: Initial page source sample: {driver.page_source[:500]}\")  # Print sample to verify content (new debug)\n",
        "\n",
        "        # Main loop: Continue until we have enough books or no more pages\n",
        "        while len(collected_books) < MAX_BOOKS_PER_SUBGENRE:\n",
        "            retry_count = 0  # Reset retry counter for this page\n",
        "            page_loaded = False  # Flag for successful load\n",
        "            while retry_count < max_retries and not page_loaded:\n",
        "                # Wait for book items to appear on the page (Goodreads loads dynamically)\n",
        "                try:\n",
        "                    # Increased timeout to 30s for slower loads; wait for presence of elementList\n",
        "                    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'elementList')))  # Wait for at least one book container\n",
        "                    print(f\"Debug: Page {page} loaded successfully on attempt {retry_count + 1}.\")  # Success message\n",
        "                    page_loaded = True  # Flag as loaded\n",
        "                except TimeoutException:\n",
        "                    retry_count += 1  # Increment retry\n",
        "                    print(f\"Debug: Timeout on page {page}, attempt {retry_count}. Retrying...\")  # Log retry\n",
        "                    time.sleep(5)  # Wait before retry\n",
        "                    driver.refresh()  # Refresh the page on retry\n",
        "                    if retry_count >= max_retries:\n",
        "                        print(f\"Debug: Max retries exceeded for page {page}. Printing page source for diagnosis...\")  # Log failure\n",
        "                        print(driver.page_source[:1000])  # Print first 1000 chars of HTML to inspect\n",
        "                        print(\"Debug: Stopping due to persistent timeout.\")  # Stop the loop\n",
        "                        return collected_books  # Early return if failed\n",
        "\n",
        "            if not page_loaded:\n",
        "                break  # Exit if still not loaded after retries\n",
        "\n",
        "            # Check for and handle potential overlays before parsing (adapted from your extract_characters function)\n",
        "            try:\n",
        "                WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//span[@tabindex='-1']\")))  # Wait for any overlay to disappear\n",
        "                print(\"Debug: Overlay handled (invisible).\")  # Log success\n",
        "            except TimeoutException:\n",
        "                print(\"Debug: No overlay found or it didn't disappear; proceeding anyway.\")  # Not critical, continue\n",
        "\n",
        "            # Parse the current page source with BeautifulSoup (easier for complex HTML extraction than pure Selenium)\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')  # Get the full HTML after JS loads\n",
        "\n",
        "            # Find all book items on the page (adjust selector based on page type)\n",
        "            if \"shelf/show\" in base_url:\n",
        "                book_items = soup.find_all('div', class_='elementList')  # For shelf pages like Contemporary Romance\n",
        "            else:\n",
        "                book_items = soup.find_all('div', class_='grid-item')  # For genre pages (e.g., Romantic Fantasy); adjust if structure differs\n",
        "\n",
        "            # If no items found, stop the loop\n",
        "            if not book_items:\n",
        "                print(f\"Debug: No book items found on page {page}. Stopping.\")  # Debug if selector fails\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Found {len(book_items)} book items on page {page}\")  # Show how many potential books found\n",
        "\n",
        "            added_this_page = 0  # Counter for new books added this page (to detect if we're at the end)\n",
        "            for item in book_items:  # Loop through each potential book element\n",
        "                # Extract title from the item\n",
        "                title_tag = item.find('a', class_='bookTitle')  # Find the <a> tag with class tempore 'bookTitle'\n",
        "                title = title_tag.text.strip() if title_tag else None  # Strip whitespace; None if not found\n",
        "\n",
        "                # Extract author\n",
        "                author_tag = item.find('a', class_='authorName')  # Find the author link\n",
        "                author = author_tag.text.strip() if author_tag else None  # Strip and handle missing\n",
        "\n",
        "                # Extract partial URL and book_id\n",
        "                if title_tag and title_tag['href']:  # Check if tag exists and has href\n",
        "                    url_partial = title_tag['href']  # Get the relative URL\n",
        "                    full_url = f\"https://www.goodreads.com{url_partial.split('?')[0]}\"  # Build full clean URL (remove query params)\n",
        "                    book_id_match = re.search(r'/show/(\\d+)', url_partial)  # Regex to extract numeric ID from URL\n",
        "                    book_id = book_id_match.group(1) if book_id_match else None  # Get the ID or None\n",
        "                else:\n",
        "                    full_url = None\n",
        "                    book_id = None\n",
        "\n",
        "                # Add the book if it's valid (has ID, title, author) and not already seen\n",
        "                if book_id and title and author and book_id not in seen_ids:\n",
        "                    seen_ids.add(book_id)  # Mark as seen\n",
        "                    collected_books.append({  # Add dict to list\n",
        "                        'book_id': book_id,\n",
        "                        'title': title,\n",
        "                        'author': author,\n",
        "                        'url': full_url,\n",
        "                        'subgenre': genre\n",
        "                    })\n",
        "                    print(f\"Debug: Added book - ID: {book_id}, Title: {title[:50]}..., Author: {author}, URL: {full_url}\")  # Truncate title for readability\n",
        "                    added_this_page += 1  # Increment counter\n",
        "                elif book_id in seen_ids:\n",
        "                    print(f\"Debug: Skipped duplicate book ID: {book_id}\")  # Log duplicates\n",
        "                else:\n",
        "                    print(\"Debug: Skipped invalid book (missing title/author/ID)\")  # Log invalid items\n",
        "\n",
        "                # Check if we've reached the max books limit\n",
        "                if len(collected_books) >= MAX_BOOKS_PER_SUBGENRE:\n",
        "                    print(f\"Debug: Reached max books ({MAX_BOOKS_PER_SUBGENRE}) for {genre}.\")\n",
        "                    break  # Exit the for loop\n",
        "\n",
        "            # If no new books were added this page, likely end of unique content\n",
        "            if added_this_page == 0:\n",
        "                print(f\"Debug: No new unique books on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # Check for and handle overlays again before button (extra safety)\n",
        "            try:\n",
        "                WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//span[@tabindex='-1']\")))  # Re-check\n",
        "            except TimeoutException:\n",
        "                print(\"Debug: Re-check for overlay before button; none found.\")\n",
        "\n",
        "            # Find and click the \"next\" button to go to the next page (updated with more robust XPath and debug)\n",
        "            try:\n",
        "                # Wait for button visibility first (new: ensures it's in view)\n",
        "                next_button = WebDriverWait(driver, 20).until(  # Longer timeout for button\n",
        "                    EC.visibility_of_element_located((By.XPATH, \"//a[contains(@class, 'next_page') and @rel='next']\"))  # Robust XPath with rel='next'\n",
        "                )\n",
        "                print(\"Debug: 'Next' button found and visible.\")  # Confirm presence\n",
        "\n",
        "                # Scroll and click (with fallback)\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)  # Scroll to ensure it's visible\n",
        "                try:\n",
        "                    next_button.click()  # Direct click\n",
        "                except:\n",
        "                    print(\"Debug: Direct click failed; using JS click.\")  # Fallback if direct fails\n",
        "                    driver.execute_script(\"arguments[0].click();\", next_button)  # JS click as in your code\n",
        "                print(f\"Debug: Clicked 'next' button. Moving to page {page + 1}\")  # Confirm action\n",
        "                time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))  # Delay after click to let new page load\n",
        "                page += 1  # Increment page counter\n",
        "            except TimeoutException:\n",
        "                print(f\"Debug: Timeout waiting for 'next' button on page {page}. Printing page source sample for diagnosis...\")  # Log failure\n",
        "                print(driver.page_source[:1000])  # Print sample to check if button is there but not visible\n",
        "                print(f\"Debug: No clickable 'next' button found. End of list or load issue.\")  # If no button, stop\n",
        "                break\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error: Failed to click next: {e}. Stopping.\")  # Handle click errors\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Unique books collected so far for {genre}: {len(collected_books)}\")  # Progress update\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error: Selenium error for {genre}: {e}\")  # Catch general Selenium errors\n",
        "\n",
        "    # Check if we collected enough (warn if below minimum)\n",
        "    if len(collected_books) < MIN_BOOKS_PER_SUBGENRE:\n",
        "        print(f\"Warning: Only collected {len(collected_books)} unique books for {genre} (less than min {MIN_BOOKS_PER_SUBGENRE})\")\n",
        "\n",
        "    print(f\"Debug: Finished scraping {genre}. Total unique books: {len(collected_books)}\")  # Final message\n",
        "    return collected_books  # Return the list of books\n",
        "\n",
        "# Example usage (for testing): Scrape one subgenre and add to all_books\n",
        "# Reset all_books for fresh test; change test_genre if needed\n",
        "all_books = []  # Reset for testing\n",
        "test_genre = \"Contemporary Romance\"\n",
        "all_books.extend(scrape_subgenre_books(test_genre, subgenres[test_genre]))  # Call the function and add results\n",
        "print(f\"Debug: Total books after test scrape: {len(all_books)}\")  # Show total collected"
      ],
      "metadata": {
        "id": "qz4I_6JwewOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}