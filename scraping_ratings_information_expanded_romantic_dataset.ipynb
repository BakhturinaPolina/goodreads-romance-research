{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/scraping_ratings_information_expanded_romantic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Dependencies and Imports**"
      ],
      "metadata": {
        "id": "MtR4aDooI0Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install dependencies and import libraries (Updated with anti-detection for Selenium)\n",
        "# Explanation in comments: We install packages if not already present in Colab.\n",
        "# This ensures everything runs smoothly. Selenium needs ChromeDriver setup for headless browsing.\n",
        "# Updated: Added options to hide bot detection (e.g., disable automation flags, custom user-agent) to avoid empty pages on Goodreads.\n",
        "\n",
        "# Install required packages (run this once per Colab session)\n",
        "!pip install beautifulsoup4 requests pandas selenium tqdm\n",
        "# Note: Removed webdriver_manager as it's not needed with system chromedriver\n",
        "\n",
        "# For Selenium in Colab: Install Chrome and ChromeDriver\n",
        "!apt-get update -qq  # Quiet update to avoid verbose output\n",
        "!apt install -y -qq chromium-chromedriver  # Quiet install\n",
        "\n",
        "# Ensure chromedriver is in /usr/bin (Colab often has it here already)\n",
        "import os\n",
        "chromedriver_path = '/usr/lib/chromium-browser/chromedriver'\n",
        "if os.path.exists(chromedriver_path) and not os.path.exists('/usr/bin/chromedriver'):\n",
        "    !cp {chromedriver_path} /usr/bin\n",
        "else:\n",
        "    print(\"Debug: chromedriver already exists in /usr/bin or source path. Skipping copy.\")\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import json  # For handling JSON-like data (e.g., reviews)\n",
        "import os  # For file operations\n",
        "import sys  # For system paths (debug)\n",
        "\n",
        "# Set up Selenium Chrome options for Colab (headless, no sandbox) with anti-detection\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Run without visible browser window\n",
        "chrome_options.add_argument('--no-sandbox')  # Required for Colab\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Avoid shared memory issues\n",
        "chrome_options.add_argument('--disable-gpu')  # Extra stability for headless mode\n",
        "chrome_options.add_argument('window-size=1920x1080')  # Set a reasonable window size\n",
        "chrome_options.binary_location = '/usr/bin/chromium-browser'  # Point to the installed Chromium\n",
        "\n",
        "# Anti-detection options (to avoid bot blocks and empty pages)\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # Hide Selenium flag\n",
        "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')  # Mimic real browser user-agent\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Exclude automation switches\n",
        "chrome_options.add_experimental_option('useAutomationExtension', False)  # Disable automation extension\n",
        "\n",
        "# Debug: Print system paths for troubleshooting\n",
        "print(f\"Debug: Python version: {sys.version}\")\n",
        "print(f\"Debug: Chromedriver path: /usr/bin/chromedriver\")\n",
        "print(f\"Debug: Chromium binary: {chrome_options.binary_location}\")\n",
        "\n",
        "# Initialize the WebDriver with try-except for error handling\n",
        "try:\n",
        "    driver = webdriver.Chrome(options=chrome_options)  # Use system chromedriver (no service/manager)\n",
        "    print(\"Debug: WebDriver initialized successfully.\")\n",
        "except WebDriverException as e:\n",
        "    print(f\"Error: Failed to initialize WebDriver: {e}\")\n",
        "    raise  # Re-raise to stop if critical\n",
        "\n",
        "# Debug print: Confirm WebDriver is set up by loading a test page\n",
        "try:\n",
        "    driver.get('https://www.goodreads.com/')  # Test with Goodreads home (as in your code)\n",
        "    print(f\"Debug: WebDriver test - Page title: {driver.title}\")  # Should print \"Goodreads | Meet your next favorite book\"\n",
        "    print(f\"Debug: Test page source sample: {driver.page_source[:500]}\")  # Print sample to verify content\n",
        "    print(\"Debug: All dependencies installed and imported successfully. Ready to proceed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Test page load failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGBUHJLwVr31",
        "outputId": "828e71c7-a1f7-43f5-cdd1-11ec0dd6dcd9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Debug: chromedriver already exists in /usr/bin or source path. Skipping copy.\n",
            "Debug: Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Debug: Chromedriver path: /usr/bin/chromedriver\n",
            "Debug: Chromium binary: /usr/bin/chromium-browser\n",
            "Debug: WebDriver initialized successfully.\n",
            "Debug: WebDriver test - Page title: Goodreads | Meet your next favorite book\n",
            "Debug: Test page source sample: <html class=\"desktop withSiteHeaderTopFullImage\n",
            " picture es5array es5date es5function es5object strictmode es5string json es5syntax es5undefined es5 no-touchevents cssanimations flexbox flexwrap csstransforms localstorage\"><head><script src=\"https://rules.quantcount.com/rules-p-0dUe_kJAjvkoY.js\" async=\"\"></script><script async=\"\" src=\"https://sb.scorecardresearch.com/beacon.js\"></script><script src=\"https://secure.quantserve.com/aquant.js?a=p-0dUe_kJAjvkoY\" async=\"\" type=\"text/javascript\"></scri\n",
            "Debug: All dependencies installed and imported successfully. Ready to proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Subgenres and Global Variables**"
      ],
      "metadata": {
        "id": "PnaSL-eAKNut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define subgenres, URLs, and global configurations\n",
        "# Explanation: Here we list the subgenres and their Goodreads URLs as provided.\n",
        "# We also set configurable variables for scraping limits, delays, etc.\n",
        "# This makes the code flexible—e.g., change MIN_BOOKS_PER_SUBGENRE for testing.\n",
        "\n",
        "# List of subgenres and their shelf/genre URLs\n",
        "subgenres = {\n",
        "    \"Contemporary Romance\": \"https://www.goodreads.com/shelf/show/contemporary-romance\",\n",
        "    \"Historical Romance\": \"https://www.goodreads.com/shelf/show/historical-romance\",\n",
        "    \"Paranormal Romance\": \"https://www.goodreads.com/shelf/show/paranormal-romance\",\n",
        "    \"Romantic Suspense\": \"https://www.goodreads.com/shelf/show/romantic-suspense\",\n",
        "    \"Romantic Fantasy\": \"https://www.goodreads.com/genres/fantasy-romance\",\n",
        "    \"Science Fiction Romance\": \"https://www.goodreads.com/genres/science-fiction-romance\"\n",
        "}\n",
        "\n",
        "# Configurable scraping limits\n",
        "MIN_BOOKS_PER_SUBGENRE = 200  # Minimum to collect (we'll stop if we reach this and can't get more)\n",
        "MAX_BOOKS_PER_SUBGENRE = 300  # Maximum to aim for (if available on pages)\n",
        "MAX_REVIEWS_PER_BOOK = 200  # Cap for reviews if >200; set to None for no cap\n",
        "ALL_REVIEWS = False  # Set to True to scrape ALL reviews regardless of count (warning: can be slow!)\n",
        "DELAY_MIN = 2  # Minimum delay between requests (seconds)\n",
        "DELAY_MAX = 5  # Maximum delay (for randomness to mimic human behavior)\n",
        "\n",
        "# User-agent for requests (to avoid blocks; rotate if needed)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Empty list to store all scraped book data (will convert to DataFrame later)\n",
        "all_books = []\n",
        "\n",
        "# Debug prints: Show configurations\n",
        "print(\"Debug: Subgenres defined:\")\n",
        "for genre, url in subgenres.items():\n",
        "    print(f\"  - {genre}: {url}\")\n",
        "print(f\"Debug: Config - Books per subgenre: {MIN_BOOKS_PER_SUBGENRE} to {MAX_BOOKS_PER_SUBGENRE}\")\n",
        "print(f\"Debug: Config - Max reviews per book: {MAX_REVIEWS_PER_BOOK} (all_reviews={ALL_REVIEWS})\")\n",
        "print(f\"Debug: Config - Delays between requests: {DELAY_MIN}-{DELAY_MAX} seconds\")\n",
        "print(\"Debug: Ready to scrape book lists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zJvaoWY4KSp3",
        "outputId": "3354bf46-085a-4e26-d562-2af35294d661"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Subgenres defined:\n",
            "  - Contemporary Romance: https://www.goodreads.com/shelf/show/contemporary-romance\n",
            "  - Historical Romance: https://www.goodreads.com/shelf/show/historical-romance\n",
            "  - Paranormal Romance: https://www.goodreads.com/shelf/show/paranormal-romance\n",
            "  - Romantic Suspense: https://www.goodreads.com/shelf/show/romantic-suspense\n",
            "  - Romantic Fantasy: https://www.goodreads.com/genres/fantasy-romance\n",
            "  - Science Fiction Romance: https://www.goodreads.com/genres/science-fiction-romance\n",
            "Debug: Config - Books per subgenre: 200 to 300\n",
            "Debug: Config - Max reviews per book: 200 (all_reviews=False)\n",
            "Debug: Config - Delays between requests: 2-5 seconds\n",
            "Debug: Ready to scrape book lists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Function to Scrape Book Lists from Subgenre Pages**"
      ],
      "metadata": {
        "id": "z0nhH2DnKjnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Function to scrape book lists from a subgenre URL (Using Selenium for pagination)\n",
        "# Explanation: This function uses Selenium to load the subgenre page, extract books, and click \"next\" for pagination.\n",
        "# It collects unique books by tracking book_ids in a set.\n",
        "# Selenium handles dynamic content and proper pagination better than requests (e.g., avoids repeats by simulating clicks).\n",
        "# Note: This is slower than BS4, so we use waits and delays. Run with care to avoid bans.\n",
        "# Assumes global 'driver' from Cell 1. We parse with BeautifulSoup after loading for easier tag finding.\n",
        "# We scroll to the next button and use execute_script for reliable clicks in headless mode.\n",
        "# Adapted from your provided code: Added handling for potential overlays (wait for invisibility), JS click fallback,\n",
        "# longer timeouts, anti-detection options (user-agent), and page source print on timeout for debugging.\n",
        "# New: Added a retry on timeout (up to 2 times), and an initial test load of Goodreads home for verification.\n",
        "\n",
        "def scrape_subgenre_books(genre, base_url):\n",
        "    \"\"\"\n",
        "    Scrape book details from a subgenre's paginated list using Selenium.\n",
        "    Args:\n",
        "        genre (str): Subgenre name (e.g., \"Contemporary Romance\")\n",
        "        base_url (str): The starting URL for the subgenre\n",
        "    Returns:\n",
        "        list: List of delegation dicts with book info (title, author, url, book_id, subgenre)\n",
        "    \"\"\"\n",
        "    collected_books = []  # List to hold books for this subgenre (each a dict with details)\n",
        "    seen_ids = set()  # Set to track unique book_ids and skip duplicates (prevents adding the same book multiple times)\n",
        "    current_url = base_url  # Start with the base URL; we'll update it as we paginate\n",
        "    page = 1  # Track page number for debugging and logging\n",
        "    max_retries = 2  # Number of retries on load timeout\n",
        "    print(f\"Debug: Starting Selenium scrape for {genre} at {base_url}\")  # Debug print to show start\n",
        "\n",
        "    try:\n",
        "        # Test load: Try loading Goodreads home first to verify driver (as in your code)\n",
        "        driver.get('https://www.goodreads.com/')  # Load home page\n",
        "        time.sleep(3)  # Short delay\n",
        "        home_title = driver.title  # Get title\n",
        "        print(f\"Debug: Test load - Goodreads home title: {home_title}\")  # Should be \"Goodreads | Meet your next favorite book\"\n",
        "        if \"Goodreads\" not in home_title:\n",
        "            print(\"Warning: Test load failed - Driver may not be loading content properly.\")  # Alert if test fails\n",
        "\n",
        "        # Now load the actual subgenre page\n",
        "        driver.get(current_url)\n",
        "        time.sleep(random.uniform(3, 6))  # Slightly longer initial delay for full load (adjusted from your code's time.sleep(2))\n",
        "        print(f\"Debug: Loaded/initial page: {driver.current_url}\")  # Confirm the page loaded\n",
        "        print(f' Debug: Initial page source sample: {driver.page_source[:500]}')  # Print sample to verify content (new debug)\n",
        "\n",
        "        # Main loop: Continue until we have enough books or no more pages\n",
        "        while len(collected_books) < MAX_BOOKS_PER_SUBGENRE:\n",
        "            retry_count = 0  # Reset retry counter for this page\n",
        "            page_loaded = False  # Flag for successful load\n",
        "            while retry_count < max_retries and not page_loaded:\n",
        "                # Wait for book items to appear on the page (Goodreads loads dynamically)\n",
        "                try:\n",
        "                    # Increased timeout to 30s for slower loads; wait for presence of elementList\n",
        "                    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'elementList')))  # Wait for at least one book container\n",
        "                    print(f\"Debug: Page {page} loaded successfully on attempt {retry_count + 1}.\")  # Success message\n",
        "                    page_loaded = True  # Flag as loaded\n",
        "                except TimeoutException:\n",
        "                    retry_count += 1  # Increment retry\n",
        "                    print(f\"Debug: Timeout on page {page}, attempt {retry_count}. Retrying...\")  # Log retry\n",
        "                    time.sleep(5)  # Wait before retry\n",
        "                    driver.refresh()  # Refresh the page on retry\n",
        "                    if retry_count >= max_retries:\n",
        "                        print(f\"Debug: Max retries exceeded for page {page}. Printing page source for diagnosis...\")  # Log failure\n",
        "                        print(driver.page_source[:1000])  # Print first 1000 chars of HTML to inspect\n",
        "                        print(\"Debug: Stopping due to persistent timeout.\")  # Stop the loop\n",
        "                        return collected_books  # Early return if failed\n",
        "\n",
        "            if not page_loaded:\n",
        "                break  # Exit if still not loaded after retries\n",
        "\n",
        "            # Check for and handle potential overlays (adapted from your extract_characters function)\n",
        "            try:\n",
        "                WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//span[@tabindex='-1']\")))  # Wait for any overlay to disappear\n",
        "                print(\"Debug: Overlay handled (invisible).\")  # Log success\n",
        "            except TimeoutException:\n",
        "                print(\"Debug: No overlay found or it didn't disappear; proceeding anyway.\")  # Not critical, continue\n",
        "\n",
        "            # Parse the current page source with BeautifulSoup (easier for complex HTML extraction than pure Selenium)\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')  # Get the full HTML after JS loads\n",
        "\n",
        "            # Find all book items on the page (adjust selector based on page type)\n",
        "            if \"shelf/show\" in base_url:\n",
        "                book_items = soup.find_all('div', class_='elementList')  # For shelf pages like Contemporary Romance\n",
        "            else:\n",
        "                book_items = soup.find_all('div', class_='grid-item')  # For genre pages (e.g., Romantic Fantasy); adjust if structure differs\n",
        "\n",
        "            # If no items found, stop the loop\n",
        "            if not book_items:\n",
        "                print(f\"Debug: No book items found on page {page}. Stopping.\")  # Debug if selector fails\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Found {len(book_items)} book items on page {page}\")  # Show how many potential books found\n",
        "\n",
        "            added_this_page = 0  # Counter for new books added this page (to detect if we're at the end)\n",
        "            for item in book_items:  # Loop through each potential book element\n",
        "                # Extract title from the item\n",
        "                title_tag = item.find('a', class_='bookTitle')  # Find the <a> tag with class 'bookTitle'\n",
        "                title = title_tag.text.strip() if title_tag else None  # Strip whitespace; None if not found\n",
        "\n",
        "                # Extract author\n",
        "                author_tag = item.find('a', class_='authorName')  # Find the author link\n",
        "                author = author_tag.text.strip() if author_tag else None  # Strip and handle missing\n",
        "\n",
        "                # Extract partial URL and book_id\n",
        "                if title_tag and title_tag['href']:  # Check if tag exists and has href\n",
        "                    url_partial = title_tag['href']  # Get the relative URL\n",
        "                    full_url = f\"https://www.goodreads.com{url_partial.split('?')[0]}\"  # Build full clean URL (remove query params)\n",
        "                    book_id_match = re.search(r'/show/(\\d+)', url_partial)  # Regex to extract numeric ID from URL\n",
        "                    book_id = book_id_match.group(1) if book_id_match else None  # Get the ID or None\n",
        "                else:\n",
        "                    full_url = None\n",
        "                    book_id = None\n",
        "\n",
        "                # Add the book if it's valid (has ID, title, author) and not already seen\n",
        "                if book_id and title and author and book_id not in seen_ids:\n",
        "                    seen_ids.add(book_id)  # Mark as seen\n",
        "                    collected_books.append({  # Add dict to list\n",
        "                        'book_id': book_id,\n",
        "                        'title': title,\n",
        "                        'author': author,\n",
        "                        'url': full_url,\n",
        "                        'subgenre': genre\n",
        "                    })\n",
        "                    print(f\"Debug: Added book - ID: {book_id}, Title: {title[:50]}..., Author: {author}, URL: {full_url}\")  # Truncate title for readability\n",
        "                    added_this_page += 1  # Increment counter\n",
        "                elif book_id in seen_ids:\n",
        "                    print(f\"Debug: Skipped duplicate book ID: {book_id}\")  # Log duplicates\n",
        "                else:\n",
        "                    print(\"Debug: Skipped invalid book (missing title/author/ID)\")  # Log invalid items\n",
        "\n",
        "                # Check if we've reached the max books limit\n",
        "                if len(collected_books) >= MAX_BOOKS_PER_SUBGENRE:\n",
        "                    print(f\"Debug: Reached max books ({MAX_BOOKS_PER_SUBGENRE}) for {genre}.\")\n",
        "                    break  # Exit the for loop\n",
        "\n",
        "            # If no new books were added this page, likely end of unique content\n",
        "            if added_this_page == 0:\n",
        "                print(f\"Debug: No new unique books on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # Find and click the \"next\" button to go to the next page (adapted with fallback from your code)\n",
        "            try:\n",
        "               ーミ next_button = WebDriverWait(driver, 10).until(  # Wait up to 10s for clickable button\n",
        "                    EC.element_to_be_clickable((By.XPATH, \"//a[@class='next_page' and not(contains(@class, 'disabled'))]\"))  # XPath for non.disabled next button\n",
        "                )\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)  # Scroll to ensure it's visible (helps in headless)\n",
        "                try:\n",
        "                    next_button.click()  # Try direct click\n",
        "                except:\n",
        "                    print(\"Debug: Direct click failed; trying JS click (possible overlay).\")  # Fallback if direct fails\n",
        "                    driver.execute_script(\"arguments[0].click();\", next_button)  # JS click as in your code\n",
        "                print(f\"Debug: Clicked 'next' button. Moving to page {page + 1}\")  # Confirm action\n",
        "                time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))  # Delay after click to let new page load\n",
        "                page += 1  # Increment page counter\n",
        "            except TimeoutException:\n",
        "                print(f\"Debug: No clickable 'next' button found on page {page}. End of list.\")  # If no button, stop\n",
        "                break\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error: Failed to click next: {e}. Stopping.\")  # Handle click errors\n",
        "                break\n",
        "\n",
        "            print(f\"Debug: Unique books collected so far for {genre}: {len(collected_books)}\")  # Progress update\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error: Selenium error for {genre}: {e}\")  # Catch general Selenium errors\n",
        "\n",
        "    # Check if we collected enough (warn if below minimum)\n",
        "    if len(collected_books) < MIN_BOOKS_PER_SUBGENRE:\n",
        "        print(f\"Warning: Only collected {len(collected_books)} unique books for {genre} (less than min {MIN_BOOKS_PER_SUBGENRE})\")\n",
        "\n",
        "    print(f\"Debug: Finished scraping {genre}. Total unique books: {len(collected_books)}\")  # Final message\n",
        "    return collected_books  # Return the list of books\n",
        "\n",
        "# Example usage (for testing): Scrape one subgenre and add to all_books\n",
        "# Reset all_books for fresh test; change test_genre if needed\n",
        "all_books = []  # Reset for testing\n",
        "test_genre = \"Contemporary Romance\"\n",
        "all_books.extend(scrape_subgenre_books(test_genre, subgenres[test_genre]))  # Call the function and add results\n",
        "print(f\"Debug: Total books after test scrape: {len(all_books)}\")  # Show total collected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "gQQVAkXaV8re",
        "outputId": "9e6c3313-070e-4c76-d417-f84ad969c3c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2536797224.py, line 140)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2536797224.py\"\u001b[0;36m, line \u001b[0;32m140\u001b[0m\n\u001b[0;31m    ーミ next_button = WebDriverWait(driver, 10).until(  # Wait up to 10s for clickable button\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}