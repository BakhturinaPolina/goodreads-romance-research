{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMMqrhqASJl6uVLu8Mnhtw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/Goodreads_Datasets_Romance_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Colab cell: deduplicate by work_id, keep full metadata (except exclusions), use max() for counts, and produce reports.\n",
        "# Run in Google Colab.\n",
        "\n",
        "# ---------- Setup & imports ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import gzip, json, os, logging, datetime, re\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# ---------- Config ----------\n",
        "BOOKS_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Subset options for fast testing:\n",
        "RUN_SUBSET = True      # set False to run on full corpus\n",
        "SUBSET_WORK_COUNT = 10 # if RUN_SUBSET True, process only first N distinct work_ids encountered\n",
        "\n",
        "# Columns to exclude from final CSV (explicitly requested)\n",
        "EXCLUDE_COLUMNS = {\n",
        "    \"isbn\", \"isbn13\", \"asin\", \"kindle_asin\",\n",
        "    \"url\", \"link\", \"image_url\",\n",
        "    \"edition_information\", \"format\", \"country_code\", \"publisher\"\n",
        "}\n",
        "\n",
        "# Output filenames (timestamped)\n",
        "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "LOGFILE = os.path.join(OUTPUT_DIR, f\"dedup_run_{ts}.log\")\n",
        "OUT_CSV = os.path.join(OUTPUT_DIR, f\"goodreads_romance_dedup_by_work_{ts}.csv\")\n",
        "OUT_SUMMARY_JSON = os.path.join(OUTPUT_DIR, f\"dedup_summary_{ts}.json\")\n",
        "OUT_AMBIG_TITLES = os.path.join(OUTPUT_DIR, f\"ambiguous_same_title_diff_authors_{ts}.csv\")\n",
        "OUT_QUALITY = os.path.join(OUTPUT_DIR, f\"quality_report_{ts}.txt\")\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logger = logging.getLogger(\"goodreads_dedup_full\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# Stream handler (console)\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))\n",
        "logger.handlers = [ch]\n",
        "# File handler\n",
        "fh = logging.FileHandler(LOGFILE, encoding='utf-8')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\n",
        "logger.addHandler(fh)\n",
        "\n",
        "logger.info(\"Starting deduplication pipeline\")\n",
        "logger.info(f\"BOOKS_PATH = {BOOKS_PATH}\")\n",
        "logger.info(f\"OUTPUT_DIR = {OUTPUT_DIR}\")\n",
        "logger.info(f\"RUN_SUBSET = {RUN_SUBSET}, SUBSET_WORK_COUNT = {SUBSET_WORK_COUNT}\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def iter_json_gz_lines(path):\n",
        "    \"\"\"Yield JSON objects from newline-delimited JSON .gz file (defensive).\"\"\"\n",
        "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                yield json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                # best-effort: skip malformed line but log occasionally\n",
        "                logger.debug(\"Skipping a malformed JSON line.\")\n",
        "                continue\n",
        "\n",
        "def to_int_safe(x):\n",
        "    if x is None or x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return int(str(x).replace(\",\", \"\"))\n",
        "    except:\n",
        "        try:\n",
        "            return int(float(x))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def normalize_title(t):\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    s = str(t).strip().lower()\n",
        "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def normalize_author_id_list(authors_field):\n",
        "    \"\"\"Return list of author identifiers (author_id if present else name str).\"\"\"\n",
        "    out = []\n",
        "    if not authors_field:\n",
        "        return out\n",
        "    if isinstance(authors_field, list):\n",
        "        for a in authors_field:\n",
        "            if isinstance(a, dict):\n",
        "                aid = a.get(\"author_id\") or a.get(\"id\")\n",
        "                if aid:\n",
        "                    out.append(str(aid))\n",
        "                else:\n",
        "                    name = a.get(\"name\") or a.get(\"author_name\")\n",
        "                    if name:\n",
        "                        out.append(str(name).strip().lower())\n",
        "            else:\n",
        "                out.append(str(a).strip().lower())\n",
        "    elif isinstance(authors_field, dict):\n",
        "        aid = authors_field.get(\"author_id\") or authors_field.get(\"id\")\n",
        "        if aid:\n",
        "            out.append(str(aid))\n",
        "        else:\n",
        "            name = authors_field.get(\"name\") or authors_field.get(\"author_name\")\n",
        "            if name:\n",
        "                out.append(str(name).strip().lower())\n",
        "    else:\n",
        "        out.append(str(authors_field).strip().lower())\n",
        "    return out\n",
        "\n",
        "# ---------- Phase 1: read file and build list of rows (raw) ----------\n",
        "logger.info(\"Phase 1: streaming JSON and collecting raw records (preserve all keys). This may take some time for the full dump.\")\n",
        "\n",
        "raw_rows = []\n",
        "work_id_order = []   # track first-seen work_id ordering\n",
        "work_seen = set()\n",
        "lines_read = 0\n",
        "\n",
        "for obj in tqdm(iter_json_gz_lines(BOOKS_PATH), desc=\"reading books\"):\n",
        "    lines_read += 1\n",
        "    # keep raw object; convert certain str-number fields to numeric for easier aggregation\n",
        "    # but we keep raw values too\n",
        "    row = dict(obj)  # shallow copy of all keys\n",
        "    # normalize some fields\n",
        "    row['_ratings_count_int'] = to_int_safe(row.get('ratings_count'))\n",
        "    row['_text_reviews_count_int'] = to_int_safe(row.get('text_reviews_count'))\n",
        "    # publication year might be string -> int or empty\n",
        "    row['_publication_year_int'] = to_int_safe(row.get('publication_year'))\n",
        "    # ensure book_id and work_id exist as strings\n",
        "    row['book_id'] = str(row.get('book_id')) if row.get('book_id') is not None else \"\"\n",
        "    row['work_id'] = str(row.get('work_id')) if row.get('work_id') is not None else \"\"\n",
        "    raw_rows.append(row)\n",
        "    if row['work_id'] not in work_seen:\n",
        "        work_seen.add(row['work_id'])\n",
        "        work_id_order.append(row['work_id'])\n",
        "    # Fast test cutoff if RUN_SUBSET True: we still need all editions for chosen works,\n",
        "    # so we won't break early here. We'll filter later by the first N work_ids for subset.\n",
        "# end stream\n",
        "logger.info(f\"Finished streaming. Lines read: {lines_read}. Raw records collected: {len(raw_rows)}\")\n",
        "\n",
        "# Convert to DataFrame (keeps all keys as columns; some rows may have different keys)\n",
        "logger.info(\"Converting to pandas DataFrame (this may use memory).\")\n",
        "df_raw = pd.DataFrame(raw_rows)\n",
        "logger.info(f\"DataFrame shape: {df_raw.shape}, columns: {len(df_raw.columns)}\")\n",
        "\n",
        "# ---------- Phase 1b: optionally restrict to subset of works to test quickly ----------\n",
        "if RUN_SUBSET:\n",
        "    selected_work_ids = work_id_order[:SUBSET_WORK_COUNT]\n",
        "    logger.info(f\"RUN_SUBSET is True: restricting to first {len(selected_work_ids)} work_ids for quick test.\")\n",
        "    df = df_raw[df_raw['work_id'].isin(selected_work_ids)].copy()\n",
        "else:\n",
        "    df = df_raw\n",
        "\n",
        "logger.info(f\"Working DataFrame shape after subset filter: {df.shape}\")\n",
        "\n",
        "# ---------- Phase 2: group by work_id, compute aggregates and pick canonical edition ----------\n",
        "logger.info(\"Phase 2: grouping by work_id, computing aggregates, and selecting canonical edition (edition with max ratings_count).\")\n",
        "\n",
        "grouped = df.groupby('work_id', sort=False)\n",
        "\n",
        "dedup_rows = []\n",
        "work_stats = {}  # for summary\n",
        "\n",
        "for wid, group in tqdm(grouped, desc=\"processing work groups\"):\n",
        "    if wid == \"\" or wid is None:\n",
        "        # optionally skip invalid work_id\n",
        "        continue\n",
        "    # compute number of editions\n",
        "    num_editions = len(group)\n",
        "    # inferred publication year = earliest non-null publication year across editions\n",
        "    pub_years = group['_publication_year_int'].dropna().astype(int).tolist()\n",
        "    publication_year_inferred = int(min(pub_years)) if len(pub_years) > 0 else None\n",
        "    # ratings/text_reviews chosen = MAX across editions (our fix)\n",
        "    ratings_count_chosen = None\n",
        "    text_reviews_count_chosen = None\n",
        "    if not group['_ratings_count_int'].dropna().empty:\n",
        "        ratings_count_chosen = int(group['_ratings_count_int'].dropna().max())\n",
        "    if not group['_text_reviews_count_int'].dropna().empty:\n",
        "        text_reviews_count_chosen = int(group['_text_reviews_count_int'].dropna().max())\n",
        "    # choose canonical edition row: edition with highest ratings_count; tie-break: highest text_reviews_count\n",
        "    # if no ratings_count available, pick the edition with the earliest publication_year; else fallback to first row\n",
        "    canonical_row = None\n",
        "    try:\n",
        "        if not group['_ratings_count_int'].dropna().empty:\n",
        "            idx = group['_ratings_count_int'].idxmax()\n",
        "            # if multiple have same max, idxmax returns first; good enough\n",
        "            canonical_row = group.loc[idx].to_dict()\n",
        "        elif publication_year_inferred is not None:\n",
        "            # pick earliest year edition (closest to inferred)\n",
        "            cand = group[group['_publication_year_int'] == publication_year_inferred]\n",
        "            if len(cand) > 0:\n",
        "                canonical_row = cand.iloc[0].to_dict()\n",
        "            else:\n",
        "                canonical_row = group.iloc[0].to_dict()\n",
        "        else:\n",
        "            canonical_row = group.iloc[0].to_dict()\n",
        "    except Exception as e:\n",
        "        logger.debug(f\"Error selecting canonical edition for work {wid}: {e}\")\n",
        "        canonical_row = group.iloc[0].to_dict()\n",
        "    # Ensure canonical_row contains keys (if some editions had missing keys)\n",
        "    if canonical_row is None:\n",
        "        continue\n",
        "\n",
        "    # Build final dedup row:\n",
        "    # - start from canonical_row but remove excluded columns\n",
        "    # - add aggregated columns: num_editions_for_work, publication_year_inferred, publication_year_chosen,\n",
        "    #   ratings_count_chosen, text_reviews_count_chosen\n",
        "    final_row = {}\n",
        "    # Copy canonical metadata keys, except excluded ones\n",
        "    for k, v in canonical_row.items():\n",
        "        if k in EXCLUDE_COLUMNS:\n",
        "            continue\n",
        "        # we do not want intermediate _fields in final metadata\n",
        "        if k.startswith('_'):\n",
        "            continue\n",
        "        final_row[k] = v\n",
        "    # Add aggregated columns\n",
        "    final_row['work_id'] = wid\n",
        "    final_row['canonical_book_id'] = canonical_row.get('book_id')\n",
        "    final_row['num_editions_for_work'] = num_editions\n",
        "    final_row['publication_year_inferred'] = publication_year_inferred\n",
        "    # publication_year_chosen: prefer canonical edition's publication_year if available else inferred\n",
        "    pub_year_chosen = to_int_safe(canonical_row.get('publication_year')) if canonical_row.get('publication_year') not in (None, \"\") else publication_year_inferred\n",
        "    final_row['publication_year_chosen'] = pub_year_chosen\n",
        "    final_row['ratings_count_chosen'] = ratings_count_chosen\n",
        "    final_row['text_reviews_count_chosen'] = text_reviews_count_chosen\n",
        "    # authors flatten: prefer canonical authors list but also include combined authors across editions\n",
        "    canonical_authors = canonical_row.get('authors') if canonical_row.get('authors') is not None else []\n",
        "    # compute union of author ids/names across editions\n",
        "    author_union = set()\n",
        "    for a in group['authors'].dropna().tolist():\n",
        "        if isinstance(a, list):\n",
        "            for ai in a:\n",
        "                if isinstance(ai, dict):\n",
        "                    aid = ai.get('author_id') or ai.get('id') or ai.get('name')\n",
        "                    if aid: author_union.add(str(aid))\n",
        "                else:\n",
        "                    author_union.add(str(ai))\n",
        "        elif isinstance(a, dict):\n",
        "            aid = a.get('author_id') or a.get('id') or a.get('name')\n",
        "            if aid: author_union.add(str(aid))\n",
        "        else:\n",
        "            author_union.add(str(a))\n",
        "    # canonical authors normalized list\n",
        "    canonical_authors_ids = normalize_author_id_list(canonical_authors)\n",
        "    final_row['authors_ids_or_names'] = \"|\".join(sorted(author_union)) if author_union else \"|\".join(canonical_authors_ids)\n",
        "    # store normalized title for collision detection\n",
        "    final_row['_normalized_title'] = normalize_title(final_row.get('title') or final_row.get('title_without_series') or \"\")\n",
        "    dedup_rows.append(final_row)\n",
        "    # save stats\n",
        "    work_stats[wid] = {\n",
        "        \"num_editions\": num_editions,\n",
        "        \"pub_year_inferred\": publication_year_inferred,\n",
        "        \"ratings_count_chosen\": ratings_count_chosen,\n",
        "        \"text_reviews_count_chosen\": text_reviews_count_chosen\n",
        "    }\n",
        "\n",
        "logger.info(f\"Completed grouping & canonical selection. Deduplicated works: {len(dedup_rows)}\")\n",
        "\n",
        "# ---------- Phase 3: create final DataFrame and flag ambiguous same-title different-author cases ----------\n",
        "logger.info(\"Phase 3: building final DataFrame and identifying title collisions.\")\n",
        "\n",
        "dedup_df = pd.DataFrame(dedup_rows)\n",
        "\n",
        "# Detect ambiguous titles: same normalized title maps to >1 distinct author identifiers\n",
        "title_groups = dedup_df.groupby('_normalized_title')['authors_ids_or_names'].apply(lambda s: set(s.dropna().tolist()))\n",
        "ambiguous_titles = {}\n",
        "for title_norm, authors_set in title_groups.items():\n",
        "    # authors_set is a set of strings like \"123|456\" -> need to split and count unique tokens\n",
        "    unique_authors = set()\n",
        "    for s in authors_set:\n",
        "        if not s:\n",
        "            continue\n",
        "        parts = s.split(\"|\")\n",
        "        for p in parts:\n",
        "            if p: unique_authors.add(p)\n",
        "    if len(unique_authors) > 1:\n",
        "        ambiguous_titles[title_norm] = {\n",
        "            \"n_distinct_authors\": len(unique_authors),\n",
        "            \"authors_sample\": list(sorted(unique_authors))[:10],\n",
        "            \"work_examples\": dedup_df[dedup_df['_normalized_title'] == title_norm][['work_id','canonical_book_id','authors_ids_or_names','title']].to_dict(orient='records')\n",
        "        }\n",
        "\n",
        "# add flag column possible_title_collision\n",
        "dedup_df['possible_title_collision'] = dedup_df['_normalized_title'].apply(lambda t: True if t in ambiguous_titles else False)\n",
        "\n",
        "# drop helper normalized column before saving, but keep if you want (we keep it)\n",
        "# dedup_df.drop(columns=['_normalized_title'], inplace=True)\n",
        "\n",
        "# ---------- Phase 4: Save outputs ----------\n",
        "logger.info(\"Phase 4: saving CSV, ambiguous titles CSV, and summary files.\")\n",
        "\n",
        "# Save main dedup CSV (preserve column order: work_id, canonical_book_id, title..., then aggregated columns)\n",
        "# Ensure aggregated columns exist\n",
        "agg_cols = ['work_id', 'canonical_book_id', 'title', 'title_without_series', 'authors_ids_or_names',\n",
        "            'publication_year_inferred', 'publication_year_chosen', 'ratings_count_chosen',\n",
        "            'text_reviews_count_chosen', 'num_editions_for_work', 'possible_title_collision']\n",
        "# include all other columns in dedup_df except excluded ones and helper fields\n",
        "other_cols = [c for c in dedup_df.columns if c not in agg_cols and c != '_normalized_title']\n",
        "# prioritize archives: put agg_cols first, then other_cols\n",
        "final_columns = [c for c in agg_cols if c in dedup_df.columns] + other_cols\n",
        "\n",
        "dedup_df.to_csv(OUT_CSV, index=False, columns=final_columns, encoding='utf-8')\n",
        "logger.info(f\"Saved deduplicated CSV to: {OUT_CSV}\")\n",
        "\n",
        "# Save ambiguous titles details\n",
        "if ambiguous_titles:\n",
        "    ambig_rows = []\n",
        "    for t, info in ambiguous_titles.items():\n",
        "        ambig_rows.append({\n",
        "            \"normalized_title\": t,\n",
        "            \"n_distinct_authors\": info[\"n_distinct_authors\"],\n",
        "            \"authors_sample\": \"|\".join(info[\"authors_sample\"]),\n",
        "            \"work_examples_json\": json.dumps(info[\"work_examples\"], ensure_ascii=False)\n",
        "        })\n",
        "    pd.DataFrame(ambig_rows).to_csv(OUT_AMBIG_TITLES, index=False, encoding='utf-8')\n",
        "    logger.info(f\"Saved ambiguous titles CSV to: {OUT_AMBIG_TITLES}\")\n",
        "else:\n",
        "    logger.info(\"No ambiguous titles found.\")\n",
        "\n",
        "# Save summary JSON\n",
        "summary = {\n",
        "    \"timestamp\": ts,\n",
        "    \"input\": BOOKS_PATH,\n",
        "    \"output_csv\": OUT_CSV,\n",
        "    \"n_raw_records\": len(raw_rows),\n",
        "    \"n_works_deduplicated\": len(dedup_df),\n",
        "    \"run_subset\": RUN_SUBSET,\n",
        "    \"subset_work_count\": SUBSET_WORK_COUNT if RUN_SUBSET else None,\n",
        "    \"excluded_columns\": sorted(list(EXCLUDE_COLUMNS))\n",
        "}\n",
        "with open(OUT_SUMMARY_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(summary, fh, indent=2)\n",
        "logger.info(f\"Saved summary JSON to: {OUT_SUMMARY_JSON}\")\n",
        "\n",
        "# ---------- Phase 5: Quality report ----------\n",
        "logger.info(\"Phase 5: computing quality report metrics.\")\n",
        "\n",
        "def safe_list_get(series, key=None):\n",
        "    return series.dropna()\n",
        "\n",
        "quality_lines = []\n",
        "quality_lines.append(f\"Quality report generated at {datetime.datetime.now().isoformat()}\\n\")\n",
        "quality_lines.append(f\"Input file: {BOOKS_PATH}\")\n",
        "quality_lines.append(f\"Output file: {OUT_CSV}\")\n",
        "quality_lines.append(f\"Number of raw records read: {len(raw_rows)}\")\n",
        "quality_lines.append(f\"Number of deduplicated works: {len(dedup_df)}\")\n",
        "# ratings distribution\n",
        "ratings_vals = dedup_df['ratings_count_chosen'].dropna().astype(int) if 'ratings_count_chosen' in dedup_df.columns else pd.Series(dtype=float)\n",
        "if not ratings_vals.empty:\n",
        "    quality_lines.append(f\"Ratings_count_chosen: min={int(ratings_vals.min())}, median={int(ratings_vals.median())}, mean={float(ratings_vals.mean()):.2f}, max={int(ratings_vals.max())}\")\n",
        "else:\n",
        "    quality_lines.append(\"Ratings_count_chosen: No data\")\n",
        "# text_reviews distribution\n",
        "reviews_vals = dedup_df['text_reviews_count_chosen'].dropna().astype(int) if 'text_reviews_count_chosen' in dedup_df.columns else pd.Series(dtype=float)\n",
        "if not reviews_vals.empty:\n",
        "    quality_lines.append(f\"Text_reviews_count_chosen: min={int(reviews_vals.min())}, median={int(reviews_vals.median())}, mean={float(reviews_vals.mean()):.2f}, max={int(reviews_vals.max())}\")\n",
        "else:\n",
        "    quality_lines.append(\"Text_reviews_count_chosen: No data\")\n",
        "# inferred year coverage\n",
        "inferred_years = dedup_df['publication_year_inferred'].dropna().astype(int) if 'publication_year_inferred' in dedup_df.columns else pd.Series(dtype=float)\n",
        "if not inferred_years.empty:\n",
        "    quality_lines.append(f\"Publication year inferred: min={int(inferred_years.min())}, median={int(inferred_years.median())}, max={int(inferred_years.max())}\")\n",
        "    missing_years_pct = 100.0 * (len(dedup_df) - len(inferred_years)) / len(dedup_df)\n",
        "    quality_lines.append(f\"Percentage of works missing inferred publication year: {missing_years_pct:.2f}%\")\n",
        "else:\n",
        "    quality_lines.append(\"No inferred publication years available\")\n",
        "# edition counts\n",
        "if 'num_editions_for_work' in dedup_df.columns:\n",
        "    nd = dedup_df['num_editions_for_work'].dropna().astype(int)\n",
        "    quality_lines.append(f\"Editions per work: min={int(nd.min())}, median={int(nd.median())}, mean={float(nd.mean()):.2f}, max={int(nd.max())}\")\n",
        "# ambiguous titles count\n",
        "quality_lines.append(f\"Number of ambiguous normalized titles (same title, multiple authors): {len(ambiguous_titles)}\")\n",
        "\n",
        "# Save quality report\n",
        "with open(OUT_QUALITY, \"w\", encoding=\"utf-8\") as fh:\n",
        "    fh.write(\"\\n\".join(quality_lines))\n",
        "logger.info(f\"Saved quality report to: {OUT_QUALITY}\")\n",
        "\n",
        "# Print a short console summary\n",
        "logger.info(\"=== RUN SUMMARY ===\")\n",
        "for line in quality_lines[:10]:\n",
        "    logger.info(line)\n",
        "logger.info(\"=== END SUMMARY ===\")\n",
        "\n",
        "logger.info(\"Pipeline finished successfully. Please inspect the CSV and reports in the output directory. Remember to commit the script and outputs (small samples) to your git repo before heavy reruns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e33xWaM3OVwW",
        "outputId": "670cdde9-c2a0-48f9-8b91-8bb73deca1ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] Starting deduplication pipeline\n",
            "INFO:goodreads_dedup_full:Starting deduplication pipeline\n",
            "[INFO] BOOKS_PATH = /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "INFO:goodreads_dedup_full:BOOKS_PATH = /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "[INFO] OUTPUT_DIR = /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full\n",
            "INFO:goodreads_dedup_full:OUTPUT_DIR = /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full\n",
            "[INFO] RUN_SUBSET = True, SUBSET_WORK_COUNT = 10\n",
            "INFO:goodreads_dedup_full:RUN_SUBSET = True, SUBSET_WORK_COUNT = 10\n",
            "[INFO] Phase 1: streaming JSON and collecting raw records (preserve all keys). This may take some time for the full dump.\n",
            "INFO:goodreads_dedup_full:Phase 1: streaming JSON and collecting raw records (preserve all keys). This may take some time for the full dump.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading books: 335449it [00:47, 7132.97it/s]\n",
            "[INFO] Finished streaming. Lines read: 335449. Raw records collected: 335449\n",
            "INFO:goodreads_dedup_full:Finished streaming. Lines read: 335449. Raw records collected: 335449\n",
            "[INFO] Converting to pandas DataFrame (this may use memory).\n",
            "INFO:goodreads_dedup_full:Converting to pandas DataFrame (this may use memory).\n",
            "[INFO] DataFrame shape: (335449, 32), columns: 32\n",
            "INFO:goodreads_dedup_full:DataFrame shape: (335449, 32), columns: 32\n",
            "[INFO] RUN_SUBSET is True: restricting to first 10 work_ids for quick test.\n",
            "INFO:goodreads_dedup_full:RUN_SUBSET is True: restricting to first 10 work_ids for quick test.\n",
            "[INFO] Working DataFrame shape after subset filter: (265, 32)\n",
            "INFO:goodreads_dedup_full:Working DataFrame shape after subset filter: (265, 32)\n",
            "[INFO] Phase 2: grouping by work_id, computing aggregates, and selecting canonical edition (edition with max ratings_count).\n",
            "INFO:goodreads_dedup_full:Phase 2: grouping by work_id, computing aggregates, and selecting canonical edition (edition with max ratings_count).\n",
            "processing work groups: 100%|██████████| 10/10 [00:00<00:00, 1258.08it/s]\n",
            "[INFO] Completed grouping & canonical selection. Deduplicated works: 10\n",
            "INFO:goodreads_dedup_full:Completed grouping & canonical selection. Deduplicated works: 10\n",
            "[INFO] Phase 3: building final DataFrame and identifying title collisions.\n",
            "INFO:goodreads_dedup_full:Phase 3: building final DataFrame and identifying title collisions.\n",
            "[INFO] Phase 4: saving CSV, ambiguous titles CSV, and summary files.\n",
            "INFO:goodreads_dedup_full:Phase 4: saving CSV, ambiguous titles CSV, and summary files.\n",
            "[INFO] Saved deduplicated CSV to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/goodreads_romance_dedup_by_work_20250809_002714.csv\n",
            "INFO:goodreads_dedup_full:Saved deduplicated CSV to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/goodreads_romance_dedup_by_work_20250809_002714.csv\n",
            "[INFO] Saved ambiguous titles CSV to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/ambiguous_same_title_diff_authors_20250809_002714.csv\n",
            "INFO:goodreads_dedup_full:Saved ambiguous titles CSV to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/ambiguous_same_title_diff_authors_20250809_002714.csv\n",
            "[INFO] Saved summary JSON to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/dedup_summary_20250809_002714.json\n",
            "INFO:goodreads_dedup_full:Saved summary JSON to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/dedup_summary_20250809_002714.json\n",
            "[INFO] Phase 5: computing quality report metrics.\n",
            "INFO:goodreads_dedup_full:Phase 5: computing quality report metrics.\n",
            "[INFO] Saved quality report to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/quality_report_20250809_002714.txt\n",
            "INFO:goodreads_dedup_full:Saved quality report to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/quality_report_20250809_002714.txt\n",
            "[INFO] === RUN SUMMARY ===\n",
            "INFO:goodreads_dedup_full:=== RUN SUMMARY ===\n",
            "[INFO] Quality report generated at 2025-08-09T00:28:03.504861\n",
            "\n",
            "INFO:goodreads_dedup_full:Quality report generated at 2025-08-09T00:28:03.504861\n",
            "\n",
            "[INFO] Input file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "INFO:goodreads_dedup_full:Input file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "[INFO] Output file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/goodreads_romance_dedup_by_work_20250809_002714.csv\n",
            "INFO:goodreads_dedup_full:Output file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs_full/goodreads_romance_dedup_by_work_20250809_002714.csv\n",
            "[INFO] Number of raw records read: 335449\n",
            "INFO:goodreads_dedup_full:Number of raw records read: 335449\n",
            "[INFO] Number of deduplicated works: 10\n",
            "INFO:goodreads_dedup_full:Number of deduplicated works: 10\n",
            "[INFO] Ratings_count_chosen: min=5, median=378, mean=50190.10, max=470489\n",
            "INFO:goodreads_dedup_full:Ratings_count_chosen: min=5, median=378, mean=50190.10, max=470489\n",
            "[INFO] Text_reviews_count_chosen: min=4, median=54, mean=1244.60, max=8855\n",
            "INFO:goodreads_dedup_full:Text_reviews_count_chosen: min=4, median=54, mean=1244.60, max=8855\n",
            "[INFO] Publication year inferred: min=1964, median=2014, max=2017\n",
            "INFO:goodreads_dedup_full:Publication year inferred: min=1964, median=2014, max=2017\n",
            "[INFO] Percentage of works missing inferred publication year: 20.00%\n",
            "INFO:goodreads_dedup_full:Percentage of works missing inferred publication year: 20.00%\n",
            "[INFO] Editions per work: min=1, median=2, mean=26.50, max=221\n",
            "INFO:goodreads_dedup_full:Editions per work: min=1, median=2, mean=26.50, max=221\n",
            "[INFO] === END SUMMARY ===\n",
            "INFO:goodreads_dedup_full:=== END SUMMARY ===\n",
            "[INFO] Pipeline finished successfully. Please inspect the CSV and reports in the output directory. Remember to commit the script and outputs (small samples) to your git repo before heavy reruns.\n",
            "INFO:goodreads_dedup_full:Pipeline finished successfully. Please inspect the CSV and reports in the output directory. Remember to commit the script and outputs (small samples) to your git repo before heavy reruns.\n",
            "[INFO] If you want, I can now prepare a ready-to-commit script file and README content for your GitHub repo.\n",
            "INFO:goodreads_dedup_full:If you want, I can now prepare a ready-to-commit script file and README content for your GitHub repo.\n"
          ]
        }
      ]
    }
  ]
}