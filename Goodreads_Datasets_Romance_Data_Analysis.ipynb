{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNzX4uPBGjuffXKGnGb9GAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakhturinaPolina/goodreads-romance-research/blob/main/Goodreads_Datasets_Romance_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Imports and Mount Drive"
      ],
      "metadata": {
        "id": "VQAUD75LZmg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready script: copy into cells in order.\n",
        "# Cell 1: imports and mount drive\n",
        "import json, gzip, os, re, random, math\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Iterable, Optional\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "# Mount Google Drive (run this cell in Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths (adjust if necessary)\n",
        "goodreads_book_romance = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\"\n",
        "goodreads_interactions_romance = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_interactions_romance.json.gz\"\n",
        "goodreads_reviews_romance = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_reviews_romance.json.gz\"\n",
        "\n",
        "# Output CSV path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL0yTjiidGGM",
        "outputId": "7c0ba468-96c8-4864-ce70-6006169728f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Utility readers for json / jsonlines gz"
      ],
      "metadata": {
        "id": "MvLllKqHdQie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Utility readers for json / jsonlines gz (defensive)\n",
        "def iter_json_gz(path: str, max_items: Optional[int]=None):\n",
        "    \"\"\"\n",
        "    Yields JSON objects from a gzip file which may be either:\n",
        "      - newline-delimited JSON (jsonlines), OR\n",
        "      - a single JSON array (loads whole file once; only for small files)\n",
        "    This handles both forms defensively.\n",
        "    \"\"\"\n",
        "    if path.endswith(\".gz\"):\n",
        "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
        "            # Peek first non-whitespace char\n",
        "            pos = f.tell()\n",
        "            first = f.read(1)\n",
        "            while first and first.isspace():\n",
        "                first = f.read(1)\n",
        "            f.seek(pos)\n",
        "            if first == '[':\n",
        "                # single JSON array\n",
        "                raw = f.read()\n",
        "                arr = json.loads(raw)\n",
        "                for i, obj in enumerate(arr):\n",
        "                    yield obj\n",
        "                    if max_items and i+1 >= max_items: break\n",
        "            else:\n",
        "                # assume one JSON object per line\n",
        "                for i, line in enumerate(f):\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        obj = json.loads(line)\n",
        "                    except json.JSONDecodeError:\n",
        "                        # attempt to fix trailing commas or other problems (best-effort)\n",
        "                        try:\n",
        "                            # sometimes files contain Python-style reprs or similar — skip\n",
        "                            continue\n",
        "                        except:\n",
        "                            continue\n",
        "                    yield obj\n",
        "                    if max_items and i+1 >= max_items:\n",
        "                        break\n",
        "    else:\n",
        "        # non-gz fallback\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            raw = f.read()\n",
        "            if raw.strip().startswith('['):\n",
        "                arr = json.loads(raw)\n",
        "                for obj in arr:\n",
        "                    yield obj\n",
        "            else:\n",
        "                for line in f:\n",
        "                    if not line.strip(): continue\n",
        "                    yield json.loads(line)"
      ],
      "metadata": {
        "id": "f6TDfo8YdVvI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Quick file inspection (first N examples + aggregated key stats)"
      ],
      "metadata": {
        "id": "OvqheViadZOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "\n",
        "# === CONFIG ===\n",
        "JSON_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\"\n",
        "SAMPLE_SIZE = 1000  # number of objects to sample for initial inspection\n",
        "SAMPLE_PRINT_COUNT = 4  # number of full records to print\n",
        "\n",
        "print(f\"[INFO] Inspecting file: {JSON_PATH}\")\n",
        "\n",
        "# === LOAD SAMPLE ===\n",
        "sample_objects = []\n",
        "key_counter = Counter()\n",
        "\n",
        "with gzip.open(JSON_PATH, 'rt', encoding='utf-8') as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            sample_objects.append(obj)\n",
        "            key_counter.update(obj.keys())\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"[ERROR] JSON parsing error at line {idx}: {e}\")\n",
        "        if len(sample_objects) >= SAMPLE_SIZE:\n",
        "            break\n",
        "\n",
        "print(f\"[INFO] Number of sample objects collected: {len(sample_objects)}\")\n",
        "\n",
        "# === LOG: Top keys by frequency ===\n",
        "print(f\"\\n[INFO] Top keys (by frequency in first {SAMPLE_SIZE} objects):\")\n",
        "for key, count in key_counter.most_common():\n",
        "    print(f\"  {key:25} : {count}\")\n",
        "\n",
        "# === LOG: Missing value percentage per column ===\n",
        "df_sample = pd.DataFrame(sample_objects)\n",
        "missing_pct = df_sample.isna().mean().sort_values(ascending=False) * 100\n",
        "print(\"\\n[INFO] Missing value percentage (sample):\")\n",
        "print(missing_pct.to_string())\n",
        "\n",
        "# === LOG: Columns to drop (KEEPING book_id and work_id for joins) ===\n",
        "columns_to_drop = [\n",
        "    \"isbn\", \"isbn13\", \"asin\", \"kindle_asin\",\n",
        "    \"url\", \"link\", \"image_url\",\n",
        "    \"edition_information\", \"country_code\", \"publisher\"\n",
        "]\n",
        "print(\"\\n[INFO] Candidate columns for drop (book_id/work_id KEPT):\")\n",
        "print(columns_to_drop)\n",
        "\n",
        "# === LOG: Metadata fields that may hint at subgenres ===\n",
        "subgenre_hint_fields = [\"popular_shelves\", \"series\", \"description\", \"publisher\", \"format\", \"language_code\"]\n",
        "print(\"\\n[INFO] Metadata fields with potential subgenre clues:\")\n",
        "print(subgenre_hint_fields)\n",
        "\n",
        "# === LOG: Pretty print sample records ===\n",
        "print(f\"\\n[DEBUG] Pretty print of first {SAMPLE_PRINT_COUNT} sample objects:\")\n",
        "for i in range(min(SAMPLE_PRINT_COUNT, len(sample_objects))):\n",
        "    print(f\"\\n--- SAMPLE BOOK {i+1} ---\")\n",
        "    pprint(sample_objects[i], indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgzvxQ7Ki40o",
        "outputId": "9211b212-cba4-481c-b04f-d36c05b4ca42",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Inspecting file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "[INFO] Number of sample objects collected: 1000\n",
            "\n",
            "[INFO] Top keys (by frequency in first 1000 objects):\n",
            "  isbn                      : 1000\n",
            "  text_reviews_count        : 1000\n",
            "  series                    : 1000\n",
            "  country_code              : 1000\n",
            "  language_code             : 1000\n",
            "  popular_shelves           : 1000\n",
            "  asin                      : 1000\n",
            "  is_ebook                  : 1000\n",
            "  average_rating            : 1000\n",
            "  kindle_asin               : 1000\n",
            "  similar_books             : 1000\n",
            "  description               : 1000\n",
            "  format                    : 1000\n",
            "  link                      : 1000\n",
            "  authors                   : 1000\n",
            "  publisher                 : 1000\n",
            "  num_pages                 : 1000\n",
            "  publication_day           : 1000\n",
            "  isbn13                    : 1000\n",
            "  publication_month         : 1000\n",
            "  edition_information       : 1000\n",
            "  publication_year          : 1000\n",
            "  url                       : 1000\n",
            "  image_url                 : 1000\n",
            "  book_id                   : 1000\n",
            "  ratings_count             : 1000\n",
            "  work_id                   : 1000\n",
            "  title                     : 1000\n",
            "  title_without_series      : 1000\n",
            "\n",
            "[INFO] Missing value percentage (sample):\n",
            "isbn                    0.0\n",
            "text_reviews_count      0.0\n",
            "series                  0.0\n",
            "country_code            0.0\n",
            "language_code           0.0\n",
            "popular_shelves         0.0\n",
            "asin                    0.0\n",
            "is_ebook                0.0\n",
            "average_rating          0.0\n",
            "kindle_asin             0.0\n",
            "similar_books           0.0\n",
            "description             0.0\n",
            "format                  0.0\n",
            "link                    0.0\n",
            "authors                 0.0\n",
            "publisher               0.0\n",
            "num_pages               0.0\n",
            "publication_day         0.0\n",
            "isbn13                  0.0\n",
            "publication_month       0.0\n",
            "edition_information     0.0\n",
            "publication_year        0.0\n",
            "url                     0.0\n",
            "image_url               0.0\n",
            "book_id                 0.0\n",
            "ratings_count           0.0\n",
            "work_id                 0.0\n",
            "title                   0.0\n",
            "title_without_series    0.0\n",
            "\n",
            "[INFO] Candidate columns for drop (book_id/work_id KEPT):\n",
            "['isbn', 'isbn13', 'asin', 'kindle_asin', 'url', 'link', 'image_url', 'edition_information', 'country_code', 'publisher']\n",
            "\n",
            "[INFO] Metadata fields with potential subgenre clues:\n",
            "['popular_shelves', 'series', 'description', 'publisher', 'format', 'language_code']\n",
            "\n",
            "[DEBUG] Pretty print of first 4 sample objects:\n",
            "\n",
            "--- SAMPLE BOOK 1 ---\n",
            "{ 'asin': '',\n",
            "  'authors': [{'author_id': '5807700', 'role': ''}],\n",
            "  'average_rating': '3.86',\n",
            "  'book_id': '34883016',\n",
            "  'country_code': 'US',\n",
            "  'description': 'Secrets. Sometimes keeping them in confidence is a good '\n",
            "                 \"thing. Other times secrets can slowly allow a woman's soul \"\n",
            "                 'to rot.\\n'\n",
            "                 'Whitney Beaupre has been hiding a big secret for years, one '\n",
            "                 \"that's beginning to wear her down both on and off the ice. \"\n",
            "                 \"Pretending to be something she's not is exhausting. Wanting \"\n",
            "                 'to be free but afraid to break out of her prison is '\n",
            "                 'terrifying. Seeking love but then hiding from it is crushing '\n",
            "                 'to the spirit, yet Whitney feels compelled to keep living '\n",
            "                 'the lie. Until the night Hannah Kym appears in her life. '\n",
            "                 \"Whitney's attraction to Hannah is deep, fierce, and \"\n",
            "                 'instantaneous. The Temple art major is everything Whitney '\n",
            "                 'has dreamed of and more. But those old fears keep clawing at '\n",
            "                 'the Venom center, keeping her locked in the closet despite '\n",
            "                 'the passion and affection she feels for Hannah.\\n'\n",
            "                 \"Can love finally break the shackles holding Whitney's heart \"\n",
            "                 'and soul captive?',\n",
            "  'edition_information': '',\n",
            "  'format': 'ebook',\n",
            "  'image_url': 'https://images.gr-assets.com/books/1493525974m/34883016.jpg',\n",
            "  'is_ebook': 'true',\n",
            "  'isbn': '',\n",
            "  'isbn13': '9781370889471',\n",
            "  'kindle_asin': '',\n",
            "  'language_code': '',\n",
            "  'link': 'https://www.goodreads.com/book/show/34883016-playmaker',\n",
            "  'num_pages': '',\n",
            "  'popular_shelves': [ {'count': '4', 'name': 'to-read'},\n",
            "                       {'count': '1', 'name': 'ibooks'},\n",
            "                       {'count': '1', 'name': 'favorite-authors'},\n",
            "                       {'count': '1', 'name': 'may-2017-dr-reads'},\n",
            "                       {'count': '1', 'name': 'sports-romance'},\n",
            "                       {'count': '1', 'name': 'series-i-like'},\n",
            "                       {'count': '1', 'name': 'romance'},\n",
            "                       {'count': '1', 'name': 'favorites'},\n",
            "                       {'count': '1', 'name': 'f-f'},\n",
            "                       {'count': '1', 'name': 'hockey-romance'},\n",
            "                       {'count': '1', 'name': 'ebook'}],\n",
            "  'publication_day': '3',\n",
            "  'publication_month': '5',\n",
            "  'publication_year': '2017',\n",
            "  'publisher': 'Gone Writing Publishing',\n",
            "  'ratings_count': '5',\n",
            "  'series': [],\n",
            "  'similar_books': [],\n",
            "  'text_reviews_count': '4',\n",
            "  'title': 'Playmaker: A Venom Series Novella',\n",
            "  'title_without_series': 'Playmaker: A Venom Series Novella',\n",
            "  'url': 'https://www.goodreads.com/book/show/34883016-playmaker',\n",
            "  'work_id': '56135087'}\n",
            "\n",
            "--- SAMPLE BOOK 2 ---\n",
            "{ 'asin': 'B01BLJGA9S',\n",
            "  'authors': [{'author_id': '5360266', 'role': ''}],\n",
            "  'average_rating': '4.23',\n",
            "  'book_id': '29074693',\n",
            "  'country_code': 'US',\n",
            "  'description': '',\n",
            "  'edition_information': '',\n",
            "  'format': '',\n",
            "  'image_url': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png',\n",
            "  'is_ebook': 'true',\n",
            "  'isbn': '',\n",
            "  'isbn13': '',\n",
            "  'kindle_asin': 'B01BLJGA9S',\n",
            "  'language_code': 'en-US',\n",
            "  'link': 'https://www.goodreads.com/book/show/29074693-prowled-darkness',\n",
            "  'num_pages': '',\n",
            "  'popular_shelves': [ {'count': '598', 'name': 'to-read'},\n",
            "                       {'count': '28', 'name': 'currently-reading'},\n",
            "                       {'count': '14', 'name': 'paranormal'},\n",
            "                       {'count': '10', 'name': 'shifters'},\n",
            "                       {'count': '10', 'name': 'paranormal-romance'},\n",
            "                       {'count': '5', 'name': 'romance'},\n",
            "                       {'count': '5', 'name': 'arc'},\n",
            "                       {'count': '4', 'name': 'carrie-ann-ryan'},\n",
            "                       {'count': '4', 'name': 'shapeshifters'},\n",
            "                       {'count': '3', 'name': 'series'},\n",
            "                       {'count': '3', 'name': 'fantasy'},\n",
            "                       {'count': '3', 'name': 'kindle'},\n",
            "                       {'count': '2', 'name': 'to-read-paranormal'},\n",
            "                       {'count': '2', 'name': 'favorites'},\n",
            "                       {'count': '2', 'name': 'phoenix'},\n",
            "                       {'count': '2', 'name': 'books-i-have'},\n",
            "                       {'count': '2', 'name': 'nook'},\n",
            "                       {'count': '2', 'name': '2016-books-read'},\n",
            "                       {'count': '2', 'name': 'contemporary'},\n",
            "                       {'count': '2', 'name': 'ebook'},\n",
            "                       {'count': '2', 'name': 'dragons'},\n",
            "                       {'count': '2', 'name': 'ryan-carrie-ann'},\n",
            "                       {'count': '2', 'name': 'dantes-circle'},\n",
            "                       {'count': '2', 'name': 'supernatural'},\n",
            "                       {'count': '1', 'name': 'my-owned-books'},\n",
            "                       {'count': '1', 'name': 'shapeshifters-all-species'},\n",
            "                       { 'count': '1',\n",
            "                         'name': 'interspecies-forbidden-fated-mate'},\n",
            "                       {'count': '1', 'name': 'fated-mates-all-kinds'},\n",
            "                       {'count': '1', 'name': 'ttp-proj'},\n",
            "                       { 'count': '1',\n",
            "                         'name': 'tbr-my-books-hanging-out-on-kindle'},\n",
            "                       {'count': '1', 'name': 're-reading-worthy'},\n",
            "                       {'count': '1', 'name': 'pnrm-pnranthologies'},\n",
            "                       {'count': '1', 'name': 'fantasy-romance'},\n",
            "                       {'count': '1', 'name': 'sub-genre-pnr'},\n",
            "                       {'count': '1', 'name': 'no-thanks'},\n",
            "                       {'count': '1', 'name': 'have-not-read'},\n",
            "                       {'count': '1', 'name': 'f-shifters'},\n",
            "                       {'count': '1', 'name': '0-my-library-of-books'},\n",
            "                       {'count': '1', 'name': '0-books-w-audio-companion'},\n",
            "                       {'count': '1', 'name': 'tbr'},\n",
            "                       {'count': '1', 'name': 'itunes'},\n",
            "                       {'count': '1', 'name': 'p-q-r-s'},\n",
            "                       {'count': '1', 'name': 'pick-fantasy'},\n",
            "                       {'count': '1', 'name': 'para-world'},\n",
            "                       {'count': '1', 'name': 'flightsfantasy'},\n",
            "                       {'count': '1', 'name': '1234'},\n",
            "                       {'count': '1', 'name': '100000'},\n",
            "                       {'count': '1', 'name': '100'},\n",
            "                       {'count': '1', 'name': 'multi-paranormal-species'},\n",
            "                       {'count': '1', 'name': 'i-own-a-copy'},\n",
            "                       {'count': '1', 'name': 'dl'},\n",
            "                       {'count': '1', 'name': 'audio'},\n",
            "                       {'count': '1', 'name': 'fave-fantasy'},\n",
            "                       {'count': '1', 'name': 'kindle-shelfari'},\n",
            "                       {'count': '1', 'name': 'thriller'},\n",
            "                       {'count': '1', 'name': 'suspense'},\n",
            "                       {'count': '1', 'name': 'netgalley'},\n",
            "                       {'count': '1', 'name': 'adult'},\n",
            "                       {'count': '1', 'name': 'on-kindle'},\n",
            "                       {'count': '1', 'name': 'book-challenge-2017'},\n",
            "                       {'count': '1', 'name': 'werewolves'},\n",
            "                       {'count': '1', 'name': 'uninteresting'},\n",
            "                       {'count': '1', 'name': 'strong-heroines'},\n",
            "                       {'count': '1', 'name': 'singel-parent'},\n",
            "                       {'count': '1', 'name': 'gentle-hero'},\n",
            "                       {'count': '1', 'name': 'owned'},\n",
            "                       {'count': '1', 'name': 'pub-may'},\n",
            "                       {'count': '1', 'name': 'secrect-baby'},\n",
            "                       {'count': '1', 'name': 'second-chance'},\n",
            "                       {'count': '1', 'name': 'books-from-carrie-ann'},\n",
            "                       {'count': '1', 'name': '3-paranormal-series'},\n",
            "                       {'count': '1', 'name': 'comptr-nook'},\n",
            "                       {'count': '1', 'name': 'a-books-want-to-read'},\n",
            "                       {'count': '1', 'name': 'x-pub-2016'},\n",
            "                       {'count': '1', 'name': 'books-to-buy'},\n",
            "                       {'count': '1', 'name': 'para-fantasy-romance'},\n",
            "                       {'count': '1', 'name': 'kindle-ebooks-i-have-read'},\n",
            "                       {'count': '1', 'name': 'title-p'},\n",
            "                       {'count': '1', 'name': 'pages-100-150'},\n",
            "                       {'count': '1', 'name': 'read-mee'},\n",
            "                       {'count': '1', 'name': 'on-my-shelf'},\n",
            "                       {'count': '1', 'name': 'audio-book'},\n",
            "                       { 'count': '1',\n",
            "                         'name': 'complete-unfinished-series-owned'},\n",
            "                       {'count': '1', 'name': 'giveaway-entry'},\n",
            "                       {'count': '1', 'name': 'received-audio-book'},\n",
            "                       {'count': '1', 'name': 'received'},\n",
            "                       {'count': '1', 'name': '01-books'},\n",
            "                       {'count': '1', 'name': 'wolves'},\n",
            "                       {'count': '1', 'name': 'wizards'},\n",
            "                       {'count': '1', 'name': 'succubus'},\n",
            "                       {'count': '1', 'name': 'siren'},\n",
            "                       {'count': '1', 'name': 'pixie'},\n",
            "                       {'count': '1', 'name': 'n-ok'},\n",
            "                       {'count': '1', 'name': 'n-gregory-salinas'},\n",
            "                       {'count': '1', 'name': 'mermen'},\n",
            "                       {'count': '1', 'name': 'lion'},\n",
            "                       {'count': '1', 'name': 'feline'},\n",
            "                       {'count': '1', 'name': 'fae'},\n",
            "                       {'count': '1', 'name': 'djinn'},\n",
            "                       {'count': '1', 'name': 'demons'}],\n",
            "  'publication_day': '',\n",
            "  'publication_month': '',\n",
            "  'publication_year': '',\n",
            "  'publisher': '',\n",
            "  'ratings_count': '149',\n",
            "  'series': ['811663'],\n",
            "  'similar_books': [ '25515353',\n",
            "                     '20483269',\n",
            "                     '25650829',\n",
            "                     '18913492',\n",
            "                     '22578299',\n",
            "                     '25781561',\n",
            "                     '29518816',\n",
            "                     '20958132',\n",
            "                     '21854392',\n",
            "                     '27505924',\n",
            "                     '25205453'],\n",
            "  'text_reviews_count': '21',\n",
            "  'title': \"Prowled Darkness (Dante's Circle, #7)\",\n",
            "  'title_without_series': \"Prowled Darkness (Dante's Circle, #7)\",\n",
            "  'url': 'https://www.goodreads.com/book/show/29074693-prowled-darkness',\n",
            "  'work_id': '46079519'}\n",
            "\n",
            "--- SAMPLE BOOK 3 ---\n",
            "{ 'asin': '',\n",
            "  'authors': [{'author_id': '1265', 'role': ''}],\n",
            "  'average_rating': '3.99',\n",
            "  'book_id': '3209316',\n",
            "  'country_code': 'US',\n",
            "  'description': 'The funny and heartwarming story of a young lady whose zeal, '\n",
            "                 'snobbishness, and self-satisfaction lead to several errors '\n",
            "                 'in judgment\\n'\n",
            "                 'Emma takes Harriet Smith, a young woman previously unknown '\n",
            "                 'to good society, under her wing, scheming for her '\n",
            "                 'advancement through an advantageous marriage. Her efforts to '\n",
            "                 \"find Harriet a suitor occupy all of Emma's time. However, in \"\n",
            "                 'the midst of her often fumbled attempts, she settles on a '\n",
            "                 'most unlikely union with her own constant critic: Mr. '\n",
            "                 'Knightly.\\n'\n",
            "                 \"This novel is part of Brilliance Audio's extensive Classic \"\n",
            "                 'Collection, bringing you timeless masterpieces that you and '\n",
            "                 'your family are sure to love.',\n",
            "  'edition_information': '',\n",
            "  'format': 'Audio CD',\n",
            "  'image_url': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png',\n",
            "  'is_ebook': 'false',\n",
            "  'isbn': '1597371289',\n",
            "  'isbn13': '9781597371285',\n",
            "  'kindle_asin': 'B0083Z3O8Y',\n",
            "  'language_code': 'eng',\n",
            "  'link': 'https://www.goodreads.com/book/show/3209316-emma',\n",
            "  'num_pages': '544',\n",
            "  'popular_shelves': [ {'count': '16215', 'name': 'classics'},\n",
            "                       {'count': '7070', 'name': 'to-read'},\n",
            "                       {'count': '4564', 'name': 'fiction'},\n",
            "                       {'count': '4279', 'name': 'favorites'},\n",
            "                       {'count': '3125', 'name': 'romance'},\n",
            "                       {'count': '2750', 'name': 'classic'},\n",
            "                       {'count': '1942', 'name': 'books-i-own'},\n",
            "                       {'count': '1503', 'name': 'owned'},\n",
            "                       {'count': '1277', 'name': 'jane-austen'},\n",
            "                       {'count': '996', 'name': 'historical-fiction'},\n",
            "                       {'count': '993', 'name': 'clàssics'},\n",
            "                       {'count': '904', 'name': 'literature'},\n",
            "                       {'count': '665', 'name': 'historical'},\n",
            "                       {'count': '575', 'name': 'kindle'},\n",
            "                       {'count': '560', 'name': 'owned-books'},\n",
            "                       {'count': '530', 'name': 'favourites'},\n",
            "                       {'count': '497', 'name': '19th-century'},\n",
            "                       {'count': '478', 'name': 'classic-literature'},\n",
            "                       {'count': '445', 'name': 'british'},\n",
            "                       {'count': '419', 'name': 'novels'},\n",
            "                       {'count': '398', 'name': '1001-books'},\n",
            "                       {'count': '369', 'name': 'austen'},\n",
            "                       {'count': '350', 'name': 'currently-reading'},\n",
            "                       {'count': '340', 'name': 'british-literature'},\n",
            "                       {'count': '329', 'name': 'historical-romance'},\n",
            "                       {'count': '309', 'name': 'classics-to-read'},\n",
            "                       {'count': '305', 'name': 'adult'},\n",
            "                       {'count': '282', 'name': 'book-club'},\n",
            "                       {'count': '282', 'name': 'regency'},\n",
            "                       {'count': '279', 'name': 'england'},\n",
            "                       {'count': '269', 'name': 'to-buy'},\n",
            "                       {'count': '265', 'name': 'english'},\n",
            "                       {'count': '250', 'name': 'adult-fiction'},\n",
            "                       {'count': '248', 'name': 'my-books'},\n",
            "                       {'count': '244', 'name': 'my-library'},\n",
            "                       {'count': '243', 'name': 'audiobook'},\n",
            "                       {'count': '236', 'name': 'library'},\n",
            "                       { 'count': '235',\n",
            "                         'name': 'rory-gilmore-reading-challenge'},\n",
            "                       {'count': '235', 'name': 'chick-lit'},\n",
            "                       {'count': '230', 'name': 'i-own'},\n",
            "                       {'count': '219', 'name': 'default'},\n",
            "                       {'count': '216', 'name': 'audiobooks'},\n",
            "                       {'count': '215', 'name': 'my-ebooks'},\n",
            "                       {'count': '207', 'name': 'classic-fiction'},\n",
            "                       {'count': '206', 'name': '1001'},\n",
            "                       {'count': '202', 'name': 'novel'},\n",
            "                       {'count': '200', 'name': 'ebooks'},\n",
            "                       {'count': '193', 'name': 're-read'},\n",
            "                       {'count': '173', 'name': 'classic-lit'},\n",
            "                       {'count': '171', 'name': 'english-literature'},\n",
            "                       {'count': '170', 'name': 'school'},\n",
            "                       {'count': '168', 'name': 'to-read-classics'},\n",
            "                       {'count': '163', 'name': 'literary-fiction'},\n",
            "                       {'count': '161', 'name': 'own-it'},\n",
            "                       {'count': '152', 'name': 'audio'},\n",
            "                       {'count': '150', 'name': 'brit-lit'},\n",
            "                       { 'count': '142',\n",
            "                         'name': '1001-books-to-read-before-you-die'},\n",
            "                       {'count': '140', 'name': 'all-time-favorites'},\n",
            "                       {'count': '139', 'name': 'the-classics'},\n",
            "                       {'count': '134', 'name': 'tbr'},\n",
            "                       {'count': '133', 'name': 'unfinished'},\n",
            "                       {'count': '128', 'name': 'british-lit'},\n",
            "                       {'count': '127', 'name': 'rory-gilmore-challenge'},\n",
            "                       {'count': '116', 'name': 'general-fiction'},\n",
            "                       {'count': '116', 'name': 'on-my-shelf'},\n",
            "                       {'count': '107', 'name': 'women'},\n",
            "                       {'count': '107', 'name': 'e-book'},\n",
            "                       {'count': '105', 'name': 'did-not-finish'},\n",
            "                       {'count': '103', 'name': 'abandoned'},\n",
            "                       {'count': '98', 'name': 'humor'},\n",
            "                       {'count': '98', 'name': 'e-books'},\n",
            "                       {'count': '97', 'name': 'rory-gilmore'},\n",
            "                       {'count': '96', 'name': 'female-authors'},\n",
            "                       {'count': '95', 'name': 'kindle-books'},\n",
            "                       {'count': '94', 'name': 'romantic'},\n",
            "                       {'count': '94', 'name': 'reread'},\n",
            "                       {'count': '93', 'name': 'read-in-2015'},\n",
            "                       {'count': '93', 'name': 'female-author'},\n",
            "                       {'count': '92', 'name': 'classici'},\n",
            "                       {'count': '91', 'name': 'favorite-books'},\n",
            "                       {'count': '90', 'name': 'own-to-read'},\n",
            "                       {'count': '90', 'name': 'stand-alone'},\n",
            "                       {'count': '89', 'name': 'on-hold'},\n",
            "                       {'count': '89', 'name': 'my-bookshelf'},\n",
            "                       {'count': '89', 'name': 'literary'},\n",
            "                       {'count': '88', 'name': 'home-library'},\n",
            "                       {'count': '88', 'name': 'love'},\n",
            "                       {'count': '85', 'name': 'to-re-read'},\n",
            "                       {'count': '84', 'name': 'tbr-pile'},\n",
            "                       {'count': '84', 'name': 'audio-book'},\n",
            "                       {'count': '84', 'name': 'drama'},\n",
            "                       {'count': '83', 'name': 'history'},\n",
            "                       {'count': '83', 'name': 'victorian'},\n",
            "                       {'count': '82', 'name': 'read-in-2016'},\n",
            "                       {'count': '82', 'name': 'wish-list'},\n",
            "                       {'count': '82', 'name': 'audio-books'},\n",
            "                       {'count': '81', 'name': 'bookshelf'},\n",
            "                       {'count': '80', 'name': 'read-in-2017'},\n",
            "                       {'count': '80', 'name': 'dnf'},\n",
            "                       {'count': '79', 'name': 'personal-library'}],\n",
            "  'publication_day': '25',\n",
            "  'publication_month': '9',\n",
            "  'publication_year': '2005',\n",
            "  'publisher': 'Brilliance Audio',\n",
            "  'ratings_count': '42',\n",
            "  'series': [],\n",
            "  'similar_books': [ '31242',\n",
            "                     '374380',\n",
            "                     '20564',\n",
            "                     '383206',\n",
            "                     '7891',\n",
            "                     '6335178',\n",
            "                     '31175',\n",
            "                     '372811',\n",
            "                     '77395',\n",
            "                     '856190',\n",
            "                     '686278',\n",
            "                     '5797',\n",
            "                     '32110',\n",
            "                     '3102',\n",
            "                     '264',\n",
            "                     '99329',\n",
            "                     '31667'],\n",
            "  'text_reviews_count': '8',\n",
            "  'title': 'Emma',\n",
            "  'title_without_series': 'Emma',\n",
            "  'url': 'https://www.goodreads.com/book/show/3209316-emma',\n",
            "  'work_id': '3360164'}\n",
            "\n",
            "--- SAMPLE BOOK 4 ---\n",
            "{ 'asin': 'B01HX6PENG',\n",
            "  'authors': [ {'author_id': '90411', 'role': ''},\n",
            "               {'author_id': '14356708', 'role': ''}],\n",
            "  'average_rating': '4.31',\n",
            "  'book_id': '30838933',\n",
            "  'country_code': 'US',\n",
            "  'description': 'In the Finding Fatherhood series, these shifters become '\n",
            "                 'daddies in unconventional ways.\\n'\n",
            "                 'Cougar-shifter Jackson Sperry rescues Hannah from the ocean, '\n",
            "                 'and she has no memory of how she ended up floating in the '\n",
            "                 \"Pacific. She also doesn't remember who she is or how she \"\n",
            "                 \"came to be pregnant. He's drawn to her and determined to \"\n",
            "                 'protect her as they unravel the mystery of her past. It '\n",
            "                 \"isn't long before he's ready to claim her as his mate and \"\n",
            "                 'raise her son as his own--but without knowing the details of '\n",
            "                 'her past, the future remains uncertain, and he fears someone '\n",
            "                 'is still hunting Hannah and her baby.',\n",
            "  'edition_information': '',\n",
            "  'format': '',\n",
            "  'image_url': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png',\n",
            "  'is_ebook': 'true',\n",
            "  'isbn': '',\n",
            "  'isbn13': '',\n",
            "  'kindle_asin': 'B01HX6PENG',\n",
            "  'language_code': 'en-GB',\n",
            "  'link': 'https://www.goodreads.com/book/show/30838933-guardian-cougar',\n",
            "  'num_pages': '',\n",
            "  'popular_shelves': [ {'count': '25', 'name': 'to-read'},\n",
            "                       {'count': '20', 'name': 'currently-reading'},\n",
            "                       {'count': '3', 'name': 'kindle-unlimited'},\n",
            "                       {'count': '2', 'name': 'paranormal'},\n",
            "                       {'count': '2', 'name': 'shifters'},\n",
            "                       {'count': '2', 'name': 'paranormal-romance'},\n",
            "                       {'count': '1', 'name': 'shifter'},\n",
            "                       {'count': '1', 'name': '2017-books'},\n",
            "                       {'count': '1', 'name': 'law-officer'},\n",
            "                       {'count': '1', 'name': '0-paranormal'},\n",
            "                       {'count': '1', 'name': 'tbr-incomplete'},\n",
            "                       {'count': '1', 'name': '2016-reads'},\n",
            "                       {'count': '1', 'name': 'strong-sexual-content'},\n",
            "                       {'count': '1', 'name': 'shifter-romance'},\n",
            "                       {'count': '1', 'name': 'shifter-haven-2016'},\n",
            "                       {'count': '1', 'name': 'shifter-haven'},\n",
            "                       {'count': '1', 'name': 'shapeshifters'},\n",
            "                       {'count': '1', 'name': 'series'},\n",
            "                       {'count': '1', 'name': 'mf-shifters'},\n",
            "                       {'count': '1', 'name': 'comptr-nook'},\n",
            "                       {'count': '1', 'name': 'babies-or-kids'},\n",
            "                       {'count': '1', 'name': 'read-and-reviewed'},\n",
            "                       {'count': '1', 'name': 'romance'},\n",
            "                       {'count': '1', 'name': 'funny'},\n",
            "                       {'count': '1', 'name': 'kindle'},\n",
            "                       {'count': '1', 'name': 'owned'},\n",
            "                       {'count': '1', 'name': '1'},\n",
            "                       {'count': '1', 'name': 'ebooks-kobo'},\n",
            "                       {'count': '1', 'name': 'e-books'}],\n",
            "  'publication_day': '',\n",
            "  'publication_month': '',\n",
            "  'publication_year': '',\n",
            "  'publisher': '',\n",
            "  'ratings_count': '139',\n",
            "  'series': ['938303'],\n",
            "  'similar_books': [],\n",
            "  'text_reviews_count': '27',\n",
            "  'title': 'Guardian Cougar (Finding Fatherhood, #2)',\n",
            "  'title_without_series': 'Guardian Cougar (Finding Fatherhood, #2)',\n",
            "  'url': 'https://www.goodreads.com/book/show/30838933-guardian-cougar',\n",
            "  'work_id': '51437308'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: investigate edition vs work counts & cross-check reviews/interactions"
      ],
      "metadata": {
        "id": "dKEfThN3kmhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell: investigate edition vs work counts & cross-check reviews/interactions ===\n",
        "import gzip, json, os, csv\n",
        "from collections import defaultdict, Counter\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Paths (adjust if needed)\n",
        "BOOKS_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\"\n",
        "REVIEWS_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_reviews_romance.json.gz\"  # may be big\n",
        "INTERACTIONS_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_interactions_romance.json.gz\"  # optional\n",
        "\n",
        "# Config: book to inspect (defaults from your sample)\n",
        "TARGET_BOOK_ID = \"3209316\"  # string type to match dump format\n",
        "TARGET_TITLE = \"Emma\"\n",
        "SAMPLE_EDITIONS_TO_PRINT = 25  # how many editions sharing the same work_id to print\n",
        "\n",
        "# Helpers\n",
        "def to_int_safe(x):\n",
        "    if x is None or x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return int(str(x).replace(\",\", \"\"))\n",
        "    except:\n",
        "        try:\n",
        "            return int(float(x))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def pretty_print_book(obj):\n",
        "    keys_of_interest = [\"book_id\", \"title\", \"title_without_series\", \"work_id\", \"authors\",\n",
        "                        \"publication_year\", \"ratings_count\", \"text_reviews_count\", \"num_pages\",\n",
        "                        \"format\", \"is_ebook\", \"language_code\", \"series\", \"popular_shelves\"]\n",
        "    for k in keys_of_interest:\n",
        "        print(f\"{k:20}: {obj.get(k, '')}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# Step 1: find target edition record and collect work_id\n",
        "print(\"[INFO] Scanning books file for target book and related editions (this reads the file once).\")\n",
        "target_record = None\n",
        "work_id_of_target = None\n",
        "editions_for_work = []  # list of raw objects for same work\n",
        "all_books_index = {}  # book_id -> selected fields for quick lookup\n",
        "\n",
        "with gzip.open(BOOKS_PATH, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f, desc=\"scanning books\"):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "        bid = str(obj.get(\"book_id\", \"\"))\n",
        "        wid = str(obj.get(\"work_id\", \"\") if obj.get(\"work_id\", \"\") is not None else \"\")\n",
        "        # index minimal info\n",
        "        all_books_index[bid] = {\n",
        "            \"book_id\": bid,\n",
        "            \"title\": obj.get(\"title\"),\n",
        "            \"work_id\": wid,\n",
        "            \"ratings_count\": to_int_safe(obj.get(\"ratings_count\")),\n",
        "            \"text_reviews_count\": to_int_safe(obj.get(\"text_reviews_count\")),\n",
        "            \"publication_year\": obj.get(\"publication_year\"),\n",
        "            \"authors\": obj.get(\"authors\"),\n",
        "            \"popular_shelves\": obj.get(\"popular_shelves\")\n",
        "        }\n",
        "        if bid == str(TARGET_BOOK_ID):\n",
        "            target_record = obj\n",
        "            work_id_of_target = wid\n",
        "\n",
        "# Logging results\n",
        "if target_record is None:\n",
        "    print(f\"[WARN] Target book_id {TARGET_BOOK_ID} not found in books dump.\")\n",
        "else:\n",
        "    print(f\"[INFO] Found target book_id {TARGET_BOOK_ID}. Title: {target_record.get('title')!r}, work_id: {work_id_of_target}\")\n",
        "    print(\"\\n[INFO] Target edition fields (concise):\")\n",
        "    pretty_print_book(target_record)\n",
        "\n",
        "# Step 2: collect all other editions in dump sharing the same work_id\n",
        "if work_id_of_target:\n",
        "    print(f\"\\n[INFO] Collecting all editions that share work_id = {work_id_of_target}\")\n",
        "    editions_for_work = [v for k, v in all_books_index.items() if v.get(\"work_id\") == work_id_of_target]\n",
        "    print(f\"[INFO] Number of editions in this dump that share the work_id: {len(editions_for_work)}\")\n",
        "    # sort by ratings_count descending\n",
        "    editions_for_work_sorted = sorted(editions_for_work, key=lambda x: x.get(\"ratings_count\") or 0, reverse=True)\n",
        "    print(f\"\\n[INFO] Top {min(SAMPLE_EDITIONS_TO_PRINT, len(editions_for_work_sorted))} editions for this work (dump values):\")\n",
        "    for e in editions_for_work_sorted[:SAMPLE_EDITIONS_TO_PRINT]:\n",
        "        print(f\"  book_id={e['book_id']}, title={str(e.get('title'))[:70]:70} | ratings={e.get('ratings_count')} | text_reviews={e.get('text_reviews_count')}\")\n",
        "\n",
        "    # aggregate sums across editions found in dump\n",
        "    total_ratings_in_dump = sum((e.get(\"ratings_count\") or 0) for e in editions_for_work_sorted)\n",
        "    total_text_reviews_in_dump = sum((e.get(\"text_reviews_count\") or 0) for e in editions_for_work_sorted)\n",
        "    print(f\"\\n[INFO] Sum across editions present in dump -> ratings_count: {total_ratings_in_dump}, text_reviews_count: {total_text_reviews_in_dump}\")\n",
        "\n",
        "# Step 3: Cross-check detailed reviews file for counts (streams file; can be slow)\n",
        "def stream_count_reviews(reviews_path, book_ids_set=None, work_id=None, max_lines=None):\n",
        "    \"\"\"\n",
        "    Count detailed review records that reference book_id or work_id in the reviews dump.\n",
        "    Returns (per_book_counter, total_counter_matched_lines).\n",
        "    \"\"\"\n",
        "    per_book = Counter()\n",
        "    matched_lines = 0\n",
        "    if not os.path.exists(reviews_path):\n",
        "        print(f\"[WARN] Reviews path {reviews_path} not found on disk.\")\n",
        "        return per_book, 0\n",
        "    with gzip.open(reviews_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"streaming reviews\")):\n",
        "            if max_lines and i >= max_lines:\n",
        "                break\n",
        "            try:\n",
        "                r = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            # defensive: reviews may use 'book_id' or 'bookId'\n",
        "            r_book_id = str(r.get(\"book_id\") or r.get(\"bookId\") or \"\")\n",
        "            r_work_id = str(r.get(\"work_id\") or r.get(\"workId\") or \"\")\n",
        "            if book_ids_set and r_book_id in book_ids_set:\n",
        "                per_book[r_book_id] += 1\n",
        "                matched_lines += 1\n",
        "            elif work_id and r_work_id == work_id:\n",
        "                # if review references work_id, increment an aggregated key\n",
        "                per_book[\"_work_level_\"] += 1\n",
        "                matched_lines += 1\n",
        "    return per_book, matched_lines\n",
        "\n",
        "# Prepare book id set to check: target edition and all known editions for same work\n",
        "book_ids_to_check = set()\n",
        "if target_record:\n",
        "    book_ids_to_check.add(str(target_record.get(\"book_id\")))\n",
        "if editions_for_work:\n",
        "    for e in editions_for_work:\n",
        "        book_ids_to_check.add(e[\"book_id\"])\n",
        "\n",
        "print(\"\\n[INFO] Streaming reviews file to count detailed reviews for these book_ids/work_id (this can take a few minutes).\")\n",
        "per_book_review_counts, matched = stream_count_reviews(REVIEWS_PATH, book_ids_set=book_ids_to_check, work_id=work_id_of_target, max_lines=None)\n",
        "print(f\"[INFO] Matched {matched} review records in dump referencing our target book_ids/work_id.\")\n",
        "print(\"[INFO] Review counts found in the reviews dump (per book_id):\")\n",
        "pprint(per_book_review_counts.most_common(30))\n",
        "\n",
        "# Step 4: (Optional) Stream interactions file similarly if you want to compare user-shelf interactions\n",
        "def stream_count_interactions(inter_path, book_ids_set=None, max_lines=None):\n",
        "    per_book = Counter()\n",
        "    if not os.path.exists(inter_path):\n",
        "        print(f\"[WARN] Interactions path {inter_path} not found on disk.\")\n",
        "        return per_book, 0\n",
        "    with gzip.open(inter_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"streaming interactions\")):\n",
        "            if max_lines and i >= max_lines:\n",
        "                break\n",
        "            try:\n",
        "                r = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            r_book_id = str(r.get(\"book_id\") or r.get(\"bookId\") or \"\")\n",
        "            if book_ids_set and r_book_id in book_ids_set:\n",
        "                per_book[r_book_id] += 1\n",
        "    return per_book, sum(per_book.values())\n",
        "\n",
        "if os.path.exists(INTERACTIONS_PATH):\n",
        "    print(\"\\n[INFO] Streaming interactions file for the same book_ids (this can also take several minutes).\")\n",
        "    per_book_inter_counts, total_inter_matches = stream_count_interactions(INTERACTIONS_PATH, book_ids_set=book_ids_to_check, max_lines=None)\n",
        "    print(f\"[INFO] Found {total_inter_matches} interaction records referencing these book_ids in the interactions dump.\")\n",
        "    pprint(per_book_inter_counts.most_common(30))\n",
        "else:\n",
        "    print(\"\\n[INFO] Interactions file not found at INTERACTIONS_PATH; skipping interactions streaming.\")\n",
        "\n",
        "# Step 5: Save a small report to CSV for traceability\n",
        "report_rows = []\n",
        "if target_record:\n",
        "    # add target edition row\n",
        "    report_rows.append({\n",
        "        \"query_book_id\": TARGET_BOOK_ID,\n",
        "        \"found_book_id\": target_record.get(\"book_id\"),\n",
        "        \"title\": target_record.get(\"title\"),\n",
        "        \"work_id\": work_id_of_target,\n",
        "        \"edition_ratings_count\": to_int_safe(target_record.get(\"ratings_count\")),\n",
        "        \"edition_text_reviews_count\": to_int_safe(target_record.get(\"text_reviews_count\")),\n",
        "        \"sum_ratings_across_editions\": total_ratings_in_dump if work_id_of_target else None,\n",
        "        \"sum_text_reviews_across_editions\": total_text_reviews_in_dump if work_id_of_target else None,\n",
        "        \"reviews_in_reviews_dump_for_edition\": per_book_review_counts.get(str(target_record.get(\"book_id\")), 0),\n",
        "        \"reviews_in_reviews_dump_for_other_editions_total\": sum(per_book_review_counts.get(bid, 0) for bid in book_ids_to_check if bid != str(target_record.get(\"book_id\"))),\n",
        "        \"reviews_in_reviews_dump_for_worklevel\": per_book_review_counts.get(\"_work_level_\", 0)\n",
        "    })\n",
        "# Save\n",
        "OUT_DIR = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/inspection_reports\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "out_csv = os.path.join(OUT_DIR, f\"book_inspection_{TARGET_BOOK_ID}.csv\")\n",
        "with open(out_csv, \"w\", newline='', encoding='utf-8') as cf:\n",
        "    if report_rows:\n",
        "        writer = csv.DictWriter(cf, fieldnames=list(report_rows[0].keys()))\n",
        "        writer.writeheader()\n",
        "        for r in report_rows:\n",
        "            writer.writerow(r)\n",
        "print(f\"\\n[INFO] Saved inspection report to: {out_csv}\")\n",
        "\n",
        "# Final log summary & suggestions\n",
        "print(\"\\n[SUMMARY]\")\n",
        "print(\" - If dump's edition-level counts are much smaller than the live Goodreads page numbers, likely causes:\")\n",
        "print(\"    * The dump stores edition (book_id) counts while the live page shows work-level aggregates (across editions).\")\n",
        "print(\"    * The dataset is a 2017 snapshot; live site counts change over time.\")\n",
        "print(\"    * The dump's reviews/interactions are a subset (not all Goodreads data), so counts in the dump can be much smaller.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIICu0PkuyK",
        "outputId": "9c299fbf-b4c3-4522-9e1b-82bdc4ad2cde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Scanning books file for target book and related editions (this reads the file once).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "scanning books: 335449it [00:28, 11938.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found target book_id 3209316. Title: 'Emma', work_id: 3360164\n",
            "\n",
            "[INFO] Target edition fields (concise):\n",
            "book_id             : 3209316\n",
            "title               : Emma\n",
            "title_without_series: Emma\n",
            "work_id             : 3360164\n",
            "authors             : [{'author_id': '1265', 'role': ''}]\n",
            "publication_year    : 2005\n",
            "ratings_count       : 42\n",
            "text_reviews_count  : 8\n",
            "num_pages           : 544\n",
            "format              : Audio CD\n",
            "is_ebook            : false\n",
            "language_code       : eng\n",
            "series              : []\n",
            "popular_shelves     : [{'count': '16215', 'name': 'classics'}, {'count': '7070', 'name': 'to-read'}, {'count': '4564', 'name': 'fiction'}, {'count': '4279', 'name': 'favorites'}, {'count': '3125', 'name': 'romance'}, {'count': '2750', 'name': 'classic'}, {'count': '1942', 'name': 'books-i-own'}, {'count': '1503', 'name': 'owned'}, {'count': '1277', 'name': 'jane-austen'}, {'count': '996', 'name': 'historical-fiction'}, {'count': '993', 'name': 'clàssics'}, {'count': '904', 'name': 'literature'}, {'count': '665', 'name': 'historical'}, {'count': '575', 'name': 'kindle'}, {'count': '560', 'name': 'owned-books'}, {'count': '530', 'name': 'favourites'}, {'count': '497', 'name': '19th-century'}, {'count': '478', 'name': 'classic-literature'}, {'count': '445', 'name': 'british'}, {'count': '419', 'name': 'novels'}, {'count': '398', 'name': '1001-books'}, {'count': '369', 'name': 'austen'}, {'count': '350', 'name': 'currently-reading'}, {'count': '340', 'name': 'british-literature'}, {'count': '329', 'name': 'historical-romance'}, {'count': '309', 'name': 'classics-to-read'}, {'count': '305', 'name': 'adult'}, {'count': '282', 'name': 'book-club'}, {'count': '282', 'name': 'regency'}, {'count': '279', 'name': 'england'}, {'count': '269', 'name': 'to-buy'}, {'count': '265', 'name': 'english'}, {'count': '250', 'name': 'adult-fiction'}, {'count': '248', 'name': 'my-books'}, {'count': '244', 'name': 'my-library'}, {'count': '243', 'name': 'audiobook'}, {'count': '236', 'name': 'library'}, {'count': '235', 'name': 'rory-gilmore-reading-challenge'}, {'count': '235', 'name': 'chick-lit'}, {'count': '230', 'name': 'i-own'}, {'count': '219', 'name': 'default'}, {'count': '216', 'name': 'audiobooks'}, {'count': '215', 'name': 'my-ebooks'}, {'count': '207', 'name': 'classic-fiction'}, {'count': '206', 'name': '1001'}, {'count': '202', 'name': 'novel'}, {'count': '200', 'name': 'ebooks'}, {'count': '193', 'name': 're-read'}, {'count': '173', 'name': 'classic-lit'}, {'count': '171', 'name': 'english-literature'}, {'count': '170', 'name': 'school'}, {'count': '168', 'name': 'to-read-classics'}, {'count': '163', 'name': 'literary-fiction'}, {'count': '161', 'name': 'own-it'}, {'count': '152', 'name': 'audio'}, {'count': '150', 'name': 'brit-lit'}, {'count': '142', 'name': '1001-books-to-read-before-you-die'}, {'count': '140', 'name': 'all-time-favorites'}, {'count': '139', 'name': 'the-classics'}, {'count': '134', 'name': 'tbr'}, {'count': '133', 'name': 'unfinished'}, {'count': '128', 'name': 'british-lit'}, {'count': '127', 'name': 'rory-gilmore-challenge'}, {'count': '116', 'name': 'general-fiction'}, {'count': '116', 'name': 'on-my-shelf'}, {'count': '107', 'name': 'women'}, {'count': '107', 'name': 'e-book'}, {'count': '105', 'name': 'did-not-finish'}, {'count': '103', 'name': 'abandoned'}, {'count': '98', 'name': 'humor'}, {'count': '98', 'name': 'e-books'}, {'count': '97', 'name': 'rory-gilmore'}, {'count': '96', 'name': 'female-authors'}, {'count': '95', 'name': 'kindle-books'}, {'count': '94', 'name': 'romantic'}, {'count': '94', 'name': 'reread'}, {'count': '93', 'name': 'read-in-2015'}, {'count': '93', 'name': 'female-author'}, {'count': '92', 'name': 'classici'}, {'count': '91', 'name': 'favorite-books'}, {'count': '90', 'name': 'own-to-read'}, {'count': '90', 'name': 'stand-alone'}, {'count': '89', 'name': 'on-hold'}, {'count': '89', 'name': 'my-bookshelf'}, {'count': '89', 'name': 'literary'}, {'count': '88', 'name': 'home-library'}, {'count': '88', 'name': 'love'}, {'count': '85', 'name': 'to-re-read'}, {'count': '84', 'name': 'tbr-pile'}, {'count': '84', 'name': 'audio-book'}, {'count': '84', 'name': 'drama'}, {'count': '83', 'name': 'history'}, {'count': '83', 'name': 'victorian'}, {'count': '82', 'name': 'read-in-2016'}, {'count': '82', 'name': 'wish-list'}, {'count': '82', 'name': 'audio-books'}, {'count': '81', 'name': 'bookshelf'}, {'count': '80', 'name': 'read-in-2017'}, {'count': '80', 'name': 'dnf'}, {'count': '79', 'name': 'personal-library'}]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[INFO] Collecting all editions that share work_id = 3360164\n",
            "[INFO] Number of editions in this dump that share the work_id: 221\n",
            "\n",
            "[INFO] Top 25 editions for this work (dump values):\n",
            "  book_id=6969, title=Emma                                                                   | ratings=470489 | text_reviews=8855\n",
            "  book_id=76691, title=Emma                                                                   | ratings=5253 | text_reviews=213\n",
            "  book_id=7938542, title=Emma                                                                   | ratings=4382 | text_reviews=142\n",
            "  book_id=18626814, title=Emma                                                                   | ratings=4043 | text_reviews=102\n",
            "  book_id=7181805, title=Emma                                                                   | ratings=922 | text_reviews=82\n",
            "  book_id=157421, title=Emma                                                                   | ratings=838 | text_reviews=113\n",
            "  book_id=894017, title=Emma                                                                   | ratings=746 | text_reviews=51\n",
            "  book_id=14926, title=Emma                                                                   | ratings=704 | text_reviews=74\n",
            "  book_id=1063179, title=Emma                                                                   | ratings=695 | text_reviews=94\n",
            "  book_id=586497, title=Emma                                                                   | ratings=618 | text_reviews=32\n",
            "  book_id=437131, title=Emma                                                                   | ratings=564 | text_reviews=66\n",
            "  book_id=6282632, title=Emma                                                                   | ratings=505 | text_reviews=49\n",
            "  book_id=563200, title=Emma                                                                   | ratings=458 | text_reviews=37\n",
            "  book_id=894048, title=Emma                                                                   | ratings=418 | text_reviews=37\n",
            "  book_id=18300260, title=Emma                                                                   | ratings=411 | text_reviews=62\n",
            "  book_id=15777407, title=Emma                                                                   | ratings=386 | text_reviews=52\n",
            "  book_id=274404, title=Emma                                                                   | ratings=379 | text_reviews=50\n",
            "  book_id=10931947, title=Emma                                                                   | ratings=356 | text_reviews=61\n",
            "  book_id=111025, title=Emma                                                                   | ratings=351 | text_reviews=49\n",
            "  book_id=468001, title=Emma                                                                   | ratings=346 | text_reviews=63\n",
            "  book_id=2726761, title=Emma                                                                   | ratings=334 | text_reviews=37\n",
            "  book_id=643084, title=Emma                                                                   | ratings=334 | text_reviews=43\n",
            "  book_id=13506315, title=Emma                                                                   | ratings=299 | text_reviews=8\n",
            "  book_id=198196, title=Emma                                                                   | ratings=293 | text_reviews=25\n",
            "  book_id=2003182, title=Emma                                                                   | ratings=291 | text_reviews=26\n",
            "\n",
            "[INFO] Sum across editions present in dump -> ratings_count: 504005, text_reviews_count: 11703\n",
            "\n",
            "[INFO] Streaming reviews file to count detailed reviews for these book_ids/work_id (this can take a few minutes).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "streaming reviews: 3565378it [00:44, 79767.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Matched 2508 review records in dump referencing our target book_ids/work_id.\n",
            "[INFO] Review counts found in the reviews dump (per book_id):\n",
            "[('6969', 1658),\n",
            " ('76691', 51),\n",
            " ('7938542', 36),\n",
            " ('18626814', 27),\n",
            " ('18300260', 25),\n",
            " ('7181805', 25),\n",
            " ('157421', 24),\n",
            " ('1063179', 23),\n",
            " ('10931947', 19),\n",
            " ('437131', 18),\n",
            " ('14926', 17),\n",
            " ('15777407', 17),\n",
            " ('6282632', 16),\n",
            " ('2726761', 15),\n",
            " ('563200', 13),\n",
            " ('894017', 12),\n",
            " ('111025', 12),\n",
            " ('15875763', 12),\n",
            " ('10249902', 11),\n",
            " ('894011', 11),\n",
            " ('274404', 11),\n",
            " ('586497', 11),\n",
            " ('7576798', 10),\n",
            " ('2003182', 10),\n",
            " ('894048', 9),\n",
            " ('468001', 9),\n",
            " ('2381674', 9),\n",
            " ('22738927', 9),\n",
            " ('8618588', 8),\n",
            " ('6550323', 8)]\n",
            "\n",
            "[INFO] Streaming interactions file for the same book_ids (this can also take several minutes).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "streaming interactions: 42792856it [03:13, 220752.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 83630 interaction records referencing these book_ids in the interactions dump.\n",
            "[('6969', 72315),\n",
            " ('76691', 1438),\n",
            " ('7938542', 825),\n",
            " ('18626814', 749),\n",
            " ('7181805', 531),\n",
            " ('18300260', 448),\n",
            " ('157421', 336),\n",
            " ('1063179', 294),\n",
            " ('24611702', 287),\n",
            " ('15777407', 278),\n",
            " ('10931947', 257),\n",
            " ('586497', 208),\n",
            " ('6282632', 196),\n",
            " ('437131', 178),\n",
            " ('894017', 176),\n",
            " ('22676096', 174),\n",
            " ('894048', 165),\n",
            " ('14926', 158),\n",
            " ('8618588', 151),\n",
            " ('13584589', 136),\n",
            " ('2003182', 134),\n",
            " ('563200', 130),\n",
            " ('274404', 127),\n",
            " ('1575363', 113),\n",
            " ('9666483', 109),\n",
            " ('111025', 108),\n",
            " ('2726761', 105),\n",
            " ('643084', 102),\n",
            " ('13506315', 97),\n",
            " ('468001', 96)]\n",
            "\n",
            "[INFO] Saved inspection report to: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/inspection_reports/book_inspection_3209316.csv\n",
            "\n",
            "[SUMMARY]\n",
            " - If dump's edition-level counts are much smaller than the live Goodreads page numbers, likely causes:\n",
            "    * The dump stores edition (book_id) counts while the live page shows work-level aggregates (across editions).\n",
            "    * The dataset is a 2017 snapshot; live site counts change over time.\n",
            "    * The dump's reviews/interactions are a subset (not all Goodreads data), so counts in the dump can be much smaller.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell: build work->editions mapping, infer original publication year, deduplicate by work_id, flag same-title diff-authors ===\n",
        "# Paste into a single Colab cell and run.\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from collections import defaultdict, Counter\n",
        "from statistics import mean, median\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "BOOKS_PATH = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Toggle for quick subcorpus test: uncomment the two lines below to run only on first 10 distinct work_ids.\n",
        "# (Leave commented to run on whole file.)\n",
        "QUICK_TEST_ONLY = True\n",
        "QUICK_TEST_WORK_COUNT = 10\n",
        "\n",
        "LOGFILE = os.path.join(OUTPUT_DIR, f\"dedup_workid_run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "\n",
        "# ---------- SETUP LOGGING ----------\n",
        "logger = logging.getLogger(\"goodreads_dedup\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# console handler\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))\n",
        "logger.handlers = [ch]\n",
        "# file handler\n",
        "fh = logging.FileHandler(LOGFILE, encoding='utf-8')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\n",
        "logger.addHandler(fh)\n",
        "\n",
        "logger.info(\"Starting work_id deduplication & edition analysis.\")\n",
        "logger.info(f\"Books path: {BOOKS_PATH}\")\n",
        "logger.info(f\"Output dir: {OUTPUT_DIR}\")\n",
        "logger.info(f\"Log file: {LOGFILE}\")\n",
        "\n",
        "# ---------- HELPERS ----------\n",
        "def to_int_safe(x) -> Optional[int]:\n",
        "    if x is None or x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return int(str(x).replace(\",\", \"\"))\n",
        "    except:\n",
        "        try:\n",
        "            return int(float(x))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def normalize_title(t: Optional[str]) -> str:\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    s = str(t).strip().lower()\n",
        "    # basic normalization: remove punctuation-like chars and extra whitespace\n",
        "    import re\n",
        "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def normalize_author_list(authors_field) -> List[str]:\n",
        "    \"\"\"\n",
        "    authors_field is often a list of dicts like [{'author_id': '1265', 'role': ''}]\n",
        "    We will extract author_id if present, else try to extract name.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if not authors_field:\n",
        "        return out\n",
        "    if isinstance(authors_field, list):\n",
        "        for a in authors_field:\n",
        "            if isinstance(a, dict):\n",
        "                aid = a.get(\"author_id\") or a.get(\"id\")\n",
        "                if aid:\n",
        "                    out.append(str(aid).strip())\n",
        "                else:\n",
        "                    # fallback to name if present\n",
        "                    name = a.get(\"name\") or a.get(\"author_name\")\n",
        "                    if name:\n",
        "                        out.append(str(name).strip().lower())\n",
        "            else:\n",
        "                out.append(str(a).strip().lower())\n",
        "    elif isinstance(authors_field, dict):\n",
        "        aid = authors_field.get(\"author_id\") or authors_field.get(\"id\")\n",
        "        if aid:\n",
        "            out.append(str(aid).strip())\n",
        "        else:\n",
        "            name = authors_field.get(\"name\") or authors_field.get(\"author_name\")\n",
        "            if name:\n",
        "                out.append(str(name).strip().lower())\n",
        "    else:\n",
        "        out.append(str(authors_field).strip().lower())\n",
        "    return out\n",
        "\n",
        "# ---------- PHASE 1: scan file once and build work_id -> list(editions) mapping ----------\n",
        "logger.info(\"Phase 1: scanning books file and building mapping work_id -> editions (single pass).\")\n",
        "\n",
        "work_to_editions: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "seen_work_ids = []\n",
        "seen_book_ids = set()\n",
        "num_lines = 0\n",
        "\n",
        "with gzip.open(BOOKS_PATH, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f, desc=\"reading books file\"):\n",
        "        num_lines += 1\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            logger.debug(f\"JSON decode error at line {num_lines}, skipping.\")\n",
        "            continue\n",
        "        book_id = str(obj.get(\"book_id\", \"\")).strip()\n",
        "        work_id = str(obj.get(\"work_id\", \"\")).strip()\n",
        "        if not work_id:\n",
        "            # place into special bucket for missing work_id (shouldn't happen per your statements)\n",
        "            work_id = \"__NO_WORK_ID__\"\n",
        "        # store selected fields to keep memory moderate but preserve raw for later retrieval\n",
        "        edition_info = {\n",
        "            \"book_id\": book_id,\n",
        "            \"work_id\": work_id,\n",
        "            \"title\": obj.get(\"title\"),\n",
        "            \"title_without_series\": obj.get(\"title_without_series\"),\n",
        "            \"authors_raw\": obj.get(\"authors\"),\n",
        "            \"authors_ids_or_names\": normalize_author_list(obj.get(\"authors\")),\n",
        "            \"publication_year\": to_int_safe(obj.get(\"publication_year\")),\n",
        "            \"ratings_count\": to_int_safe(obj.get(\"ratings_count\")),\n",
        "            \"text_reviews_count\": to_int_safe(obj.get(\"text_reviews_count\")),\n",
        "            \"num_pages\": to_int_safe(obj.get(\"num_pages\")),\n",
        "            \"language_code\": obj.get(\"language_code\"),\n",
        "            \"format\": obj.get(\"format\"),\n",
        "            \"is_ebook\": obj.get(\"is_ebook\"),\n",
        "            \"popular_shelves\": obj.get(\"popular_shelves\"),\n",
        "            \"_raw\": obj\n",
        "        }\n",
        "        work_to_editions[work_id].append(edition_info)\n",
        "        seen_work_ids.append(work_id)\n",
        "        seen_book_ids.add(book_id)\n",
        "\n",
        "logger.info(f\"Completed scan. Lines read: {num_lines}. Distinct works found (in mapping): {len(work_to_editions)}. Distinct book_ids seen: {len(seen_book_ids)}\")\n",
        "\n",
        "# ---------- QUICK-TEST: optionally shrink to first N works (for debugging) ----------\n",
        "# Uncomment next two lines for fast testing on a small subcorpus (10 works).\n",
        "if 'QUICK_TEST_ONLY' in globals() and QUICK_TEST_ONLY:\n",
        "     selected_work_ids = list(work_to_editions.keys())[:QUICK_TEST_WORK_COUNT]\n",
        "else:\n",
        "  selected_work_ids = list(work_to_editions.keys())\n",
        "\n",
        "logger.info(f\"Number of work_ids to process in Phase 2: {len(selected_work_ids)} (use QUICK_TEST_* to reduce)\")\n",
        "\n",
        "# ---------- PHASE 2: compute edition-count statistics and infer original publication year ----------\n",
        "logger.info(\"Phase 2: computing edition-count stats & inferring earliest publication year per work.\")\n",
        "\n",
        "edition_counts = []\n",
        "work_inferred_year = {}  # work_id -> inferred_first_pub_year (earliest among editions)\n",
        "work_selected_canonical_edition = {}  # work_id -> canonical edition (dict)\n",
        "\n",
        "for wid in tqdm(selected_work_ids, desc=\"processing works\"):\n",
        "    editions = work_to_editions[wid]\n",
        "    n_editions = len(editions)\n",
        "    edition_counts.append(n_editions)\n",
        "\n",
        "    # collect publication_years that are valid ints\n",
        "    pub_years = [e[\"publication_year\"] for e in editions if e.get(\"publication_year\") is not None]\n",
        "    inferred_year = None\n",
        "    if pub_years:\n",
        "        # take earliest year as inferred original publication year\n",
        "        inferred_year = min(pub_years)\n",
        "    work_inferred_year[wid] = inferred_year\n",
        "\n",
        "    # choose canonical edition for dedup: prefer earliest publication_year, tie-break by ratings_count (desc)\n",
        "    # if no publication_year available, choose edition with highest ratings_count\n",
        "    # if ratings_count missing, fallback to first edition\n",
        "    # Note: canonical edition stored as shallow copy (without raw sometimes to reduce memory)\n",
        "    chosen = None\n",
        "    # filter editions with non-null year\n",
        "    candidate_with_year = [e for e in editions if e.get(\"publication_year\") is not None]\n",
        "    if candidate_with_year:\n",
        "        # pick earliest year\n",
        "        earliest_year = min(e[\"publication_year\"] for e in candidate_with_year)\n",
        "        candidates = [e for e in candidate_with_year if e[\"publication_year\"] == earliest_year]\n",
        "        # tie-break by highest ratings_count\n",
        "        candidates_sorted = sorted(candidates, key=lambda r: (r.get(\"ratings_count\") or 0), reverse=True)\n",
        "        chosen = candidates_sorted[0]\n",
        "    else:\n",
        "        # fallback to highest ratings_count among all editions\n",
        "        editions_sorted = sorted(editions, key=lambda r: (r.get(\"ratings_count\") or 0), reverse=True)\n",
        "        if editions_sorted:\n",
        "            chosen = editions_sorted[0]\n",
        "    # Save chosen canonical edition (shallow)\n",
        "    if chosen:\n",
        "        work_selected_canonical_edition[wid] = {\n",
        "            \"book_id\": chosen[\"book_id\"],\n",
        "            \"title\": chosen[\"title\"],\n",
        "            \"publication_year_chosen\": chosen.get(\"publication_year\"),\n",
        "            \"ratings_count_chosen\": chosen.get(\"ratings_count\"),\n",
        "            \"text_reviews_count_chosen\": chosen.get(\"text_reviews_count\"),\n",
        "            \"authors_ids_or_names\": chosen.get(\"authors_ids_or_names\"),\n",
        "            \"num_editions_for_work\": n_editions\n",
        "        }\n",
        "    else:\n",
        "        work_selected_canonical_edition[wid] = {\n",
        "            \"book_id\": None,\n",
        "            \"title\": None,\n",
        "            \"publication_year_chosen\": None,\n",
        "            \"ratings_count_chosen\": None,\n",
        "            \"text_reviews_count_chosen\": None,\n",
        "            \"authors_ids_or_names\": [],\n",
        "            \"num_editions_for_work\": n_editions\n",
        "        }\n",
        "\n",
        "# compute edition-count distribution statistics\n",
        "if edition_counts:\n",
        "    stats = {\n",
        "        \"n_works_processed\": len(edition_counts),\n",
        "        \"min_editions_per_work\": min(edition_counts),\n",
        "        \"max_editions_per_work\": max(edition_counts),\n",
        "        \"mean_editions_per_work\": mean(edition_counts),\n",
        "        \"median_editions_per_work\": median(edition_counts),\n",
        "        \"pct_25\": sorted(edition_counts)[max(0, math.floor(0.25*len(edition_counts))-1)],\n",
        "        \"pct_75\": sorted(edition_counts)[max(0, math.floor(0.75*len(edition_counts))-1)]\n",
        "    }\n",
        "else:\n",
        "    stats = {}\n",
        "\n",
        "logger.info(\"Edition-count distribution stats computed:\")\n",
        "for k,v in stats.items():\n",
        "    logger.info(f\"  {k}: {v}\")\n",
        "\n",
        "# Save edition stats and inferred years\n",
        "pd.DataFrame({\n",
        "    \"work_id\": list(work_inferred_year.keys()),\n",
        "    \"inferred_first_publication_year\": list(work_inferred_year.values()),\n",
        "    \"canonical_book_id\": [work_selected_canonical_edition[w][\"book_id\"] for w in work_inferred_year.keys()],\n",
        "    \"canonical_title\": [work_selected_canonical_edition[w][\"title\"] for w in work_inferred_year.keys()],\n",
        "    \"canonical_pub_year\": [work_selected_canonical_edition[w][\"publication_year_chosen\"] for w in work_inferred_year.keys()],\n",
        "    \"canonical_ratings_count\": [work_selected_canonical_edition[w][\"ratings_count_chosen\"] for w in work_inferred_year.keys()],\n",
        "    \"num_editions_for_work\": [work_selected_canonical_edition[w][\"num_editions_for_work\"] for w in work_inferred_year.keys()]\n",
        "}).to_csv(os.path.join(OUTPUT_DIR, \"work_level_inferred_years_and_canonical.csv\"), index=False, encoding=\"utf-8\")\n",
        "logger.info(\"Saved work-level inferred years and canonical edition summary CSV.\")\n",
        "\n",
        "# ---------- PHASE 3: remove duplicates by work_id (i.e., build deduplicated list) ----------\n",
        "logger.info(\"Phase 3: building deduplicated dataset (one canonical edition per work_id).\")\n",
        "\n",
        "dedup_rows = []\n",
        "for wid, can in work_selected_canonical_edition.items():\n",
        "    dedup_rows.append({\n",
        "        \"work_id\": wid,\n",
        "        \"canonical_book_id\": can[\"book_id\"],\n",
        "        \"title\": can[\"title\"],\n",
        "        \"publication_year_inferred\": work_inferred_year.get(wid),\n",
        "        \"publication_year_chosen\": can.get(\"publication_year_chosen\"),\n",
        "        \"ratings_count_chosen\": can.get(\"ratings_count_chosen\"),\n",
        "        \"text_reviews_count_chosen\": can.get(\"text_reviews_count_chosen\"),\n",
        "        \"num_editions_for_work\": can.get(\"num_editions_for_work\"),\n",
        "        \"authors_ids_or_names\": \"|\".join(can.get(\"authors_ids_or_names\") or [])\n",
        "    })\n",
        "dedup_df = pd.DataFrame(dedup_rows)\n",
        "dedup_out_path = os.path.join(OUTPUT_DIR, \"deduplicated_by_workid.csv\")\n",
        "dedup_df.to_csv(dedup_out_path, index=False, encoding=\"utf-8\")\n",
        "logger.info(f\"Saved deduplicated-by-work CSV: {dedup_out_path} (rows = {len(dedup_df)})\")\n",
        "\n",
        "# ---------- PHASE 4: flag same title with different authors (potential ambiguous titles) ----------\n",
        "logger.info(\"Phase 4: flagging same title but different authors across deduplicated works.\")\n",
        "\n",
        "# build normalized title -> set of author identifiers mapping\n",
        "title_to_authors = defaultdict(set)\n",
        "title_to_workids = defaultdict(list)\n",
        "for _, r in dedup_df.iterrows():\n",
        "    norm_title = normalize_title(r[\"title\"])\n",
        "    authors_field = r[\"authors_ids_or_names\"] or \"\"\n",
        "    auths = [a for a in authors_field.split(\"|\") if a]\n",
        "    if not auths:\n",
        "        auths = [\"UNKNOWN_AUTHOR\"]\n",
        "    for a in auths:\n",
        "        title_to_authors[norm_title].add(a)\n",
        "    title_to_workids[norm_title].append((r[\"work_id\"], r[\"canonical_book_id\"], auths))\n",
        "\n",
        "# find titles with >1 distinct author id/name\n",
        "ambiguous_titles = {t: title_to_workids[t] for t,a in title_to_authors.items() if len(a) > 1}\n",
        "logger.info(f\"Found {len(ambiguous_titles)} normalized titles that map to multiple distinct authors (flagged for manual review).\")\n",
        "\n",
        "# Save ambiguous titles and examples\n",
        "ambig_rows = []\n",
        "for t, entries in ambiguous_titles.items():\n",
        "    ambig_rows.append({\n",
        "        \"normalized_title\": t,\n",
        "        \"n_distinct_authors\": len(set([a for sub in entries for a in sub[2]])),\n",
        "        \"work_examples\": \";\".join([f\"{wid}:{bid}\" for wid,bid,_ in entries]),\n",
        "        \"authors_examples\": \";\".join([\",\".join(sub[2]) for sub in entries])\n",
        "    })\n",
        "pd.DataFrame(ambig_rows).to_csv(os.path.join(OUTPUT_DIR, \"ambiguous_same_title_diff_authors.csv\"), index=False, encoding=\"utf-8\")\n",
        "logger.info(\"Saved ambiguous title report: ambiguous_same_title_diff_authors.csv\")\n",
        "\n",
        "# ---------- PHASE 5: summary & sample prints ----------\n",
        "logger.info(\"Phase 5: printing short sample summaries for manual inspection (4 works).\")\n",
        "\n",
        "# pick some sample work_ids (first few)\n",
        "sample_wids = selected_work_ids[:4]\n",
        "sample_report = []\n",
        "for wid in sample_wids:\n",
        "    can = work_selected_canonical_edition.get(wid, {})\n",
        "    editions = work_to_editions[wid]\n",
        "    sample_report.append({\n",
        "        \"work_id\": wid,\n",
        "        \"canonical_book_id\": can.get(\"book_id\"),\n",
        "        \"canonical_title\": can.get(\"title\"),\n",
        "        \"inferred_first_pub_year\": work_inferred_year.get(wid),\n",
        "        \"num_editions\": len(editions),\n",
        "        \"top_editions (book_id | year | ratings_count)\": \"; \".join([f\"{e['book_id']}|{e.get('publication_year')}|{e.get('ratings_count')}\" for e in sorted(editions, key=lambda x: (x.get('publication_year') if x.get('publication_year') is not None else 9999, -(x.get('ratings_count') or 0)))[:6]])\n",
        "    })\n",
        "logger.info(\"Sample deduplication report (first 4 works):\")\n",
        "for r in sample_report:\n",
        "    logger.info(json.dumps(r, ensure_ascii=False))\n",
        "\n",
        "pd.DataFrame(sample_report).to_csv(os.path.join(OUTPUT_DIR, \"sample_dedup_report_4works.csv\"), index=False, encoding=\"utf-8\")\n",
        "logger.info(\"Saved sample dedup report CSV.\")\n",
        "\n",
        "# ---------- PHASE 6: final small summary JSON ----------\n",
        "summary = {\n",
        "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "    \"books_file\": BOOKS_PATH,\n",
        "    \"n_lines_read\": num_lines,\n",
        "    \"n_works_in_mapping\": len(work_to_editions),\n",
        "    \"n_works_processed\": len(selected_work_ids),\n",
        "    \"edition_count_stats\": stats,\n",
        "    \"n_ambiguous_titles_multi_author\": len(ambiguous_titles),\n",
        "    \"deduplicated_rows_saved\": len(dedup_df),\n",
        "    \"output_dir\": OUTPUT_DIR\n",
        "}\n",
        "import json as _json\n",
        "with open(os.path.join(OUTPUT_DIR, \"dedup_summary.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
        "    _json.dump(summary, fh, indent=2)\n",
        "logger.info(\"Saved summary JSON to output directory.\")\n",
        "\n",
        "logger.info(\"Done. Files produced:\")\n",
        "for fn in os.listdir(OUTPUT_DIR):\n",
        "    logger.info(f\"  - {fn}\")\n",
        "\n",
        "logger.info(\"NOTE: If you uncomment QUICK_TEST_* variables at the top, you'll run on a small subset for testing. For full runs, leave them commented.\")\n",
        "\n",
        "# End of cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFlyPVV09XaF",
        "outputId": "f4b9e9ed-3166-4cc3-a429-03d1b7a2e747"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] Starting work_id deduplication & edition analysis.\n",
            "INFO:goodreads_dedup:Starting work_id deduplication & edition analysis.\n",
            "[INFO] Books path: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "INFO:goodreads_dedup:Books path: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/goodreads_books_romance.json.gz\n",
            "[INFO] Output dir: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs\n",
            "INFO:goodreads_dedup:Output dir: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs\n",
            "[INFO] Log file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs/dedup_workid_run_20250808_232403.log\n",
            "INFO:goodreads_dedup:Log file: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs/dedup_workid_run_20250808_232403.log\n",
            "[INFO] Phase 1: scanning books file and building mapping work_id -> editions (single pass).\n",
            "INFO:goodreads_dedup:Phase 1: scanning books file and building mapping work_id -> editions (single pass).\n",
            "reading books file: 335449it [00:35, 9509.05it/s] \n",
            "[INFO] Completed scan. Lines read: 335449. Distinct works found (in mapping): 189420. Distinct book_ids seen: 335449\n",
            "INFO:goodreads_dedup:Completed scan. Lines read: 335449. Distinct works found (in mapping): 189420. Distinct book_ids seen: 335449\n",
            "[INFO] Number of work_ids to process in Phase 2: 189420 (use QUICK_TEST_* to reduce)\n",
            "INFO:goodreads_dedup:Number of work_ids to process in Phase 2: 189420 (use QUICK_TEST_* to reduce)\n",
            "[INFO] Phase 2: computing edition-count stats & inferring earliest publication year per work.\n",
            "INFO:goodreads_dedup:Phase 2: computing edition-count stats & inferring earliest publication year per work.\n",
            "processing works: 100%|██████████| 189420/189420 [00:01<00:00, 182169.54it/s]\n",
            "[INFO] Edition-count distribution stats computed:\n",
            "INFO:goodreads_dedup:Edition-count distribution stats computed:\n",
            "[INFO]   n_works_processed: 189420\n",
            "INFO:goodreads_dedup:  n_works_processed: 189420\n",
            "[INFO]   min_editions_per_work: 1\n",
            "INFO:goodreads_dedup:  min_editions_per_work: 1\n",
            "[INFO]   max_editions_per_work: 520\n",
            "INFO:goodreads_dedup:  max_editions_per_work: 520\n",
            "[INFO]   mean_editions_per_work: 1.7709270404392357\n",
            "INFO:goodreads_dedup:  mean_editions_per_work: 1.7709270404392357\n",
            "[INFO]   median_editions_per_work: 1.0\n",
            "INFO:goodreads_dedup:  median_editions_per_work: 1.0\n",
            "[INFO]   pct_25: 1\n",
            "INFO:goodreads_dedup:  pct_25: 1\n",
            "[INFO]   pct_75: 2\n",
            "INFO:goodreads_dedup:  pct_75: 2\n",
            "[INFO] Saved work-level inferred years and canonical edition summary CSV.\n",
            "INFO:goodreads_dedup:Saved work-level inferred years and canonical edition summary CSV.\n",
            "[INFO] Phase 3: building deduplicated dataset (one canonical edition per work_id).\n",
            "INFO:goodreads_dedup:Phase 3: building deduplicated dataset (one canonical edition per work_id).\n",
            "[INFO] Saved deduplicated-by-work CSV: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs/deduplicated_by_workid.csv (rows = 189420)\n",
            "INFO:goodreads_dedup:Saved deduplicated-by-work CSV: /content/drive/MyDrive/Goodreads_Metadata_Reviews_2017/processing_outputs/deduplicated_by_workid.csv (rows = 189420)\n",
            "[INFO] Phase 4: flagging same title but different authors across deduplicated works.\n",
            "INFO:goodreads_dedup:Phase 4: flagging same title but different authors across deduplicated works.\n",
            "[INFO] Found 18618 normalized titles that map to multiple distinct authors (flagged for manual review).\n",
            "INFO:goodreads_dedup:Found 18618 normalized titles that map to multiple distinct authors (flagged for manual review).\n",
            "[INFO] Saved ambiguous title report: ambiguous_same_title_diff_authors.csv\n",
            "INFO:goodreads_dedup:Saved ambiguous title report: ambiguous_same_title_diff_authors.csv\n",
            "[INFO] Phase 5: printing short sample summaries for manual inspection (4 works).\n",
            "INFO:goodreads_dedup:Phase 5: printing short sample summaries for manual inspection (4 works).\n",
            "[INFO] Sample deduplication report (first 4 works):\n",
            "INFO:goodreads_dedup:Sample deduplication report (first 4 works):\n",
            "[INFO] {\"work_id\": \"56135087\", \"canonical_book_id\": \"34883016\", \"canonical_title\": \"Playmaker: A Venom Series Novella\", \"inferred_first_pub_year\": 2017, \"num_editions\": 1, \"top_editions (book_id | year | ratings_count)\": \"34883016|2017|5\"}\n",
            "INFO:goodreads_dedup:{\"work_id\": \"56135087\", \"canonical_book_id\": \"34883016\", \"canonical_title\": \"Playmaker: A Venom Series Novella\", \"inferred_first_pub_year\": 2017, \"num_editions\": 1, \"top_editions (book_id | year | ratings_count)\": \"34883016|2017|5\"}\n",
            "[INFO] {\"work_id\": \"46079519\", \"canonical_book_id\": \"26129545\", \"canonical_title\": \"Prowled Darkness (Dante's Circle, #7)\", \"inferred_first_pub_year\": 2016, \"num_editions\": 3, \"top_editions (book_id | year | ratings_count)\": \"26129545|2016|260; 26129711|2016|5; 29074693|None|149\"}\n",
            "INFO:goodreads_dedup:{\"work_id\": \"46079519\", \"canonical_book_id\": \"26129545\", \"canonical_title\": \"Prowled Darkness (Dante's Circle, #7)\", \"inferred_first_pub_year\": 2016, \"num_editions\": 3, \"top_editions (book_id | year | ratings_count)\": \"26129545|2016|260; 26129711|2016|5; 29074693|None|149\"}\n",
            "[INFO] {\"work_id\": \"3360164\", \"canonical_book_id\": \"13589693\", \"canonical_title\": \"Emma\", \"inferred_first_pub_year\": 1964, \"num_editions\": 221, \"top_editions (book_id | year | ratings_count)\": \"13589693|1964|4; 18343199|1965|12; 2999650|1968|27; 1793650|1980|59; 894047|1981|30; 17192369|1981|1\"}\n",
            "INFO:goodreads_dedup:{\"work_id\": \"3360164\", \"canonical_book_id\": \"13589693\", \"canonical_title\": \"Emma\", \"inferred_first_pub_year\": 1964, \"num_editions\": 221, \"top_editions (book_id | year | ratings_count)\": \"13589693|1964|4; 18343199|1965|12; 2999650|1968|27; 1793650|1980|59; 894047|1981|30; 17192369|1981|1\"}\n",
            "[INFO] {\"work_id\": \"51437308\", \"canonical_book_id\": \"30838933\", \"canonical_title\": \"Guardian Cougar (Finding Fatherhood, #2)\", \"inferred_first_pub_year\": null, \"num_editions\": 1, \"top_editions (book_id | year | ratings_count)\": \"30838933|None|139\"}\n",
            "INFO:goodreads_dedup:{\"work_id\": \"51437308\", \"canonical_book_id\": \"30838933\", \"canonical_title\": \"Guardian Cougar (Finding Fatherhood, #2)\", \"inferred_first_pub_year\": null, \"num_editions\": 1, \"top_editions (book_id | year | ratings_count)\": \"30838933|None|139\"}\n",
            "[INFO] Saved sample dedup report CSV.\n",
            "INFO:goodreads_dedup:Saved sample dedup report CSV.\n",
            "[INFO] Saved summary JSON to output directory.\n",
            "INFO:goodreads_dedup:Saved summary JSON to output directory.\n",
            "[INFO] Done. Files produced:\n",
            "INFO:goodreads_dedup:Done. Files produced:\n",
            "[INFO]   - dedup_workid_run_20250808_231313.log\n",
            "INFO:goodreads_dedup:  - dedup_workid_run_20250808_231313.log\n",
            "[INFO]   - dedup_workid_run_20250808_232403.log\n",
            "INFO:goodreads_dedup:  - dedup_workid_run_20250808_232403.log\n",
            "[INFO]   - work_level_inferred_years_and_canonical.csv\n",
            "INFO:goodreads_dedup:  - work_level_inferred_years_and_canonical.csv\n",
            "[INFO]   - deduplicated_by_workid.csv\n",
            "INFO:goodreads_dedup:  - deduplicated_by_workid.csv\n",
            "[INFO]   - ambiguous_same_title_diff_authors.csv\n",
            "INFO:goodreads_dedup:  - ambiguous_same_title_diff_authors.csv\n",
            "[INFO]   - sample_dedup_report_4works.csv\n",
            "INFO:goodreads_dedup:  - sample_dedup_report_4works.csv\n",
            "[INFO]   - dedup_summary.json\n",
            "INFO:goodreads_dedup:  - dedup_summary.json\n",
            "[INFO] NOTE: If you uncomment QUICK_TEST_* variables at the top, you'll run on a small subset for testing. For full runs, leave them commented.\n",
            "INFO:goodreads_dedup:NOTE: If you uncomment QUICK_TEST_* variables at the top, you'll run on a small subset for testing. For full runs, leave them commented.\n"
          ]
        }
      ]
    }
  ]
}